# 目标检测网络之RCNN系列

## RCNN
RCNNRegions with CNN features）是将CNN方法应用到目标检测问题上的一个里程碑
借助CNN良好的特征提取和分类性能,通过RegionProposal方法实现目标检测问题的转化

算法可以分为四步:
**1.候选区域选择**
Region Proposal是一类传统的区域提取方法,可以看作不同宽高的滑动窗口,通过窗口滑动获得潜在的目标图像，一般Candidate选项为2k个即可
根据Proposal提取的目标图像进行归一化，作为CNN的标准输入。
**2.CNN特征提取**
标准CNN过程，根据输入进行卷积/池化等操作，得到固定维度的输出；
**3.分类与边界回归**
实际包含两个子步骤，一是对上一步的输出向量进行分类（需要根据特征训练分类器）；二是通过边界回归（bounding-box regression) 得到精确的目标区域，由于实际目标会产生多个子区域，旨在对完成分类的前景目标进行精确的定位与合并，避免多个检出。

![image](https://img-blog.csdn.net/20170111155719842?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

RCNN存在三个明显的问题:
1）多个候选区域对应的图像需要预先提取，占用较大的磁盘空间；
2）针对传统CNN需要固定尺寸的输入图像，crop/warp（归一化）产生物体截断或拉伸，会导致输入CNN的信息丢失；
3)每一个ProposalRegion都需要进入CNN网络计算，上千个Region存在大量的范围重叠，重复的特征提取带来巨大的计算浪费。

---

## SPP-Net
既然CNN的特征提取过程如此耗时（大量的卷积计算），为什么要对每一个候选区域独立计算，而不是提取整体特征，仅在分类之前做一次Region截取呢？智者提出疑问后会立即付诸实践，于是SPP-Net诞生了。
![image](https://img-blog.csdn.net/20170111163710620?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

SPP-Net在RCNN的基础上做了实质性的改进：
1）取消了crop/warp图像归一化过程，解决图像变形导致的信息丢失以及存储问题；
2）采用空间金字塔池化（SpatialPyramid Pooling ）替换了 全连接层之前的最后一个池化层（上图top），翠平说这是一个新词，我们先认识一下它。

为了适应不同分辨率的特征图，定义一种可伸缩的池化层，不管输入分辨率是多大，都可以划分成m*n个部分。这是SPP-net的第一个显著特征，它的输入是conv5特征图 以及特征图候选框（原图候选框 通过stride映射得到），输出是固定尺寸（m*n）特征；

还有金字塔呢？通过多尺度增加所提取特征的鲁棒性，这并不关键，在后面的Fast-RCNN改进中该特征已经被舍弃；

最关键的是SPP的位置，它放在所有的卷积层之后，有效解决了卷积层的重复计算问题（测试速度提高了24~102倍），这是论文的核心贡献。
![image](https://img-blog.csdn.net/20170111163815829?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

尽管SPP-Net贡献很大,仍然存在很多问题:
1）和RCNN一样，训练过程仍然是隔离的，提取候选框 | 计算CNN特征| SVM分类 | Bounding Box回归独立训练，大量的中间结果需要转存，无法整体训练参数；
2）SPP-Net在无法同时Tuning在SPP-Layer两边的卷积层和全连接层，很大程度上限制了深度CNN的效果；
3）在整个过程中，Proposal Region仍然很耗时。

---

## Fast-RCNN

问题很多,解决思路非常巧妙
Fast-RCNN主要贡献在于对RCNN进行加速，快是我们一直追求的目标，问题在以下方面得到改进：
1）卖点1 - 借鉴SPP思路，提出简化版的ROI池化层（注意，没用金字塔），同时加入了候选框映射功能，使得网络能够反向传播，解决了SPP的整体网络训练问题；
2）卖点2 - 多任务Loss层
A）SoftmaxLoss代替了SVM，证明了softmax比SVM更好的效果；
B）SmoothL1Loss取代Bouding box回归。

将分类和边框回归进行合并（又一个开创性的思路），通过多任务Loss层进一步整合深度网络，统一了训练过程，从而提高了算法准确度。
3）全连接层通过SVD加速
这个大家可以自己看，有一定的提升但不是革命性的。
4）结合上面的改进，模型训练时可对所有层进行更新，除了速度提升外（训练速度是SPP的3倍，测试速度10倍），得到了更好的检测效果.

接下来分别展开:


对于yj = max(xi) 传统的max pooling的映射公式：

![](https://img-blog.csdn.net/20170111164025236?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

其中 为判别函数，为1时表示选中为最大值，0表示被丢弃，误差不需要回传，即对应 权值不需要更新。如下图所示，对于输入 xi 的扩展公式表示为：

![](https://img-blog.csdn.net/20170111164153690?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

(i,r,j) 表示 xi 在第 r 个框的第  j 个节点是否被选中为最大值（对应上图 y0,8 和 y1,0），xi 参数在前向传导时受后面梯度误差之和的影响。

![](https://img-blog.csdn.net/20170111164339457?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

多任务Loss层（全连接层）是第二个核心思路，如上图所示，其中cls_score用于判断分类，bbox_reg计算边框回归，label为训练样本标记。

其中Lcls为分类误差：

![](https://img-blog.csdn.net/20170111165447192?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

px 为对应Softmax分类概率，pl 即为label所对应概率（正确分类的概率），pl = 1时，计算结果Loss为0， 越小，Loss值越大（0.01对应Loss为2）。

Lreg为边框回归误差：

![](https://img-blog.csdn.net/20170111165451661?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

即在正确分类的情况下，回归框与Label框之间的误差（Smooth L1）， 对应描述边框的4个参数（上下左右or平移缩放），g对应单个参数的差异，|x|>1 时，变换为线性以降低离群噪声：
![](https://img-blog.csdn.net/20170111165455040?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

Ltotal为加权目标函数（背景不考虑回归Loss）：

![](https://img-blog.csdn.net/20170111165757744?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

Fast-RCNN几乎达到了实时.

---

## Faster-RCNN

对于提取候选框最常用的SelectiveSearch方法，提取一副图像大概需要2s的时间，改进的EdgeBoxes算法将效率提高到了0.2s，但是这还不够。
候选框提取不一定要在原图上做，特征图上同样可以，低分辨率特征图意味着更少的计算量，基于这个假设，MSRA的任少卿等人提出RPN（RegionProposal Network），完美解决了这个问题，我们先来看一下网络拓扑。

![](https://img-blog.csdn.net/20170111165802135?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**通过添加额外的RPN分支网络,将候选框提取合并到深度网络中,这正是Faster-RCNN里程碑的贡献**
RPN网络的特点在于通过滑动窗口的方式实现候选框的提取，每个滑动窗口位置生成9个候选窗口（不同尺度、不同宽高），提取对应9个候选窗口（anchor）的特征，用于目标分类和边框回归，与FastRCNN类似。






























