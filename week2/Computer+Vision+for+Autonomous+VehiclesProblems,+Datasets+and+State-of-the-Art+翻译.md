# Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art 翻译

## 自动驾驶计算机视觉研究综述：难题、数据集与前沿成果



### 摘要

近年来计算机视觉、机器学习和自动驾驶等人工智能相关领域发生了惊人的进展。然而，和每一个飞速发展的领域一样，人工智能领域也出现了业内人员难以跟上行业节奏或者业外人员难入行的问题。虽然已经有人发表了几篇关于这方面的专题调查论文，但是到目前为止，还没有关于自动驾驶计算机视觉（computer vision for autonomous vehicle）难题、数据集和方法的综合性调查。本文通过提供有关自动驾驶计算机视觉这一主题的最新调查以填补这一空白。我们的调查既包括最为相关的历史资料，也包括识别、重建、运动估测、追踪、场景理解以及端到端学习等当前最先进的专业主题。为了完成这一目标，我们首先通过分类学对每个方法进行分类，接着在 KITTI、ISPRS、MOT 和 Cityscapes 等若干个挑战性的基准数据集上分析每个方法的最佳性能。此外，我们还讨论了一些开放问题和当前的研究挑战。考虑到访问的轻松性和缺失的引用，我们还提供了一个具有主题和方法导航功能的互动平台，提供额外信息和每篇论文的项目链接。

### 前言

从 20 世纪 80 年代首次成功演示以来（Dickmanns & Mysliwetz(1992); Dickmanns & Graefe (1988); Thorpe et al. (1988)），自动驾驶汽车领域已经取得了巨大进展。尽管有了这些进展，但在任意复杂环境中实现完全自动驾驶导航仍被认为还需要数十年的发展。原因有两点：第一，在复杂的动态环境中运行的自动驾驶系统需要人工智能归纳不可预测的情境，从而进行实时推论。第二，信息性决策需要准确的感知，目前大部分已有的计算机视觉系统有一定的错误率，这是自动驾驶导航所无法接受的。

在此论文中，我们聚焦于第二个问题，也就是自动驾驶视觉（autonomous vision)，并调查了目前自动驾驶汽车中感知系统的表现。面向此目标，我们首先给出了问题分类，归类了已有的数据集，以及在这些类别中可使用的技术（论文成果），描述了每种方法的优缺点。第二，我们在数个流行数据集上分析了一些顶尖成果的表现。特别是我们给出了 KITTI 基准的全新深度质量分析，这些分析展示了那些用提交到评估服务器上的方法运行出来的最容易与最困难的例子。基于这些分析，我们讨论了开放的研究问题和挑战。为了更轻松的阅读，我们还给出了一个交互式在线工具，使用图像可视化了我们的分类，并提供了额外的信息与项目主页链接。通过提供详尽的综述，希望我们的成果能够成为自动驾驶视觉领域研究人员进行研究的有用工具，也能降低新人进入该领域的门槛。

目前也有其他相关的研究。Winner et al. (2015) 详细解释了主动安全性与驾驶辅助系统，考虑到了它们的结构与功能。他们的研究注重覆盖到辅助驾驶系统的所有方面，但关于机器视觉的章节只覆盖到了自动驾驶视觉问题中最基础的概念。Klette (2015) 给出了基于视觉的驾驶辅助系统的概述。他们描述了高层次的感知问题的大部分方面，但并不像我们一样提供了在各种任务上顶级成果的深度评测。Zhu et al. (2017) 提供了智能汽车环境感知的概述，聚焦于车道检测、交通信号／灯识别以及汽车追踪问题，该论文可与我们的研究互补。但相较之下，我们的目标是通过提供广泛的综述和对比（包括所有领域的成果），在机器人、智能汽车、摄影测绘学和计算机视觉社区之间架起一座桥梁。

## 1.自动驾驶的历史

### 1.1 自动驾驶项目

​       世界各地的许多政府机构启动各式各样的项目来开发智能交通系统（ITS）。PROMETHEUS这个项目1986年在欧洲立项并且包括超过13个交通工具生产商，当中的许多研究成员来自19个欧洲国家的政府和高校。美国的第一个项目就是1988年由卡耐基梅隆大学的Navlab Thorpe等人创建的.由于这个项目完成了第一次从Pittsburgh，PA, Sand Diego和CA的自动驾驶，在1995年是一个重要的里程碑。在许多大学，研究中心和自动驾驶公司的倡议下，美国政府在1995年成立了自动化公路系统联盟（NAHSC）。和美国一样，日本于1996年在各大自动驾驶高斯和研究中心成立了高级巡航公路系统研究协会来促进自动驾驶导航的研究。Bertozzi等人（2000）对自主道路后续开发的挑战性任务进行了多方面的探讨。他们得出结论，算法计算能力越来越好，但像反射，湿面潮湿，阳光直射，隧道和阴影这样的困难仍然使数据解释具有挑战性。因此，他们建议提高传感器性能，同时也指出应该重点并且认真的考虑自动驾驶对行人法律方面的责任和影响。总之，无人驾驶可能会限制仅仅用在特殊的基础设施上，然后慢慢的普及开来。
  项目PROMETHEUS可以实现在高速公路上自动驾驶，在这个成功的案例推动下，Franke等人描述了在复杂的城市交通场景下的自动驾驶的实时视觉系统。虽然在此之前公路场景情况已经有很多深入的研究，但城市场景却从未得到解决。他们的系统包括基于深度的障碍检测和立体追踪，以及针对相关物体（比如：交通信号）的单目检测和识别框架。
  Vis-Lab3提出的多种传感系统的融合把几款原型车包括ARGO Broggi等 （1999），TerraMax Braid 等人（2006）和BRAiVE Grisleri＆Fedriga（2010）带到了人的视野中。 BRAiVE是目前VisLab开发的整合所有系统的最新车型。 Bertozzi等人（2011）在VisLab洲际自动驾驶挑战赛（意大利到中国的半自主驾驶）展示了其系统的稳健性。车载系统允许检测障碍物，车道标记，沟渠，护堤，并识别前方是否存在车辆和车辆位置。感应系统提供的信息用于执行不同的任务，如引导跟随和前进&停止。
PROUD项目Broggi等人（2015年）略微修改了BRAiVE原型Grisleri＆Fedriga（2010）使得汽车可以在帕尔马城市道路和高速公路的常规交通情况下开车。为了实现这一目标，他们丰富了一份公开授权的地图，其中包含有待完成的机动信息（比如行人过路，交通信号灯等）。该车辆能够在没有人为干涉的情况下处理复杂的场景，例如回旋处，交叉口，优先道路，站点，隧道，人行横道，交通信号灯，高速公路和城市道路。

V-Charge项目Furgale等人 （2013年）提供配备了接近产品级的（close-to-market）传感器的电动自动车。提出了一个完全可控的系统，包括视觉定位，映射，导航和控制。该项目解决了诸多困难比如，Heng et al. (2013, 2015)的校准calibration问题, H¨ane 等人(2014)的立体匹配问题,Haene等人的 (2012, 2013, 2014)重建问题, Grimmett等人(2015)的SLAM问题和 H¨ane等人的(2015)空白区于检测的问题。除了这些研究目标，该项目还非常重视在现实环境中部署和评估系统。

Google于2009年开始了自驾车项目，直到2016年3月4日完成了超过1,498,000英里的驾驶距离，在美国加利福尼亚州奥斯汀市的Mountain View，WA和柯克兰。不同的传感器（例如摄像机，雷达，LiDAR，车轮编码器，GPS）可以全方位的检测行人，骑自行车的人，车辆，道路工作等等。据他们的事故报道，Google的自驾车只涉及14次碰撞，13次是由别人造成的。在2016年，这个项目分引入到了一家独立的自动驾驶技术公司Waymo5。
  Tesla Autopilot是由特斯拉开发的高级驾驶员辅助系统，该系统于2015年第一次推出其第七版的软件。系统的自动化级别满足完全自动化要求，但是需要驾驶员充分注意，如果有必要的话需要驾驶员接管控制。从2016年10月起，特斯拉生产的所有车辆配备了8台摄像机，12台超声波传感器和一个前置雷达，以实现全自动驾驶。
长距离测试演示：1995年，PROMETHEUS项目里Dickmanns等人（1990）;弗兰卡等人（1994）; Dickmanns等人（1994年）的团队演示了从德国慕尼黑到丹麦欧登塞以高达175公里/小时的速度进行的第一次自动长途驾驶，其中约95％为自主驾驶。同样，在美国Pomerleau和Jochem（1996年）在“不用双手横穿美国”旅程中从华盛顿特区驶向圣地亚哥，整个行程中有98％的自动驾驶和偶尔的手动纵向控制。

2014年，Zieglar等人 （2014年）以近乎完全自动的方式，展示了从曼海姆（Mannheim）到德国普福尔茨海姆（Pforzheim Germany）的103km的骑行，也就是众人所熟知的Bertha Benz纪念路线。他们展示了一种装配有接近产品级的传感器硬件的自动驾驶车辆。由雷达和立体视觉来进行物体检测和空白区域分析。单目视觉用来检测交通信号灯和物体分类。两种互补的算法，基于点特征和基于场景标记，允许相对于手动注释的数字路线图进行精确定位。他们得出结论，即使认为自动驾驶虽然成功完成了，但是整体行为远远达不到细心的驾驶司机的水平。
  最近，Bojarski等人 （2016年）从霍尔姆德尔（Holmdel）到新泽西州蒙茅斯县的大西洋高原，以及在花园州立大道没有任何干扰的自动行驶了10英里，也不是说100%，其中98%是在自动驾驶。为了实现这一目标，在NVIDIA DRIVETM PX自动驾驶车中使用了一种从图像直接预测车辆控制的卷积神经网络。该系统在第11节中有更详细的讨论。
  虽然所有上述表现令人印象深刻，但通常采取精确注释路线图以及预录用于本定位的地图证明了自主性系统仍然不及人的能力。最重要的是，不仅需要视觉信息的强大的感知，也需要普遍的人工智能才能达到人的可靠性水平，那样即使在复杂的城市情况下也能安全地做出反应。

### 1.2 自动驾驶竞赛

  European Land Robot Trial （ELROB）是现实场景和地形中无人系统的示范与竞赛，主要集中在军事方面，如侦察监视，自主航行和车队运输。与自主驾驶挑战相反，ELROB场景通常包括崎岖地形的导航。2004年，美国国防高级研究计划署（DARPA）发起了第一个专注于道路场景（主要是泥土路）的自动驾驶比赛。DARPA 2004年大挑战赛为首先完成从加利福尼亚州内华达州过境的150英里的路线的队伍提供100万美元的奖金。然而，机器人车辆都没有完成路线。
  一年后，也就是2005年，DARPA公布了第二版的挑战，5辆车顺利完成了路线（Buehler等人（2007））。DARPA大挑战赛的第三场比赛，被称为城市挑战赛（Buehler等人（2009）），于2007年11月3日在乔治航空加利福尼亚州的基地举办。这个挑战涉及到一个96公里的城市地区航线，在这段路程中车辆在与其他车辆进行协调并汇合到一起时，必须遵守交通法规。
  2011年首次和2016年第二版的专注于自动合作驾驶行为的大型合作驾驶挑战（GCDC8，见Geiger（2012a））在荷兰赫尔蒙德举行。在比赛中，队伍必须与护航队协调，加入护航队和引导护航队。获胜者是依靠为随机混合队伍打分的系统选出来的。

## 2数据集和基准

数据集在许多研究领域进展方面发挥了关键作用，通过提供带有真实值（ground truth）的问题实例。它们允许对方法进行定量评估，提供关于其能力和局限性的关键见解。特别地，这些数据集中的几个比如Geiger等人（2012b）;Scharstein＆Szeliski（2002）;Baker等人（2011）;Everinghamet （2010）; Cordts等人 （2016）也提供在线评估服务器允许在给定的测试中进行公平的比较，而且为该领域的研究人员提供目前最好的算法的概况。这种方式可以让研究人员很容易地确定目前的进展和剩下的挑战。在自主车辆的环境中，KITTI数据集Geiger等人（2012b）和Cityscapes数据集Cordts等人（2016）为重建、运动估计和识别任务引入了挑战性的基准，因此缩小了实验室设置与挑战现实世界的情况之间的差距。仅仅几年前，大家认为有数百个注释的例子的数据集对于解决很多问题是足够的。然而，随着有数百到数千个有标签的例子的数据集的引入，通过以监督的方式训练大容量深度模型，已经使得许多计算机视觉学科的重大突破。然而，收集大量的注释数据不是一个容易的事情，特别是对于诸如光流或者语义分割的任务。这就发起了一项集体努力，通过搜索尽可能多的自动化过程的方法(如通过半监督学习或合成)，在几个领域生成这类数据。

### 2.1 真实数据集

虽然某些算法领域可以使用合成数据检验，但实际数据集对于确保算法在实际情况下的表现是必要的。例如，在实践中使用的算法需要处理复杂的对象和环境，同时面对挑战性的环境条件，例如直接照明，镜面反射，雾或雨。获取真实值通常是劳动密集型的，因为这种信息通常不能用传感器直接获得，而是需要繁琐的手动注释。例如，（Scharstein＆Szeliski（2002），Baker等人（2011））在受控实验室环境中获得了密集的像素级注释，而Geiger等人（2012B）;Kondermann等人（2016）使用LiDAR激光扫描仪提供实际街景场景的稀疏像素级注解。
  最近，亚马逊的Mechanical Turk众包业务常用于为大型数据集创建注释，例如Deng等人(2009);Lin等人（2014）; Leal-Taix’e等人（2015）; Milan等人（2016）。然而，通过Mechanical Turk获得的注释质量通常不太合适作为参考，并且通常需要在后处理和清理获得的标签方面作出重大努力。在下文中，我们将首先讨论最流行的计算机视觉数据集和基准，以解决与自主视觉相关的任务。此后，我们将专注于那些致力于自动驾驶车辆的应用的数据集。
  **立体与 3D 重建类数据集**：由Scharstein＆Szeliski（2002）引入的Middlebury立体测试基准提供了多个立体图像数据集，用于比较立体匹配算法的性能。通过手工标注并在分段平面场景中重构平面分量，得到像素级的真实值。Scharstein和Szeliski（2002）进一步提供立体匹配算法的分类法，允许通过比较设计决策和测试台来进行定量评估。使用均方误差以及估计值和真实视差图之间误匹配像素的百分比来评估提交给其基准网站的方法。
  Scharstein & Szeliski (2003) 和 Scharstein et al. (2014)为Middlebury基准引入了一种新颖的数据集，这个数据集包含更多复杂的场景和普通的物体，比如椅子、桌子、植物等对象。在这两个工作中，均使用一个结构化的照明系统来创造真实值。对于最新版本的Middlebury v3，Scharstein等人(2014)提出了一种用于摄像机和投影仪的二维亚像素对应搜索和自校准的新技术，为高分辨率立体图像生成高度精确的真实值。与现有数据集相比，该新版本的差异和整改精度明显提高，可以进行更精确的评估。图1是来自数据集的示例深度图。

![img](https://img-blog.csdnimg.cn/20190104171056386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图1:Scharstein等人(2014)的结构光系统提供了高度精确的深度真实值，以颜色和阴影显示(顶部)。(a)、(b)提供了近距离观察，(c )显示了圆角差异，(d)中使用基线法得到的表面。改编自Scharstein et al.(2014)。
  Seitz等人的Middlebury多视角立体（MVS）测试基准（2006）是一种校准过的带有真实3D模型的用于比较MVS方法图像数据集。基准测试在MVS方法的进步中发挥了关键作用，但只有两个场景，尺寸相对较小。相比之下，Jensen等人的TUDMVS数据集（2014年）提供了124个不同的场景，这些场景也是受控实验室环境中采集得到。参考数据通过组合来自每个摄像机位置的结构光扫描获得，并且所得到的扫描图非常密集，平均每个图包含13.4million个点。对于44个场景，通过以90度的间隔旋转和扫描四次获得完整的360度模型。 与迄今为止的数据集相比，Sch¨ops等人（2017年）提供了在受控实验室环境中未仔细分级的场景，从而代表了现实世界的挑战。Sch¨ops et al. (2017) 录制了高分辨率DSLR单反相机图像以及各种室内和室外场景中同步的低分辨率立体视频。高精度激光扫描仪允许以强大的方法记录所有图像。高分辨率图像可以评估详细的3D重建，同时提供低分辨率立体图像来比较移动设备的方法。
  **光流类数据集**：Baker等人的“Middlebury基准” （2011）提供了具有非刚性运动序列，合成序列和Middlebury立体基准序列（静态场景）的子集的序列，用于评估光流方法。对于所有非刚性序列，通过使用toothbrush牙刷追踪在物体上喷洒的隐藏的荧光纹理来获得真实流。
  数据集包含八个不同的序列，每个序列具有八个帧。 每个序列提供一对帧的真实值。除了有限的大小之外，由于数据集需要实验室条件，允许在各个捕获之间操纵光源，所以缺少像复杂结构，照明变化和阴影这样的真实世界挑战。此外，它只包含最多十二个像素的非常小的运动，不能提供对快速运动的验证。 然而，与其他数据集相比，Middlebury数据集可以评估亚像素精度，因为它提供了非常精确和密集的真实值。使用角度误差（AEE）和估计流与真实值之间的绝对终点误差（EPE）来测量性能。
  Janai等人 （2017）提出了一个新颖的光流数据集，其中包括复杂的现实世界场景，与Middlebury的实验室设置相反。高速视频摄像机用于通过密集采样的时空容量跟踪像素来创建精确的参考数据。该方法允许以自动方式在挑战性的日常场景中获取光流场地真相，并且增加诸如运动模糊的现实效果以在不同条件下比较方法。Janai等人 （2017年）提供了160个不同的现实世界动态场景序列，具有比以前的光学数据集显着更大的分辨率（1280x1024像素），并比较了这些数据的几种最先进的光学技术。
  **对象识别与分割类数据集**：大量的公开数据集，如ImageNet（Deng等人（2009）），PASCAL VOC（Everingham等（2010）），Microsoft COCO（Lin等人（2014）），Cityscapes（Cordts （2016））和TorontoCity（Wang等人（2016年））对物体分类，目标检测和语义分割任务中深入学习的成功产生了重大影响。
  由Everingham等人（2010）提供的PASCAL视觉对象类（VOC）挑战是对象分类，物体检测，物体分割和动作识别的基准。它由具有高质量标注的Flickr收集的有挑战性的消费者照片组成，并且包含姿势，照明和遮挡的大变化。自其面世以来，VOC的挑战一直很受欢迎并且逐年更新以适应社区的需求，直到2012年计划结束。而2005年的第一个挑战只有4个不同的类，2007年引入了20个不同的对象类。多年来，基准规模在2012年达到总共11,530张图像当中共有27,450张ROI注释物体。
2014年，Lin等 （2014）介绍了Microsoft COCO数据集，用于物体检测，实例分割和上下文推理。它们在自然环境中提供包含常见对象的复杂日常场景的图像。 数据集总共包括91个对象类，250万个注释实例和328k个图像。 Microsoft COCO在PASCAL VOC对象分割基准测试中每个类的实例数显著增加。所有物体都在广泛的人群工作人员的努力下对每个实例进行标注。与PASCAL VOC类似，IOU度量用于评估。
  **追踪类数据集**：Leal-Taixe等(2015);Milan等(2016)提出的MOTChallenge解决了多目标跟踪缺乏集中基准的问题。该基准测试包含了14个在无约束环境下用静态和动态摄像机拍摄的具有挑战性的视频序列，并包含了许多现有的多目标跟踪基准测试，如PETS(Ferryman & Shahrokni(2009))和KITTI (Geiger et al. (2012b))。提供了三个对象类的注释:移动或站着的行人、不在直立位置的人和其他人。他们使用Stiefelhagen等人(2007)引入的两种流行的跟踪方法:多目标跟踪准确度(MOTA)和多目标跟踪精度(MOTP)来评估这些方法。由作者提供的检测真实值能够在不依赖检测系统的情况下分析跟踪系统的性能。使用检测器的方法和使用检测真实值的方法可以在其网站上进行比较。
  **航空图像数据集**：航空图像数据集:ISPRS benchmark (Rottensteiner et al. 2013, 2014)提供机载传感器获取的数据，用于城市目标检测和三维建筑重建和分割。它包含两个数据集:Vaihingen和多伦多市中心。在对象检测任务中考虑的对象类包括建筑、道路、树、地面和汽车。Vaihingen数据集提供了三个领域的各种对象类和一个大型的道路检测算法测试站点。多伦多市中心数据集位于加拿大多伦多市中心，面积约1.45平方公里。与Vaihingen类似，有两个较小的区域用于对象提取和建筑重建，以及一个较大的区域用于道路检测。对于每个测试区域，提供具有方位参数的航空图像、数字表面模型(DSM)、正射影像拼接和机载激光扫描。这些方法的质量是通过检测和重建的几个指标来评估的。在这两种情况下，完整性、正确性和质量都是在每个区域级别和每个对象级别上进行评估的。
  **自动驾驶数据集**：2012年，Geiger等人(2012b、2013)介绍了KITTI视觉测评基准为立体匹配、光流,视觉测距/SLAM和3D对象检测(图2)。数据集是从一个自主驾驶平台上采集,包括6个小时的录像，采用的是高分辨率彩色和灰度立体相机,一个三维激光扫描仪和高精度GPS / IMU惯性导航系统。来自这个数据集立体图像和光流基准组成194训练和195测试图像对，图像的分辨率为1280×376像素，和通过将三维激光点云积累投射到图像获得的稀疏的真实值。由于旋转激光扫描仪作为参考传感器的局限性，立体图像和光流基准仅适用于有摄像机运动的静态场景。

![img](https://img-blog.csdnimg.cn/20190104171326834.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图2:Geiger等人(2012b)提出的KITTI基准测试中带有传感器(左上)、轨迹(上中心)、视差和光流(右上)以及3D对象标签(下)的记录平台。改编自Geiger等人(2012b)。
  为了为动态场景提供真实值运动场，Menze& Geiger(2015)对400个动态场景进行了标注，将精确的3D CAD模型拟合到所有运动中的车辆上，以获得这些物体的流和立体图像的真实值。KITTI的流和立体匹配基准测试使用错误(坏)像素的百分比来评估提交的方法的性能。另外，Menze &Geiger(2015)将立体匹配和流的真实值相结合，形成了一个新的3D场景流基准。为了评估场景流，他们结合了经典的立体匹配和光流测量的方式。
  视觉测距/SLAM挑战由22个立体序列组成，总长度为39.2公里。利用带有RTK校正信号的GPS/IMU定位单元，得到真实值位姿。考虑了某一特定轨迹长度上的平均平移误差和旋转误差。
  针对KITTI对象检测挑战，开发了一种特殊的3D标记工具，可以用3D边框标注所有的3D对象，用于7481个训练和7518个测试图像。目标检测任务的基准被分为车辆检测任务、行人检测任务和自行车检测任务，允许重点分析自动车辆环境中最重要的问题。继PASCAL VOC Everingham等人(2010)之后，我们使用交叉-联合(IOU)度量来进行评估。为了进行额外的评估，这个度量被扩展到捕获2D检测和3D方向估计性能。一个真正的3D评估计划即将发布。
  Fritsch等人(2013)将KITTI基准扩展到道路/车道检测任务。总共挑选了600幅不同的训练和测试图像，用于人工标注道路和车道区域。Mattyus et al.(2016)利用航拍图像对KITTI数据集进行了增强，使用了细粒度的细分类别，如停车位和人行道，以及道路车道的数量和位置。KITTI数据集已经成为所有上述任务的标准基准之一，特别是在自动驾驶应用程序的环境中。
  作为对其他数据集的补充，Kondermann等人(2016)提出的HCI基准包含了现实的、系统的不同辐射测量和几何挑战。总体上，提供了28,504对立体图像和流真实值。与以前的数据集相比，所有静态区域的真实值不确定度已被估计。不确定度估计是由基于蒙特卡罗抽样的每个帧的像素级误差分布得到的。动态区域被手动屏蔽，并以近似真值注释3500对图像。
  该数据集的主要局限性在于所有序列都记录在单一街道段，缺乏多样性。另一方面，这可以更好地控制内容和环境条件。与KITTI的移动式激光扫描方案相比，静态场景仅使用高精度激光扫描仪扫描一次，以获得所有静态部件的密集且高度精确的真实值。除了KITTI和Middlebury所使用的指标外，他们在评估Honauer等人(2015)时还使用了语义上有意义的性能指标，比如边缘畸变和表面平滑度。HCI基准是一个相当新的但尚未建立完整的数据集，受控的环境能够模拟很少发生的事件，如事故，这评估自动驾驶系统很有意义。
  由Dollar et al.(2009)提出的加州理工学院行人检测基准提供了25万帧由车辆在城市环境中正常行驶时记录的序列。35万个边界框和2300个独特的行人进行了注释，包括边界框之间的时间对应和详细的遮挡标签。通过绘制误报漏检率和改变检测置信度的阈值来评估方法。
  Cordts et al.(2016)的Cityscapes数据集为像素级和实例级语义标注提供了一个基准和大规模数据集，可以捕捉现实世界城市场景的复杂性。它由大量的、多样的立体视频序列组成，这些序列记录在不同城市的街道上。为5000张图片提供了高质量的像素级注释，而另外20000张图片则使用了新的众包平台获得的粗糙标签进行注释。对于两个语义粒度，即他们报告平均性能得分，并在实例级别上评估IoU度量，以评估个别实例在标签中的表现如何。
  Wang等人(2016)提出的多伦多城市基准覆盖了大多伦多地区，面积712平方公里，道路8439公里，建筑面积约40万幢。该基准涵盖了大量的任务，包括建筑高度估计(重建)，道路中心线和路缘提取，建筑实例分割，建筑轮廓提取，语义标记和场景类型分类。该数据集是通过飞机、无人机和在城市中行驶的汽车获取的，以提供不同的视角。
  **长期自动驾驶（Long-Term Autonomy）类数据集**：像KITTI或Cityscapes这样的数据集关注的是自动驾驶的算法能力的发展，但没有解决长期自动驾驶的挑战，比如环境随时间的变化。为了解决这个问题，Maddern等人(2016)提出了一种新的自动驾驶数据集。他们在英国牛津中部1000公里的路程中收集了图像、激光雷达和GPS数据。这使得他们能够捕捉到由于光照、天气和季节变化、动态物体和建筑而产生的场景外观的巨大变化。这样的长期数据集允许深入调查阻碍实现自动驾驶的问题，如在一年内的不同时间进行定位。

### 2.2 合成类数据集

对于真实的例子，产生真实值是非常劳动密集型的，并且在需要像素级注释时通常甚至不可能大规模地实现。另一方面，可以容易地获取大规模合成数据集的像素级基础真值。然而，创造现实的虚拟世界是耗时的。电影和视频游戏的普及导致了行业创造非常现实的3D内容，这些内容丰富了使用合成数据集完全替代实际数据的希望。因此，最近已经提出了几个合成数据集，但是现实主义和多样性是否足以替代现实世界的数据集仍然是一个悬而未决的问题。此外，创造现实的虚拟内容是一个耗时和昂贵的过程，实际数据和合成（或增强）数据之间权衡还不清楚。
  **MPI Sintel**: Butler等人(2012)提出的MPI Sintel流基准，利用开源动画短片Sintel，利用光流真实值渲染不同复杂的场景。Sintel总共有1628帧。使用呈现管道的不同通道，获得的数据集在复杂性上有所不同，如图3所示

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190104172518417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图3:这个图改编自Butler等人(2012)，展示了渲染管道不同通道得到的Sintel基准的不同复杂度:反射版、纯净版和最终效果(从上到下)。
  反射版有大致分段不变的颜色，没有照度效果，而纯净版则引入了各种照度。最后通过添加大气效果，模糊，颜色校正和晕渲。除了平均终端误差之外，测评基准网站还根据速度、遮挡边界和违抗提供不同的方法排名。
  **Flying Chairs and Flying Things**:光流数据集有限的大小阻碍了深度大容量模型的训练。为了训练卷积神经网络，Dosovitskiy等人(2015)因此引入了一个简单的合成2D数据集，这些数据集是在Flickr的随机背景图像上呈现的。由于该数据集有限的真实性和规模证明不足以学习高度精确的模型，Mayer等人(2016)提出了另一个大型数据集，包含三个合成立体视频数据集:FlyingThings3D、Monkaa、Driving。FlyingThings3D提供了日常的3D物体在随机创建的场景中沿随机的3D轨迹飞行。受到KITTI数据集的启发，一个驾驶数据集已经被创建，它使用来自于FlyingThings3D相同池的汽车模型，另外高度详细的树和从3D仓库建立模型。Monkaa是一部动画短片，类似于MPI Sintel基准测试中使用的Sintel。
  **游戏引擎**:不幸的是，来自动画电影的数据非常有限，因为内容很难更改，而且此类电影很少是开源的。相比之下，游戏引擎允许创建无限数量的数据。Gaidon等人(2016)提出了一种使用游戏引擎创建虚拟世界的方法，该方法引入了虚拟KITTI数据集。他们提出了一种有效的从现实世界到虚拟世界的克隆方法来创建真实的代理世界。一个克隆的虚拟世界允许不同的条件，如天气或照明和使用不同的相机设置。通过这种方式，可以利用代理世界进行虚拟数据增强来训练深度网络。虚拟KITTI包含35个逼真的合成视频，共17000帧高分辨率。它们为目标检测、跟踪、场景和实例分割、深度和光流提供真实值。
  Ros等人(2016)在并行工作中创建了SYNTHIA，这是一种综合收集了城市场景的图像和注释，用于进行语义分割。他们用Unity引擎渲染了一个虚拟城市。该数据集由从城市中随机采集的13400张虚拟图像和总共20万帧的4个视频序列组成。为13个类提供了像素级语义注释。
  Richter et al.(2016)从商业视频游戏《侠盗飞车5》中提取图像像素精确语义标签映射，为了实现这一目标，他们开发了一种在游戏和图形硬件之间运行的包装器，以跨时间和实例获取像素精确的对象标签。这个包装器可以让他们在不需要人工监督的情况下，为2.5万张图片合成出密集的语义注释。然而，由于法律原因，提取的三维几何图形不能公开。同样地，邱与余乐(2016)通过访问和修改虚幻引擎的内部数据结构，提供了一个开源工具来创建虚拟世界。他们展示了如何通过将虚拟世界与Caffe Jia等人(2014)的深度学习框架链接起来来测试深度学习算法。

## 3 相机模型和校准

### 3.1 校准

多传感器包括里程传感器、距离传感器和不同类型的摄像机如广泛应用于汽车环境中的透视相机和鱼眼相机。标定是对这些传感器的内、外参数进行估计，使二维图像点与三维世界点相关联，并在多个传感器的情况下表示共同坐标系中的感知信息。棋盘上的基准标记是校准的标准工具。几乎所有系统都使用它们进行初始化或联合优化，以改进内部特性。重投影误差是指投影点与实测点之间的像素距离，是定量测量精度的一种方法。在需要三维推理的驾驶员辅助应用中，校准的准确性是一个关键问题，因此在自动驾驶汽车的安全性方面也是如此。除了精确度之外，校准系统的其他要求还包括速度、对各种成像条件的鲁棒性、完全自动化、在假设方面的最小限制，如重叠视场或所需信息(如参数的初始猜测)。
  现代系统为不同的目的配备了多个传感器。Geiger等人(2012c)使用了一个包括两个摄像头和一个测距传感器的装置，比如Kinect或Velodyne激光扫描仪。他们提出了两种算法，一种是相机对相机，另一种是使用每个传感器的单个图像进行相机对距离校准。它们为传感器假设了一个共同的视场，这对于产生立体图像或场景流地面真值等应用特别有用。Heng et al.(2013)和Heng et al.(2015)在不考虑重叠视场的情况下，解决了一个多摄像机系统的内部和外部自动校准问题。Heng等人(2015)提出了Heng等人(2013)的改进版本。Geiger等人(2012c)在每次运行前都要求重新校准系统的基准标记，但他们不需要使用地图和自然特征来修改基础设施。他们首先建立一个校准区域的地图，然后使用这个地图和基于图像的地理定位进行校准。与基于slam的自校准方法相比，基于图像的定位消除了在不同相机之间进行详尽特征匹配和约束调整的负担。

### 3.2 全向相机

 在自动驾驶中，为了获得关于周围区域的最大信息以便安全导航，需要一个全景视野。一种360度视野的全向相机，通过消除对更多摄像机或机械可旋转摄像机的需求，提供了增强的覆盖。有不同类型的全向摄像机，它们的视野可以覆盖整个半球甚至整个球体。反射折射照相机将标准相机与形状镜子(如抛物面镜、双曲镜或椭圆镜)结合起来，而折射照相机则使用纯折射鱼眼透镜。多折射摄像机使用多个具有重叠视场的摄像机来提供完整的球形视场。
文献中常用的一种全向相机的分类是基于投影中心:中心和非中心。在中央摄像机中，到达被观察对象的光线在三维中相交于一个点上，这被称为单有效视点特性。这种特性允许从全向摄像机捕捉的图像生成几何正确的透视图，从而应用于任何中央摄像机。通过选择镜面形状和摄像机与镜面之间的距离，建立了中心反射式摄像机。
  与针孔相机相比，全向相机的标定由于失真度高，无法用直线投影的方式进行建模。模型应该考虑镜面在反射折射相机的条件下的反射，或者鱼眼相机的镜头引起的折射。Geyer & Daniilidis(2000)为所有中央折射系统提供了统一的理论，在文献中被称为统一投影模型，广泛应用于不同的校准工具箱(Mei & Rives (2007);Heng等(2013,2015))。他们证明了每一个投影，无论是标准视角还是利用双曲反射镜、抛物面镜或椭圆镜，可以用投影映射从球面到投影中心位于球面直径和垂直于球面的平面进行建模。Scaramuzza和Martinelli(2006)提出用泰勒级数展开对成像函数进行建模，泰勒级数展开的程和系数是待估计的参数。三阶或四阶多项式能够准确地建模所有的反射摄像机和多种鱼眼摄像机。Mei & Rives(2007)在统一项目基础上进行改进，通过对具有良好识别参数的失真模型进行建模来考虑现实世界中的错误。
  尽管单视点特性是理想的，但由于可变焦距透镜和精确对准的困难，它在实践中经常被违反。但是，作为备选方案的非中心模型需要大量计算，因此不适合实时应用程序。Schonbein等人(2014)扩展了一种非中心的方法，以精确地获得观察光线的朝向，然后提出了一种快速的中心近似方法，用映射来匹配获得的朝向。这种方法在超反射光摄像机上进行了测试，实现了比中心模型更低的重投影误差(geyer&daniilidis, 2000);Scaramuzza和Martinelli (2006);Mei & Rives(2007))和非中心模型相比，速度快得多。
  应用：全向摄像头在自动驾驶中的应用越来越广泛。对于基于特征的应用，如导航，运动估计和建图，大视场允许从汽车周围提取和匹配感兴趣的点。例如，在进行目视测程或同步定位与建图(SLAM)时，全向特征匹配可以显著改善旋转估计。Scaramuzza & Siegwart(2008)通过对地面平面使用基于单应性的跟踪器和基于外观的跟踪器来估计车辆相对于道路的自我运动。三维感知还受益于全向传感器提供的统一视图，尽管有效分辨率有限，导致噪声重建。基于激光的解决方案作为一种替代方案，只提供稀疏的点云，没有颜色，且成本成本很高，并受到滚动快门的影响。Schonbein & Geiger(2014)提出了一种利用基于平面先验的方法，在一个统一的全向空间中，从两个时空相邻的全向视图联合优化视差估计，实现三维重建。Hane等人(2014)将鱼眼相机的统一投影模型直接加入到鱼眼立体匹配算法中，扩展了鱼眼相机的扫描立体匹配。这种方法允许使用GPU直接从鱼眼图像实时生成密集深度地图，并为具有大视场的密集3D重建开辟了道路。

### 3.3 事件相机

与传统的基于帧速率恒定的成像设备相反，基于事件的传感器最近才被引入。当亮度变化超过预先定义的阈值(动态视觉传感器)时(如图4所示)，它们以微秒的分辨率生成异步事件流。事件包含更改的位置、符号和精确时间戳。这种数据本质上是稀疏的，从而减少了传输和处理中的冗余。另一个优点是高时间分辨率，允许设计高反应性系统。这些特性，即低延迟和低带宽要求，使得基于事件的传感器对于自动驾驶来说很感兴趣。然而，标准的计算机视觉算法并不能直接应用于基于事件的视觉传感器的输出，这与强度图像有着本质上的区别。事件发生的频率很高，而且每个事件本身并没有携带足够的信息。一个简单的解决方案是通过在固定的时间间隔内累积事件来生成强度图像，但是这种由事件到帧的转换引入了一些延迟，并阻碍了高时间分辨率带来的效率。

![img](https://img-blog.csdnimg.cn/20190106160829477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图4:(a)标准CMOS摄像机以固定帧率(蓝色)发送图像，而动态视觉传感器(DVS)在出现时发送脉冲事件(红色)。每个事件对应一个局部的像素级亮度变化。(b)观察旋转点的DVS输出的可视化。有颜色的点代表单独事件。非螺旋的事件是由传感器噪声引起的。改编自Mueggler等人(2015b)。
  相反，算法应该理想地利用事件生成的高速率。因此，近年来出现了几种利用传感器的高时间分辨率和异步特性来解决自主视觉中不同问题的方法。这种算法的设计目标是，每个传入事件可以异步改变估计状态，从而尊重传感器基于事件的特性，并允许在高度动态的场景中进行感知和状态估计。对于轨迹估计，Mueggler等人(2015b)提出了一个连续时间模型，作为平滑参数模型描述的姿态轨迹的自然表示。Rebecq等人(2016)提出了一种基于事件的三维重建算法，以产生在CPU上实时运行的并行跟踪和映射管道。基于事件的SLAM不受运动模糊的影响，因为高速运动和非常高的动态范围场景对于标准相机来说是一个挑战。
  生命周期评估:除了能够为需要低延迟和高帧率的现有问题提供新的解决方案之外，基于事件的传感器还会产生新的问题。其中一个问题是通过建模活动事件集来估计事件的生命周期。只要由时间引起的亮度梯度在像素级别上可见，就认为事件是活动的。活动事件的显式建模可用于在任何时间点生成清晰的梯度图像，或用于跟踪多个对象时对事件进行聚类。对于这项任务，Mueggler等人(2015a)建议使用基于事件的光流，并可选择正则化，不依赖于时间窗口。

## 4 表征

计算机视觉文献中使用了各种不同粒度级别的表示。变量或参数可以直接与图像中的二维像素相关联，也可以描述三维空间中的高级基本物体。在基于像素的表示中，每个像素都是一个单独的实体，例如图形模型中的一个随机变量。像素是最细粒度的表现形式之一，但很难与我们的3D世界的物理属性联系起来。此外，由于高分辨率图像中的变量数量巨大，基于像素的表示增加了推理算法的复杂性。因此，许多方法只对像素之间的局部交互进行建模，这些交互不能很好地捕捉我们世界的结构，以克服计算机视觉正试图解决的病态逆问题中的所有歧义。
  **超像素**:因此，基于像素分组的紧凑表示，即超像素，已经得到广泛的应用。通过将图像分割成颜色和纹理理想相似的原子区域，并尊重图像边界，得到基于超像素的表示(Ren & Malik, 2003);Achanta等(2012);Li & Chen(2015))。每一种基于超像素的方法都有一个隐含的假设，即在一个超像素内，某些感兴趣的属性保持不变，例如语义类标签或表面的倾斜。然而，与这些属性相关的边界依附性很容易被打破，特别是当依赖于利用颜色或强度信息的标准分割算法去处理复杂的图像时。

如果可用，深度信息可以作为精确的超像素提取的有价值的特性(Badino et al.(2009));山口等人(2014)。超像素用作各种任务的构建模块，如立体视觉和流估计(Yamaguchi et al. (2012, 2013, 2014);Guney & Geiger (2015);Bai等人(2016);Menze等人(2015b);Lv等人(2016)，语义分割(xiao&quan (2009);Wegner等(2013)，场景理解(Ess等(2009b);Liu等(2014)和3D重构(Schonbein等(2014))。在包含几何推理(如立体估计)的情况下，超像素通常表示三维平面分段。当目标是在场景流或光流中表示具有独立对象运动的真实场景时，超像素可以被推广到刚性运动的片段(Vogel等人(2015);Menze & Geiger(2015)，或语义片段(Bai et al.， 2016);Sevilla-Lara等人(2016)。
  **Stixels**: Stixels是3D交通场景的中等水平表示，目的是弥合像素和对象之间的差距(Badino et al.(2009))。所谓的“Stixel World”来源于观察到车辆前方的可通行区域大多受垂直表面的限制。Stixels是由一组垂直立在地面上的矩形棍来表示，以近似这些表面。假设宽度不变，每个stixel都由它相对于相机的3D位置和高度来定义。主要目标是通过紧凑、完整、稳定和健壮的表征来提高效率。此外，Stixel表征提供了对场景中可通行区域和障碍的编码。

Badino等人(2009)使用SGM Hirschmuller(2008)的深度图作为输入，使用基于占用网格的动态规划来计算可通行区域(确定Stixels的较低位置)和视差图上的前景/背景分割(计算Stixels的高度)。Pfeiffer & Franke(2011)将Badino等人(2009)扩展到一个统一的概率方案。他们解除了Stixels的限制，以接触地面，并允许多个Stixels沿着一个图像列。通过这种方式，对象可以位于单个图像列的多个深度(图5)。

![img](https://img-blog.csdnimg.cn/20190106161331117.png)

图5:Pfeiffer & Franke的多层Stixel World表示(2011)。场景被分割成被称为“Stixels”的平面片段。与Badino等人(2009)的Stixel世界不同的是，物体被允许位于一个图像列的多个深度。颜色代表到障碍物的距离，红色很近，绿色很远。改编自Pfeiffer & Franke (2011)

  Pfeiffer & Franke(2010)通过使用6D卡尔曼滤波框架和光流作为输入跟踪stixels，将Stixel世界的表征扩展到动态场景。Erbs et al.(2012, 2013)提出了一个基于动态Stixel World表征的交通场景分割CRF框架。Gunyel等人(2012)指出，stixels的运动估计可以简化为一维问题，可以通过二维动态规划有效地解决，避免了昂贵的密集光流计算。
  Levi等人(2015)候选区域使用一个叫做StixelNet的CNN来学习从图像中提取每个StixelNet的脚点。Cordts等人(2014)提出将自顶向下的物体级别的线索融入到自底向上的Stixel表示中，采用概率方法。为了实现这一目标，他们利用了三种不同的目标探测器(即行人、车辆和护栏)输出的概率图像。Schneider等人(2016)提出了一种语义点素表示，通过密集的视差图和像素级的语义场景标记，共同推断场景的语义和几何布局。
  **3D基元**:三维几何基元的使用在三维重建中非常普遍，尤其是在城市重建时。具有几何意义的原子区域使城市物体的形状得到更好的保护。此外，简化的几何假设可以提供显著的加速，以及一个紧凑的模型。在Cornelis等人(2008)中，3D城市模型由立面和道路的规则面组成。Duan & Lafarge(2016)使用多边形与高程估计，根据成对的卫星图像进行城市三维建模。de Oliveira等人(2016)随着时间的推移更新大尺度多边形列表，以从3D范围测量中增加场景表示。Lafarge等人(2010)使用3D模块库来重建不同屋顶形式的建筑。Lafarge & Mallet (2012);Lafarge等人(2013)使用3d基本物体(如平面、圆柱体、球体或圆锥)来描述场景的规则结构。Dube等(2016)将点云分割为不同的元素，用于基于3D片段匹配的环闭检测算法。

## 5 目标检测

可靠的物体检测是实现自动驾驶的关键要求。由于汽车与许多交通方式共同使用道路，特别是在城市地区，因此需要了解其他交通参与者或障碍物，以避免可能危及生命的事故。在城市地区的检测十分困难，因为由其他物体或感兴趣的物体本身引起的物体外观和遮挡物的种类繁多。此外，物体彼此间或者与背景相似及投影或反射等物理效应会使区分变得困难。

**传感器**：物体检测任务可以通过各种不同的传感器来解决。摄像机是用于检测物体的最便宜和最常用的传感器类型。可见光谱（VS）通常用于白天检测，而红外光谱可用于夜间检测。热红外（TIR）摄像机捕获相对温度，可以区分行人等温暖物体与植被或道路等寒冷物体。发射信号并观察其反射的有源传感器，如激光扫描仪，可以提供范围信息，有助于检测目标并对其三维定位。根据天气条件或材料特性，仅依靠单一类型的传感器可能会有问题。VS相机和激光扫描仪受到反射或透明表面的影响，而热物体（如发动机）或温暖的温度会影响TIR相机。来自不同传感器的信息通过传感器融合的组合允许这种补充信息的稳健集成。（Enzweiler＆Gavrila（2011）;Chen等人（2016b）;González等人（2016））

**标准流程**：传统的检测流程包括以下步骤：预处理，感兴趣区域提取（ROI），对象分类和验证/细化。在预处理步骤中，通常执行诸如曝光和增益调整以及相机校准和图像校正之类的任务。一些方法利用联合检测和跟踪系统来获得时间信息。我们在第9节中详细介绍了跟踪问题。
可以使用滑动窗口方法来提取感兴趣的区域，该滑动窗口方法使检测器在不同比例的图像上移动。由于全面的搜索成本很高，因此已经提出了几种用于减少搜索空间的算法。通常，通过假设候选边界框的特定比率，大小和位置来减少评估的数量。除此之外，可以利用图像特征，立体图像或光流来集中搜索相关区域。Broggi等人（2000）使用人形的形态特征（大小，比例和形状）和垂直对称来过滤行人候选人。此外，他们在算法的ROI提取和细化步骤中利用从立体视觉获得的距离信息。选择性搜索（Uijlingsetal）（2013））是产生感兴趣区域的另一种方法。他们利用分段来有效地提取近似位置，而不是在整个图像域上执行穷举搜索。

在他们对单目图像行人检测系统的调查中，Dollar等人（2011）提出了一个广泛的评估，重点是滑动窗口方法的评估。他们声称这些方法对于低到中分辨率检测最有希望，但发现低分辨率输入和遮挡的检测仍然是所考虑的方法的问题。
  **分类**：由于需要分类的图像区域太多，使用滑动窗口方法对图像中的所有候选者进行分类可能变得成本很高。因此，需要快速决定丢弃图像的背景区域中的候选区域。Viola等人（2005）结合简单有效的分类器，使用AdaBoost学习，在一个级联中，允许快速丢弃错误的候选区域，同时花费更多的时间在有可能的区域。

随着Dalal＆Triggs（2005）的工作，线性支持向量机（SVM）从线性决策边界最大化了所有样本的边界，结合方向直方图（HOG）特征已成为分类的流行工具。但是，以前的所有方法都依赖于难以设计的手工制作的功能。随着深度学习的复兴，卷积神经网络使这项任务自动化，同时显着提高了性能。例如，Sermanet等（2013）将CNN引入行人检测问题，使用无监督卷积稀疏自动编码器预训练特征和端到端监督训练以训练分类器，同时微调特征。今天，我们将在5.1节讨论，从大型数据集中以端到端的方式获取所有的检测方法。
  **基于部分的方法**：学习清晰物体的外观很困难，因为需要考虑所有可能的清晰度。基于部分的方法的想法是将像人类这样的非刚性运动物体的复杂外观分成简单的部分，并用这些部分表示任何清晰度。这提供了更大的灵活性，并减少了学习对象外观所需的培训示例的数量。Felzenszwalb等人的可变形零件模型（DPM）（2008）试图将对象的复杂外观分解为更容易的部分，用于训练具有潜在结构变量的SVM，这些变量代表模型配置并且需要在训练时推断。他们使用覆盖整个对象的粗糙全局模板和更高分辨率的局部模板来模拟每个部分的外观，如图6所示。所有模板都使用HOG。此外，它们推广SVM以处理潜在变量，例如部分姿势位置。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/201901061623137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图6使用Felzenszwalb等人提出的可变形局部模型获得的示例检测。（2008年）DPM包括粗略和多个高分辨率模型以及用于约束每个部分的位置的空间星座模型。改编自Felzenszwalb等。（2008年）。

  这种表示的替代方案是Leibe等人提出的隐式形状模型。（2008a）学习了高度灵活的物体形状表示。它们提取感兴趣点周围的局部特征并执行聚类以构建本地外观的码本，该码本是所考虑的特定对象类的特征。基于该码本，他们了解可能出现的码本条目在何处。
  虽然到目前为止提供的基于局部的模型非常成功，但它们不能表示障碍物推理所必需的背景信息。通常，学习单独的背景模型来处理遮挡，参见Hoiem等人（2008）;涂白（2010）;德赛等人（2011）;杨等人（2012年）。And-Or模型嵌入了一个语法来表示可重构层次结构中的大型结构和外观变化。吴等人（2016a）建议学习一种And-Or模型，该模型考虑了多车，单车和部分级别的结构和外观变化，共同表示背景和遮挡。

### 5.1 2D目标检测

KITTIGeiger等（2012b）是自动驾驶汽车环境中最受欢迎的物体检测系统基准测试之一。行人检测任务的类似流行度有加州理工学院-美国数据集（Dollár等人（2012））。在这项工作中，我们希望将注意力集中在KITTI基准测试上，因为它允许我们在同一数据上比较物体和行人检测系统。我们将感兴趣的读者引用到调查论文（Benenson等人（2014）;Zhang等人（2016b）），以深入比较Caltech-USA的行人检测系统。在表1中，我们展示了KITTI基准测试对象，行人和骑车人从图像中检测的最新技术。请注意，对于本文中的所有结果表，我们仅列出具有与之关联的文章的公共方法，因为匿名条目的详细信息尚未讨论。使用PASCALVOC交叉联合（IOU）评估性能的三个难度级别（Evering-ham等人（2010））。简单的示例具有40px的最小边界框高度并且是完全可见的，而中等示例具有25px的最小高度，包括部分遮挡，并且困难示例具有相同的最小高度但包括最大遮挡水平。在表2中，使用Geiger等人提出的平均取向相似性（AOS）评估物体取向的估计。（2012B）。

卷积神经网络可以显着改善物体探测器的性能。最初，CNN被整合到滑动窗口方法中（Sermanet等人（2013））。然而，由于大的感受野和步幅，物体的精确定位具有挑战性。Girshick等。另一方面，（2014）提出RCNN用“使用区域识别”范例来解决CNN定位问题。他们使用选择性搜索生成许多区域候选区域（Uijlings等人（2013）），使用CNN为每个提案提取固定长度的特征向量，并使用线性SVM对每个区域进行分类。基于区域的CNN在计算上是昂贵的，但已经提出了若干改进以减少计算负担（He等人（2014）;Girshick（2015））。等人。（2014）使用空间金字塔池，其允许仅使用CNN的一次运行来计算整个图像的卷积特征图，而不是需要应用于许多图像区域的R-CNN。Girshick（2015）通过单阶段训练算法进一步改进，该算法共同学习对对象候选区域进行分类并改进其空间位置。尽管这些基于区域的网络已经证明在PASCALVOC基准测试中非常成功，但它们无法在KITTI上实现类似的性能。其主要原因是KITTI数据集包含许多不同尺度的对象和通常严重遮挡或截断的小对象。使用基于区域的神经网络很难检测到这些对象。因此，已经提出了几种获得更好的对象候选区域的方法（Ren等人（2015）;Chen等人（2016b，a）;Yang等人（2016）;Cai等人（2016））。

任等人（2015）引入了区域候选网络（RPN），其中区域候选网络与检测网络共享全图像卷积特征，因此不会增加计算成本。RPN经过端到端的训练，以生成高质量的区域提案，这些提案使用快速R-CNN探测器进行分类（Girshick（2015））。陈等人（2015c）使用从立体相机对估计的3D信息来提取更好的边界框候选区域。他们将3D候选框放置在地平面上，并使用3D点云特征对其进行评分。最后，CNN利用上下文信息并使用多任务丢失共同回归对象的坐标和方向。受这种方法的启发，陈等人。（2016a）学习为单眼图像生成特定于类的3D对象候选区域，利用上下文模型以及语义。他们通过在地平面上详尽地放置3D边界框并使用标准CNN管道对其进行评分来生成对象候选区域（Chenetal。（2015c））。两种方法陈等人（2015c）（2016a）在所有检测任务中获得与最佳性能方法相当的结果，同时在KITTI汽车的简单示例上优于所有其他方法（表1a）。此外，它们是用于方向估计的最佳表现方法之一（表2）。

Yang等人提出了另一种方法。（2016）。在小物体的情况下，卷积神经元的强烈激活更可能发生在较早的层中。因此，杨等人（2016）使用依赖于比例的池化，其允许使用来自相应比例的卷积特征来表示候选边界框。此外，他们提出逐层级联拒绝分类器，将早期层次的卷积特征视为弱分类器，以有效地消除负面对象候选区域。建议的与尺度相关的汇集方法是所有任务中表现最佳的方法之一（表1）。

![img](https://img-blog.csdnimg.cn/20190106162633666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

表2：KITTI检测和方向估计排行榜。在这些表中仅示出了基于图像的方法，即，没有使用激光扫描数据。数字表示Geiger等人描述的平均取向相似性。（2012B）。数字越大表示检测和方向估计越好。

  蔡等人（2016）提出了一种由候选区域子网和检测子网组成的多尺度CNN。如图7所示，候选区域网络在多个输出层执行检测，并且这些互补的比例特定检测器被组合以产生强大的多尺度物体检测器。他们的多尺度CNN优于KITTI行人和自行车手的所有其他方法（表1b，1c），而在KITTI车上排名第二（表1a）。翔等人。（2016）提出了使用从3DVP获得的子类信息的区域候选区域网络（Xiangetal。（2015b）），以指导候选区域生成过程，以及用于联合检测和子类别分类的检测网络。对象子类别是为具有相似属性或属性（如外观，姿势或形状）的对象定义的。子类别信息允许它们优于KITTI汽车上检测任务的所有其他方法（表1a），并在方向估计中实现最佳性能（表2）。
  他们的多尺度CNN优于KITTI行人和自行车手的所有其他方法（表1b，1c），而在KITTI车上排名第二（表1a）翔等人（2016）提出了使用从3DVP获得的子类信息的区域候选区域网络（Xiangetal。（2015b）），以指导候选区域生成过程，以及用于联合检测和子类别分类的检测网络。对象子类别是为具有相似属性或属性（如外观，姿势或形状）的对象定义的。子类别信息允许它们优于KITTI汽车上检测任务的所有其他方法（表1a），并在方向估计中实现最佳性能（表2）。

### 5.2 基于2D图像进行3D目标检测

对象类的几何3D表示可以比仅仅2D或3D边界框恢复更多细节，但是今天的大多数对象检测器都专注于强大的2D匹配。Zia等人（2013）利用高质量3DCAD模型可用于许多重要类别的检测。从这些模型中，他们使用主成分分析和线框顶点的列车检测器获得粗略的3D线框模型。在测试时，他们通过密集应用探测器产生顶点的证据。Zia等人（2015）通过在他们的公式中直接使用详细的3DCAD模型，将它们与可能的遮挡模式的明确表示相结合来扩展这项工作。此外，联合估计地平面以稳定姿势估计过程。该扩展优于Zia等人的伪3D模型。（2013）并展示了真实公制3D空间中推理的好处。
  虽然这些3D表示提供了对物体更忠实的描述，但它们仍然无法与使用2D边界框的最先进的探测器竞争。为了克服这个问题，Pepik等人（2015）提出了强大的可变形局部模型的3D扩展（Felzenszwalb等人（2008）），其将3D几何表示与对现实世界图像的稳健匹配相结合。他们进一步将感兴趣的对象类的3DCAD信息添加为几何提示以丰富外观模型。

### 5.3 基于三维点云的3D目标检测

KITTI数据集Geiger等（2012b）提供同步相机和激光雷达框架，并允许在相同数据上比较基于图像和基于激光雷达的方法。与相机相比，激光雷达测距传感器直接提供精确的3D信息，简化了候选对象的提取，并且在提供3D形状信息时可以有助于分类任务。然而，来自激光扫描仪的3D数据通常是稀疏的并且其空间分辨率是有限的。因此，仅依赖于激光范围数据的现有技术还不能达到基于摄像机的检测系统的性能。在表3中，我们展示了基于激光雷达的KITTI基准，用于物体，行人和骑车人检测的最新技术。通过将3D边界框投影到图像平面中，使用PASCAL交叉结合来评估性能类似于基于图像的方法。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190106162915880.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图8：Chen等人提出的网络。（2016b）结合了鸟瞰图中的区域特征，激光雷达点云的前视图以及RGB图像作为深度融合网络的输入。改编自陈等人。（2016B）。
Wang＆Posner（2015）提出了一种将常用2D滑动窗口检测方法应用于3D数据的有效方案。 更具体地说，他们利用投票方案利用问题的稀疏性来搜索所有可能的对象位置和方向。
  李等人（2016b）通过利用完全卷积神经网络从范围数据中检测车辆来改进这些结果。它们表示2D点图中的数据，并使用单个2D CNN同时预测对象置信度和边界框。用于表示数据的编码允许它们预测车辆的完整3D边界框。 Engelcke等（2016）利用特征中心投票方案来实现一个新的卷积层，利用点云的稀疏性。此外，他们建议使用L 1惩罚进行正则化。
  仅依靠激光范围数据使得检测任务由于激光扫描的有限密度而具有挑战性。 因此，与基于KITTI数据集的基于图像的对应物相比，现有的基于激光雷达的方法执行得更弱。 陈等人（2016c）将激光雷达测距数据与RGB图像结合起来进行物体检测。 在他们的方法中，使用紧凑的多视图表示来编码稀疏点云，并且候选区域生成网络利用点云的鸟瞰图表示来生成3D候选。 最后，他们将来自多个视图的区域特征与深度融合方案相结合，如图8所示。这种方法在很大程度上优于其他基于激光雷达的方法，并在KITTI汽车基准测试中实现了最先进的性能（ 表1a，3a）。

![img](https://img-blog.csdnimg.cn/20190106163016831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

表3：KITTI激光雷达检测排行榜。 介绍了专注于激光雷达扫描的方法以及将激光雷达与RGB图像相结合的方法。 数字表示不同难度级别的平均精度。 数字越大表示性能越好。

### 5.4 行人检测

虽然到目前为止我们已经讨论了一般物体检测算法，但我们现在关注人或行人检测的具体方法，这些方法与任何与真实环境相互作用的自治系统高度相关。 由于人类行为的可预测性低于汽车的行为，因此需要可靠的行人检测才能在行人附近安全驾驶。 由于不同的衣服和铰接姿势导致各种各样的外观，人们的检测特别困难。 此外，在部分遮挡的情况下，行人的清晰度和相互作用可以强烈地影响行人的外观。
  **行人保护系统**：这个问题已经深入研究了先进的驾驶员辅助系统，以提高道路安全性。 行人保护系统（PPS）检测移动车辆周围是否存在静止和移动的人员，以警告驾驶员免受危险情况的影响。
  尽管驾驶员仍然可以处理错过的PPS检测，但是自动驾驶汽车的行人检测需要完美无缺。 行人检测系统需要在所有天气条件下都能够稳健，并且能够有效地进行实时检测。 Geronimo等（2010）调查高级驾驶员辅助系统的行人检测。
  **调查**：Enzweiler和Gavrila（2009）对单目行人探测的不同架构给出了非常广泛的概述。 他们观察到Dalal＆Triggs（2005）提出的HOG / SVM组合在较高分辨率下具有较高的处理时间，而AdaBoost 的ascade方法在较低分辨率下具有优势，可实现接近实时的性能。 在他们的调查中，Benenson等人（2014）没有发现明确的证据表明某种类型的分类器（例如SVM或决策林）比其他分类器更适合。 特别是，Wojek＆Schiele（2008b）表明，如果给出足够的特征，AdaBoost和线性SVM的表现大致相同。 此外，Benenson等人（2014）观察到，与Dalal＆Triggs（2005）的简单方法相比，基于部分的模型（Felzenszwalb等人（2008））仅略微改善了结果。他们得出结论，特征的数量和多样性显然是分类器性能的重要因素，因为分类问题通过更高维度表示变得更容易。 因此，今天所有最先进的行人检测系统都使用卷积神经网络并以端到端的方式学习特征表示（Cai et al。（2016）; Xiang et al。（2016）; Zhu et al （2016）; Yang等人（2016）; Chen等人（2015c）; Ren等人（2015））。
  **时间信息**：同样，Shashua等（2004）指出了人物检测任务的良好特征的重要性。他们指出，随着时间的推移（动态步态，运动视差）和情境特定特征（例如某些姿势的腿部位置）测量的其他线索的整合是可靠检测的关键。Wojek等人（2009）注意到大多数行人检测系统仅依赖于单个图像作为输入，并且不利用视频序列中的对象的可用时间信息。它们通过结合运动提示和组合不同的互补特征类型，显示出检测性能的显着改善。
  **目标类别的稀缺性**：训练数据的扩大允许训练用于检测问题的复杂模型。 然而，由于手动标记，属于目标类的示例的生成通常是耗时的，而可以容易地获得许多负面示例。 Enzweiler和Gavrila（2008）解决了目标类样本稀缺造成的瓶颈问题。
他们使用学习的生成模型创建合成的虚拟样本，以增强判别模型。
生成模型捕获有关行人阶级的先验知识，并允许显着改进分类性能
  **实时行人检测**：在与行人发生潜在碰撞的情况下，快速检测允许自主系统的提前干预。 Benenson等（2012）通过更好的处理尺度和利用立体匹配提取的深度，提供快速和高质量的行人检测。 他们没有调整图像的大小，而是缩放HOG功能，类似于Viola＆Jones（2004）。 Stixel World表示（Badino等人（2009））提供深度信息，允许在并行框架中显著减少搜索空间并检测的行人。

### 5.5 人体姿态估计

人的姿势和注视向自主车辆提供关于人的行为和意图的重要信息。然而，姿势估计问题具有挑战性，因为姿势空间非常大并且通常人们只能在低分辨率下观察，因为它们的尺寸和到车辆的距离。已经提出了几种方法来联合估计人的姿势和身体部位。
传统上，通过首先检测身体部位然后估计姿势来使用两阶段方法（Pishchulin等人（2012）; Gkioxari等人（2014）; Sun＆Savarese（2011））。当人们彼此接近时，这是有问题的，因为身体部位可能被错误地分配给不同的实例。
  Pishchulin等（2016）提出DeepCut方法，这是一个联合估计图像中所有人的姿势的模型。该方法基于对从基于CNN的部分检测器获得的一组体部假设进行分区和标记。 该模型共同推断出人数，姿势，空间接近度和部分水平遮挡。 Bogo等（2016）使用DeepCut从单个无约束图像估计人体的3D姿势和3D形状。 SMPL，Loper等人提出的3D体形模型（2015），适用于DeepCut对2D身体关节位置的预测。 SMPL捕获人群中人体形状的相关性，即使在存在弱观察的情况下也能够稳健地拟合人体姿势。

### 5.6 讨论

在具有很少遮挡的高分辨率的情况下，物体检测已经很好地工作。对于汽车检测任务的简单和适度情况（表1a），许多方法都表现出令人印象深刻的性能。 行人和骑车人检测任务（表1b，1c）更具挑战性，因此可以观察到较弱的整体性能。 造成这种情况的一个原因是训练样本的数量有限，并且可能会使骑车人和行人混淆，这些人只能通过他们的背景和语义来区分。
  剩余主要问题是检测小对象和高度遮挡的对象。 在排行榜中，当比较简单、中等和困难的例子时，这表现出性能的显著下降。定性地，这可以在图9,10,11中观察到，其中我们示出了在KITTI数据集上表现最佳的方法的典型估计误差。
  造成错误的主要原因是行人群，骑车人群和汽车线导致许多遮挡并导致所有方法都缺失检测。此外，在某些情况下需要检测大量的远处物体，这对于现代方法来说仍然是一项具有挑战性的任务，因为这些目标所提供的信息量非常低。

![img](https://img-blog.csdnimg.cn/20190106163249150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图9：KITTI车辆检测分析。每幅图分别显示具有大量真阳性（TP）检测，假阳性（FP）检测和假阴性（FN）检测的图像。如果所有检测器都同意TP，FP或FN，则对象标记为红色。 如果只有一些探测器同意，则对象标记为黄色。 通过考虑提交时在KITTI评估服务器上发布的15种主要方法，建立了排名。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190106163324928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图11：KITTI骑车人检测分析。 每幅图分别显示具有大量真阳性（TP）检测，假阳性（FP）检测和假阴性（FN）检测的图像。 如果所有检测器都同意TP，FP或FN，则对象标记为红色。 如果只有一些探测器同意，则对象标记为黄色。 通过考虑提交时在KITTI评估服务器上发布的15种主要方法，建立了排名。

![img](https://img-blog.csdnimg.cn/201901061634453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图12：Cordts等人从Cityscapes数据集中对场景进行语义分割。（2016年）在苏黎世录制。



## 6 语义分割

语义分割是计算机视觉研究的一个重要课题。语义分割的目标是为图像中的每个像素分配一个来自预定义的一组类别的标签。该任务在图12中示出，其中在Cordts等人的Cityscapes数据集25的场景中，特定类别的所有像素被作为特定颜色着色。将图像分割成语义区域，通常出现在街道场景中，例如汽车，行人或道路，这样可以全面了解周围环境，这对自主导航至关重要。语义分割的挑战主要来自于场景的复杂性和标签空间的大小。

**公式化**：传统上来讲，语义分割问题被看做条件随机场(CRF)中的最大后验推理(MAP)，它被定义在像素或超像素之上(He等人， 2004, 2006)。然而，这些早期的公式效率不高，只能处理有限大小和少量类别的数据集。此外，只有一些非常简单的特征，诸如颜色，边缘和纹理信息等被开发利用。Shotton等人(2009)观察到更加有效的特征可以显著提高性能，并提出了一种基于一种新型特征的方法，称为“纹理布局滤波器”。该方法可以利用对象的纹理外观、纹理布局和纹理上下文。它们将纹理布局滤波器与CRF中的低级图像特征相结合，以获得像素级分割。利用随机增强和分段训练技术来有效地训练模型。
  定义在图像区域上的层次连接和远程连接以及高阶潜在信息可以解决CRF在图像内建立远距离相互作用模型的局限性。然而，基于图像区域的方法（He等人（2004）; Kumar＆Hebert（2005）; He等人（2006）; Kohli等人（2009）; Ladicky 等人l（2009,2014））受到用作输入的图像分割的准确性的限制。相比之下,Krahenbuhl & Koltun(2011)提出一个高效推理算法，用于完全连接的CRF模型，该模型模拟图像中所有像素对之间的成对潜在信息。
  到目前为止，这些方法独立地考虑每个对象类，而对象类的共同出现是语义分割的重要线索，例如汽车更有可能发生在街道场景中而不是在办公室中。因此，Ladicky等人（2010）提出将对象类共生现象作为一种全局潜在信息应用在CRF中。 他们展示了如何使用图割算法有效地优化这些潜在信息，并展示了对简单成对模型的改进。
  深度卷积神经网络在图像分类和目标检测方面的成功，激发了人们利用其功能解决像素级语义分割任务的兴趣。完全卷积神经网络(Long等人，2015)是最早将CNNs应用于图像分割问题的工作之一。然而，虽然用于图像分类的现代卷积神经网络通过连续池化和降低分辨率的子采样层来组合多尺度上下文信息，但是语义分割需要多尺度上下文推理以及全分辨率密集预测。在接下来的内容中，我们将回顾最近解决这个问题的方法。
  基于自动驾驶的背景，我们重点关注不同语义分割方法在Cordts等人的Cityscapes数据集26上的比较。表4a显示了用于像素级语义标记任务的Cityscapes的排行榜。针对两个语义粒度（即，类和类别）要提供交叉 - 聚合度量，并且除此之外针对两个粒度要报告实例加权的IoU以惩罚忽略小实例的方法。
  **结构化CNN**：最近，针对多尺度推理和全分辨率预测输出的需求提出了几种方法。已经被提出的空洞卷积方法（Chen等人（2015b）; Yu＆Koltun（2016））可以扩大神经网络的感受域而不损失分辨率。 它们的操作对应于具有空洞滤波器的常规卷积，这种方法允许有效的多尺度推理，但同时会限制模型参数数量的增加。
  在SegNet模型中，Badrinarayanan等人(2015)用一个网络取代了深层体系结构中的传统解码器，该网络由一系列解码器组成，每个解码器对应一个编码器。每个解码器将编码器(最大池化层)的低分辨率特征映射映射到更高分辨率的特征映射。特别是，其模型中的解码器利用相应编码器的最大池化步骤中计算的池化指数来实现上采样过程。这样就不需要学习向上采样，从而导致更少的参数。此外，使用这种方法已经证明了更清晰的分割边界。
  虽然CNN层次结构较低级别的激活映射缺少对象类别特异性，但它们确实包含更高的空间分辨率信息。Ghiasi＆Fowlkes（2016）利用这一假设提出了基于完全卷积网络构建的拉普拉斯金字塔。在多个尺度上聚合信息，使他们能够从低分辨率的层次上依次细化重构的边界。他们通过使用来自更高分辨率特征图的跳过连接和乘法置信门控来实现这一点，在低分辨率预测具有高置信度的区域中惩罚噪声高分辨率输出。通过这种方法，Ghiasi＆Fowlkes（2016）在Cityscapes表4a中取得了有竞争力的结果。

![img](https://img-blog.csdnimg.cn/20190114215028457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

表4：CITYSCAPES语义和实例分割排行榜。 分割性能通过类交叉结合和实例级交叉结合来测量。 实例检测性能是根据几种平均精度变量来测量的。 另见Cordts等（2016）。

![img](https://img-blog.csdnimg.cn/20190114215059619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图13：Zhao等人（2016）提出的方法概述。 金字塔解析模块（c）应用于CNN特征图（b）并馈送到卷积层以进行像素级估计（d）。 改编自Zhao等（2016）。

  Zhao等人(2016)利用金字塔场景解析网络(如图13所示)提出了一种Cityscapes上表现最佳的方法，将全局上下文信息整合到像素级的预测任务中。具体来说，他们将金字塔解析模块应用于CNN的最后一个卷积层，该层融合了多个金字塔级别的特性，以结合本地和全局上下文信息。得到的表示被输入到卷积层以获得最终的像素预测。
  Simonyan＆Zisserman（2015）和Szegedy等人（2015）已​​经证明，CNN的深度对于表示丰富的特征至关重要。然而，增加网络的深度会导致精度的饱和与降低。He等人（2016）提出深度剩余学习框架（ResNet）来解决这个问题。他们让每个堆叠层学习残差映射，而不是原始的、未引用的映射。这使得他们能够训练更深层的网络，提高准确率，而普通网络(简单堆叠网络)表现出更高的训练误差。Pohlen等人（2016）提出一种类似ResNet的架构，在提供强大的识别性能的同时，通过结合两个不同的处理流，在整个网络中保留高分辨率信息。一个流通过一系列池化层，而另一个流以全图像分辨率处理特征图。使用残差以全图像分辨率组合两个处理流。吴等人（2016b）通过对目标深度的分析，提出了一种更有效的ResNet架构。他们指出，ResNets表现为浅层网络的线性组合。在此基础上，他们设计了一组相对较浅的卷积网络来完成语义图像分割的任务。Pohlen等人(2016)在Cityscapes上(表4a)取得了有竞争力的成果，Wu等人(2016b)在实例加权类水平IoU以外的所有指标上都优于其他。
  **条件随机场**：满足多尺度推理和全分辨率预测需求的另一种方法是CNN与CRF模型的组合。 陈等人（2015b）提出使用完全连接的CRF模型（Krahenbuhl＆Koltun（2011））的卷积神经网络来获得完善的标签地图。CRF允许基于原始的RGB输入捕获细节，而由于CNN模型的空间精度有限，CNN输出中缺少原始的RGB输入。类似地，Jampani等人(2016)推广了双边滤波器，并分解CRF程序，该程序允许从数据中对(广义的)过滤器参数进行端到端训练。通过利用输入特性作为指导信号，这有效地实现了在一个卷积层中对更大的空间区域进行推理。
  受到用于语义分割的更高阶CRF的启发，Gadde等人（2016a）提出了用于CNN架构的新的双边初始模块，用于替代结构化CNN和CRF技术。 他们使用的假设是空间和光度相似的像素更可能具有相同的标签。这使他们可以直接学习远距离的相互作用，从而无需使用CRF模型进行后处理。具体来说，所提出的模块基于其空间和颜色相似性在远距离像素之间传播边缘感知信息，并结合超像素的空间布局。
通过应用具有不同尺度的高斯核的双边滤波器来实现信息的传播。
  **讨论**:对最近的方法的多尺度推理的关注导致了Cityscapes上的像素级语义分割的令人印象深刻的结果。今天，在Cityscapes表4b中排名靠前的方法在类别和类别上的借据接近81%和91%，令人印象深刻。这表明语义分割适用于覆盖大图像区域的实例，但对于覆盖小区域的实例仍然存在问题。与第5.6节中讨论的低分辨率检测类似，小区域仅提供很少的信息来分配正确的标签。分割出小的，可能被遮挡的物体是一项挑战性的任务，可能需要新颖的方法来联合进行深度估计和深度自适应识别。

### 6.1 语义实例分割

语义实例分割的目标是同时检测、分割和分类图像中的每个对象。与语义分割不同，它提供了关于单个对象的位置、语义、形状和计数的信息，因此在非紧凑型驱动中有许多应用。对于语义实例分割的任务，存在两大研究方向:基于候选区域的和无候选区域的实例分割。
  在表4b中，我们展示了Cityscapes数据集中语义实例分割方法的排行榜。对性能的评估是通过跨重叠阈值(AP)的区域水平的平均精度进行的，对重叠值为50％（AP 50％），以及100米和50米(AP 100m, AP 50m)以内的对象的平均精度进行的。
  **基于候选区域的实例分割**:基于候选区域的实例分割方法提取类无关的建议，将其分类为某个语义类的实例，以获得像素级的实例掩码。地区建议如多尺度组合分组(阿贝拉´ez et al .(2014))可以直接用作实例部分。较粗的表示符(例如绑定框)需要进一步细化以获得实例掩码。遗憾的是，基于候选区域的算法在推理时速度较慢，这是由于候选区域生成步骤的计算代价高昂。为了避免这个瓶颈，Dai等人(2016)提出了一个完全卷积网络，分为三个阶段。他们提取方框提案，使用共享特性将其细化到各个部分，最后将其分类为语义类别。各阶段输出之间的因果关系使得多任务级联训练更加复杂。然而，作者展示了如何使用不同的可调整层来克服这些困难，该层允许以端到端方式训练整个模型。
  基于候选区域的实例分割方法使用边界框的形式来预测二进制分割掩码，对候选区域生成过程中的错误非常敏感，包括错误缩放或移位的边界框。为了解决这个问题，Hayder等人(2016)提出了一种新的对象表示。更具体地说，他们提出了一个形状感知对象掩码网络，该网络为每个候选区域预测一个二进制掩码，潜在地扩展到盒子本身之外。他们通过替换原有的掩码预测阶段，将对象掩码网络集成到Dai等人(2016)的多任务网络级联框架中。这个形状感知方法是Cityscapes的第二佳表现方法(表4b)。
  **无候选区域实例分割**：最近，在文献中已经提出了许多基于候选区域的实例分割的替代方法。 这些方法通过直接将实例分割作为像素标注任务来联合推断单个实例的分割和语义类别。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190114215334272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图14：Uhrig等人（2016）从输入图像预测语义，深度和实例中心方向，以计算语义类别的模板匹配分数图。在生成实例候选区域以获得实例分段后，他们将它们融合。
改编自Uhrig等人（2016年）。

  Zhang等(2015,2016c)训练完全卷积神经网络(FCN)，在实例ID编码深度排序的同时，直接预测像素级实例分割。它们改进了预测，并加强了与随后的马尔可夫随机场的一致性。Uhrig等人(2016)提出了一种基于FCN的方法，可以联合预测语义分割、深度以及相对于每个实例的质心的实例的方向。实例分割传递途径如图14所示。然而，他们需要真实值深度数据来训练他们的模型。Kirillov等(2016)提出了一种将语义分割和通过全局推理检测对象边界相结合的多切割公式来推断语义实例分割的无候选区域方法。Bai & Urtasun(2016)将来自经典流域变换和深度学习的直觉相结合，创建了一个能量图，其中盆地对应于对象实例。这允许它们在单个能量级别上分割以获得像素级别的实例分割。Kirillov等人(2016)和Bai &
  Urtasun(2016)在Cityscape上都取得了有竞争力的成绩(表4b)。然而，Arnab & Torr(2017)通过在一个实例子网络中提供初始语义分割，超过了其他所有人。具体来说，最初的分类级分割是根据端到端CRF中对象检测器输出的线索来预测像素级实例的。
  **讨论**:实例分割任务要比语义分割任务困难得多。每个实例都需要单独注释，而在一个语义类的无记名分割组中，当它们相邻出现时，可以一起注释。此外，在不同的图像之间，实例的数量有很大的不同。在自动驾驶的背景下，通常有一个广阔的视野。因此，在图像中出现的大量实例都非常小，这使得它们很难被检测到。与5.6节中讨论的边框不同，在这个任务中需要推断每个对象实例的确切形状。由于这些原因，目前在Cityscape数据集(表4b)上的平均精度仍然不足20%。

### 6.2 标签传播

使用高度精确的像素级注释创建大型图像数据集需要大量的劳动量，因此获取所需的质量的数据集成本很高。视频序列注释的半监督方法有助于降低这种成本。与注释单个图像相比，视频序列提供了连续帧间时间一致性的优势。标签传播技术利用了这一事实，根据颜色信息和运动估计，将注释从一小组带注释的关键帧传播到所有未标记的帧。
  为了实现这个目标，Badrinarayanan等人(2010)提出了一种耦合贝叶斯网络，用于图像序列和像素级标签的联合建模。具体来说，它们采用基于从基于图像块的相似性和语义上一致的区域获得的对应关系的传播方案，以将标签信息传送到注释关键帧之间的未标记帧。
  Budvytis等人(2010)扩展了这一方法，在Badrinarayanan等人引入的标签传播模型的基础上，提出了一种混合模型，以及一个区分分类阶段，在该阶段解决遮挡和非遮挡问题，并允许在更长的时间范围内传播。为了纠正标签错误传播，  Badrinarayanan等人(2014)提出了一种基于超像素的树混合模型用于寻找短时间内的相关性。Vijayanarasimhan & Grauman(2012)解决了手工标记中选择最有潜力的关键帧的问题，使预期的传播误差最小化。
  而上述方法在2D中传递注释，Chen等人(2014);Xie等人(2016)提出直接以3D方式标注，然后将这些注释转移到图像域。在给定3D信息的来源(例如立体图像、激光)时，这些方法能够提高语义准确性和产生时间一致性标签，同时限制注释成本。为了实现这一目标，Chen等人(2014)利用KITTI (Geiger等人(2013))的注释，利用3D汽车CAD模型，对图像中所有汽车分别推断出图形-地面分段。相比之下，Xie等人(2016)联合推理场景中所有对象，并处理无法获得CAD模型或3D点测量的类别。为此，他们提出了一个非局部CRF模型，该模型联合考虑了图像中所有3D点和像素的语义标签和实例标签。

### 6.3 多帧语义分割

自主车辆等可移动平台的语义分割由于自主系统对周围环境的识别的需要，已成为研究的热点。由于这类系统通常配有摄像机，因此可以利用相邻帧之间的时间相关性来提高分割精度、效率和鲁棒性。
  为了实现这个目标，Floros& Leibe(2012)提出了基于视频序列的图形模型，以加强帧间的时间一致性。具体来说，他们提出了一种CRF，通过将相应的图像像素链接到从运动结构(SfM)中得到的推断出的3D场景点，从而保证连续视频帧之间的时间一致性。与仅使用图像的基础方法相比，它们实现了更好的分割性能，并在不同的图像条件下观察到良好的泛化能力。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190114215554141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

###  6.4 三维数据的语义分割

自动驾驶系统需要识别其周围环境以识别感兴趣的对象并与之交互。 虽然已经广泛研究了语义对象标记的问题，但是这些算法中的大多数在2D图像域中工作，其中图像中的每个像素被标记有诸如汽车，道路或路面的语义类别。 然而，2D图像缺少重要信息，诸如对象的3D形状和比例，这是对象类分割的重要线索并且便于单个对象实例的检测和分离。
  Sengupta等人(2012)提出了一种从街道水平图像生成城市场景的语义头顶地图的方法。他们使用两个CRF来描述问题。第一种方法是对街景图像进行语义分割，分别对每一幅图像进行处理。然后，每个街景图像通过一个几何函数进行关联，该函数将一个区域从图像投影到头顶的地图上。然后，这个阶段的输出聚合到许多图像上，形成第二个CRF的输入，产生一个地面图的标记。然而，他们的方法并没有超出平面世界的假设，即使用多个街景图像进行密集的语义重建。
  为了实现这一目标，Sengupta等人(2013)提出了一种方法，如图15所示，使用多个街景图像生成密集的语义3D重构。他们使用视觉测程法进行自我运动估计，根据输入立体图像对生成的深度图进行融合。这使得他们能够生成场景的立体再现。同时，使用CRF模型对输入图像进行半分类。然后将分割结果跨序列聚合，生成最终的3D语义模型。但是，对象标记是在图像域中进行的，然后投影到模型上。因此，这些方法不能充分利用道路场景中存在的所有结构约束。
V  alentin等人(2013)通过结合结构和外观线索解决了三维空间中语义场景重构问题。他们使用输入深度估计来生成场景的三角网格表示，并应用级联分类器从网格和图像中获取几何线索。随后，他们通过在场景网格上定义CRF来解决3D标签问题。然而，它们的方法需要对整个网格进行推断，并且不允许在自动驾驶环境中的在线设置中逐步添加信息。
  Hackel等人(2016)提出了一种快速的三维点云语义分割方法。他们通过对整个点云进行下采样，生成密度递减的多尺度金字塔，并在每尺度上寻找最近的邻域，从而构建了近似的多尺度邻域。该方案允许提取丰富的特征表示，在很短的时间内捕获点的局部邻域中的几何形状，例如粗糙度，表面方向，地面高度等。随机森林分类器可以预测类条件概率。该方法可以在几分钟内处理上百万个点的点云。
  **在线方法**:Vineet et al(2015)提出了一种端到端系统，该系统对数据进行增量处理，对室外环境进行实时密集立体重建和语义分割。他们使用体素散列（Nießner等人（2013））实现了这一点，这是一种散列表驱动的3D体积表示，忽略了目标环境中未占用的空间。此外，他们还采用了一种在线的体素平均场推断技术，该技术可以逐步细化体素标记。它们能够利用现代gpu的处理能力，以实时速度实现语义重构。
  McCormac等人（2016）提出了一种用于密集3D语义映射的传递途径，旨在通过融合CNN的语义预测和来自SLAM系统的几何信息来在线工作（ElasticFusion by Whelan et al。（2015））。具体而言，ElasticFusion提供2D帧与全局一致的表面图之间的对应关系。此外，他们使用贝叶斯更新方案，根据CNN的预测结果来计算每个表面的类概率。利用表面表征的优势在于它们能够融合远距离信息，例如在检测到环路闭合和姿态得到相应修正之后。
  **3D CNN**：虽然卷积网络已经证明在语义上非常成功地分割2D图像，但是使用卷积网络标记3D数据的工作相对较少。Huang＆You（2016）提出了一个使用3D卷积神经网络（3D-CNN）标记3D点云数据的框架。 具体地，他们计算以一组随机生成的关键点为中心的大小为203的3D占用网格。占用率和标签形成3D CNN的输入，3D CNN由卷积层，最大池化层，全连接层和逻辑回归层组成。由于密集的体素表示，考虑到现代GPU的存储器限制，3D CNN仅能够处理非常粗糙分辨率的体素网格。
  为了缓解这个问题，Riegler等人（2017）提出了一种3D卷积网络Oct-Nets，它允许以更高的分辨率训练深层架构。他们基于3D数据(例如点云，网格)在本质上通常是稀疏的。他们候选区域的OctNet通过将3D空间分层划分为一组八叉树并以数据自适应方式应用池来利用此稀疏性属性。这将减少计算和内存需求，因为卷积网络操作是在这些树的结构上定义的，因此可以根据输入的结构动态分配资源。

### 6.5 街边视图的语义分割

自动驾驶车辆的语义分割的一个重要应用是将街边图像（即建筑物外墙）分割成其组件（墙壁，门，窗户，植物，阳台，商店，邮箱等）。这样的语义分割对于精确的三维重建、高效的内存三维映射、稳健的定位以及路径规划都是非常有用的。
  Xiao &Quan(2009)提出了一种多视点语义分割框架，适用于汽车沿街行驶时摄像头捕捉到的图像。具体来说，他们在多个视图中定义了一个跨超像素的成对MRF，其中一元项基于2D和3D特性。此外，它们最大限度地减少了空间平滑度的色差，并使用密集的对应关系来强化不同视图的平滑度。用于多视图语义分割的现有方法通常需要标记用于3D模型的所有图像中的所有像素，这取决于语义分割算法，可能过于缓慢。 为了提高效率，Riemenschneider等人（2014）利用标记用于3D模型的所有重叠图像的固有冗余。他们提出了一种方法，利用从多视图重构得到的3D网格模型的几何形状，在执行实际的语义图像标记之前，预测网格每个面的最佳视图。这使得他们可以将传递途径加速两个数量级。
  Gadde等(2016b)描述了一种用于建筑立面二维图像和三维点云分割的系统，该系统在推理时速度快，易于适应新的数据集。与通过强加先验来利用立面图像结构的现有方法相比，它们实现了一系列增强决策树分类器，这些分类器使用自动上下文特征进行堆叠并从数据中学习所有相关性。
  肖等人(2009)提出了另一种方法，从地面拍摄的图像中生成街道侧的三维照片逼真模型。特别地，他们将每个图像分割成语义上有意义的区域，例如建筑、天空、地面、植被或汽车。然后，他们将建筑分割成独立的块，利用正字法中的建筑先验来进行推理，并使用正则化数据项。这使得他们能够处理嘈杂和缺失的重建3D数据，并产生令人信服的视觉效果。

![img](https://img-blog.csdnimg.cn/20190114220030485.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图16：Mathias等人（2016）提出的用于外观解析的三层方法。 他们首先对外观进行分段，并将概率分布分配给考虑提取的视觉特征的语义类。 在下一层中，他们使用特定对象（如门和窗）的检测器来改善底层的分类器输出。 最后，他们采用了弱的建筑先验，并使用基于抽样的方法寻找最佳的立面标签。 改编自Mathias等（2016）。

  Mathias等（2016）提出了一种灵活的3层分割建筑物外墙的方法，避免了明确指定语法的需要。首先，立面被分割成语义类，这些语义类与诸如窗户和门等建筑元素的检测器输出相结合。最后，提出了弱的建筑先验，例如对齐，对称，共生，这促使重建在建筑上是一致的。完整的传递途径如图16所示。与大多数将外墙视为平面的语义外观建模方法相比，Martinovic等（2015）提出了一种直接在3D中进行外观建模的方法。由于他们的方法避免了2D和3D表示之间耗时的转换，因此它们获得了大大缩短的运行时间。具体而言，他们使用SfM重建半径3D点云，并使用在3D特征上训练的随机森林分类器对每个点进行分类。之后，他们根据他们的语义结构分离各个立面，并强加弱的建筑先验。

### 航空影像的语义分割

航拍图像解析的目的是从机载传感器获取的数据中自动提取城市物体。由于其在自动驾驶系统导航中的应用，对道路等城市对象的准确和详细信息的需求正在迅速增加。例如，航拍图像解析可用于自动构建道路地图（即使在偏远地区）并使其保持最新状态。 此外，来自航拍图像的信息可用于定位。然而，由于诸如建筑物，街道，树木和汽车之类的物体的异质外观导致较高的类内方差但是类间方差低，因此该问题具有挑战性。此外，先前的复杂结构使推理复杂化。例如，道路必须形成一个连接的薄片段网络，其中曲线缓慢变化，在交叉点处相遇。与标准平滑度假设相比，这种类型的先验知识更难以形式化并整合到结构化预测公式中。
  Wegner等人(2013)提出了一种道路标记的CRF公式，其中先验由沿着直线段连接超像素集的簇来表示。具体而言，他们将约束表示为具有不对称潜力的高阶簇，其表示倾向于优先将所有而不是部分组成的超像素分配给道路类别。 这允许为细链放大道路可能性，同时仍然使用图形切割进行有效推断。Wegner等人(2015)也使用具有远程、高阶簇的CRF来模拟路网。然而，与Wegner等人(2013)不同的是，他们允许任意形状的路段，这些路段通过基于局部特征的搜索来适应更复杂的道路形状。Montoya等人(2015)将该公式推广到具有特定类别先验的建筑物和道路航拍图像的多标签分类。除了Wegner等人(2015)之前的公路网，他们还为特定于建筑物的簇引入了第二高阶潜力。
  与其他方法相比，Verdie＆Lafarge（2014）提出应用马尔可夫点过程从图像中恢复特定结构，包括道路网络。 马尔可夫点过程是传统MRF的概括，其可以通过直接操纵诸如线段的参数实体来解决对象识别问题，而MRF仅限于标记问题。 重要的是，它们隐含地解决了模型选择问题，即，它们允许MRF中的任意数量的变量，其可以与感兴趣对象的参数相关联。针对道路分割，选择路段的参数表示作为路段质心点，另外两个参数对路段的长度和方向进行建模。
  利用地图进行航空图像解析:Mattyus等人(2015)利用了OpenStreetMap (OSM)27中的地图信息，而不是将拓扑正确的路网检测问题框定为一个语义分割问题。OSM是道路，小径，咖啡馆，火车站的集合，以及世界各地由地图绘制者社区贡献和维护的更多东西。 它以分段线性路段的形式提供可自由获得的道路拓扑图。
  根据OSM的路线图，Mattyus等人（2015）提出了一个MRF，它推断出OSM中每个路段的道路中心线位置及其宽度。此外，它们通过鼓励它们的宽度相似来在连续线段之间结合平滑度。该公式的优点在于，它能够在将道路拓扑限制到OSM图的同时实现有效推理。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190114220359726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图17：使用Marmanis等人提出的FCN集合（2016b）从ISPRS Vaihingen拍摄的场景的语义分割。 改编自Marmanis等（2016b）。

  使用空对地推理的细粒度图像分析：虽然航拍图像提供了对世界重要部分的全面覆盖，但它们的分辨率远低于地面图像。在航空影像中，分辨率与一个像素覆盖的地面区域有关。虽然1米分辨率已经是卫星图像的高分辨率，但大多数图像数据库（例如GoogleEarth28）的标准分辨率为12英寸。 6至1英寸的分辨率被认为是航空影像的高分辨率，通常不公开。这使得从航拍图像进行细粒度分割成为具有挑战性的问题。另一方面，地面图像提供了额外的信息，这些信息可以实现细粒度的语义分割。由于这些线索的互补性，最近提出了几种用于细粒度分割的方法，它们共同导致了合并的航空和地面图像对。
  Mattyus et al.(2016)扩展了Mattyus et al.(2015)的方法，引入了一个关于细粒度道路语义的公式，如车道和人行道。为了推断这些信息，他们共同考虑从地面车辆上捕获的单目航空图像和高分辨率立体图像。具体来说，他们将问题表述为MRF中的能量最小化，推断每个路段的车道数和位置，所有停车点和人行道，以及地面和空中图像的对齐方式。为了实现这一目标，他们利用深度学习从空中和地面图像中估计语义，并利用这两种线索定义潜力。此外，他们还定义了模拟道路约束的潜力，如平行道路之间的关系和道路的平滑度。
  在相关的工作中，Wegner等人。 （2016）从航拍图像，街景图像和语义地图数据构建城市规划应用的树木地图。 他们在人类注释数据上训练基于CNN的对象检测算法。 此外，他们将来自多个街景图像和航空图像的CNN预测与CRF公式中的地图数据相结合，以实现地理定位的细粒度目录。

### 6.6.1 ISPRS分割挑战

ISPRS分割挑战Rottensteiner et al.(2013, 2014))的重点是机载传感器获取数据的详细二维语义分割，如图17所示。更具体地说，任务是为多个城市对象类别分配标签。这一挑战包括两个机载图像数据集Vaihingen和波茨坦，这两个数据集由六种最常见的陆地覆盖类别手动标注，即不透水表面、建筑、植被、树木、汽车、杂乱/背景。这两个领域都涵盖了城市景观。数据集波茨坦和Vaihingen的排行榜见表5。通过对6个级别和整体的F1分数来评估这些方法的表现。
  Paisitkriangkrai等（2015）是ISPRS分割挑战中表现最佳的方法之一。他们提出了一种语义像素标记方法，该方法将CNN特征与手工制作的特征结合在一个像素方式的CRF公式中，以推断除边缘外局部平滑的全局一致标记。Sherrah(2016)建议使用完全卷积网络，不使用任何向下采样层来保持输出的分辨率。为了利用高程数据，他们提出了一种混合网络，将预先训练的图像特征与基于可获得的数字地表模型(DSM)的特征相结合，以捕获地球表面。Sherrah(2016)在波茨坦ISPRS比赛中取得最好的成绩(表5a)，在Vaihingen比赛中取得最好的成绩(表5b)。
  Maggiori等人（2016）引入了一个模型，该模型以多种分辨率提取空间特征，并学习如何将它们组合以整合本地和全局信息。 Audebert等人（2016）通过利用SegNet的编码器 - 解码器架构进一步改进了航空图像密集场景标记的最新技术（Badrinarayanan等人（2015））。此外，他们还引入了一个多核卷积层，用于快速聚合多个尺度的预测，并使用残差校正网络从异构传感器执行数据融合。 Marmanis等（2016a）证明了表5b中ISPRS Vaihingen挑战的最佳表现。他们使用他们以前的工作Marmanis等人（2016b），它使用完全卷积网络的集合来获得航空图像全分辨率的像素分类。 Marmanis等人（2016a）提出通过将语义分割与边缘检测相结合来补偿由于合并层而导致的空间分辨率的损失。

## 6.7 道路分割

道路场景的分割是计算机视觉中的关键问题，例如自动驾驶和行人检测。 例如，为了导航，自动车辆需要确定前方可行驶的可通行区域并确定其自身在道路上相对于车道标记的位置。然而，由于存在各种不同形状的物体，例如汽车和人，不同的道路类型以及不同的照明和天气条件，该问题具有挑战性。
  穆诺兹等人(2010)提出了一种替代标准推理的图形模型，用于场景的语义标记。特别是，他们在一个层次结构过程中训练了一系列推理模型，该过程捕获了大区域上的上下文。这允许他们在精确推断难以处理时绕过训练结构化预测模型的困难并且得到非常有效和准确的场景标记算法。
  Kuehnl等人(2012)提出了一种结合场景的空间布局来改进基于外观的分类的方法。具体来说，他们提出了道路分割的两阶段方法。首先，它们表示出道路表面和边界元素，如路边石和基于局部视觉特征的信心地图的道路标记。从这些置信度图中，他们提取了包含场景全局属性的空间光线(SPRAY)特征，并在这些特征上训练分类器。他们的评估表明，空间布局特别适用于在不同空间位置的属性之间存在明显的结构对应关系的情况。
  Alvarez 等人(2010)提出了一个贝叶斯框架，通过将低级外观线索与背景三维道路线索(如地平线线、消失点、3D场景布局和3D道路阶段)相结合，从而对道路序列进行分类。此外，他们提取时间线索，以暂时平滑的结果。在后续工作中，A’lvarez＆Lo’pez（2011）将图像转换为光源不变特征空间，使其方法对阴影具有鲁棒性，然后应用分类器为每个像素分配语义标签。 Mansinghka等人（2013）提出了一种反向图形启发方法，该方法采用生成概率图形程序（GPGP）来推断从车载摄像机拍摄的图像中的道路。GPGP包括用于从道路场景先生成随机样本的随机场景生成器，用于渲染每个样本的图像分割的图形渲染器以及链接渲染器输出和数据的随机似然模型。
  基于CNN的方法：几乎所有现有的道路场景标识算法都是基于机器学习的，模型的参数是通过大型带注释的数据集估计出来的。为了减轻手动注释大数据集的负担，A’lvarez等人（2012）提出了一种用于道路分割的方法，其中使用在一般图像数据库上训练的卷积神经网络生成道路图像的噪声训练标签。他们进一步提出了一种纹理描述符，该描述符基于学习颜色平面的线性组合以减少道路纹理的可变性。
  Mohan（2014）提出了一种场景解析系统，该系统使用反卷积层与传统CNN相结合。 反卷积层学习捕获中等线索的特征，例如边缘交叉，平行和图像数据中的对称，从而获得比常规CNN更强大的表示。 Oliveira等（2016）使用Ronneberger等人（2015）的U-Nets研究了分割质量和运行时间之间的权衡。 具体来说，它们在网络的向上卷积部分引入了类和过滤器之间的新映射，以减少运行时间。它们通过单个前向传递进一步分割整个图像，这使得该方法比基于图块的方法更有效。

![img](https://img-blog.csdnimg.cn/20190114220711841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

表5：ISPRS语义标签竞赛。 数字代表F1分数和总体准确性。

  为了减轻获取人工注释的困难，Laddha等人(2016)提出了一个地图监督深度学习传递途径，不需要人工标注来训练道路分割算法。相反，他们使用GPS传感器给出的车辆姿态，根据投影到图像域中的OpenStreetMap信息获得真实值标签。

### 6.7.1可通行区域估计

可通行区域准确可靠的估计和障碍物检测是实现自主驾驶需要解决的核心问题。可通行区域是指地面上保证车辆航行不受碰撞的可用空间。障碍是指从地面来中突出出来阻挡车辆路径的结构。与道路分割方法不同的是，估计车辆前方可通行区域的方法通常依赖于从立体传感器计算出的深度地图得到的几何特征。然而，这两种方法可以有利地结合。
  Badino等人(2007)提出了一种基于立体信息计算随机占用网格的可通行区域估计方法，其中随机占用网格中的单元携带关于占用可能性的信息。立体信息随着时间的推移而整合，以减少深度的不确定性。使用占用网格上的动态编程可以稳健地获得可通行区域和占用空间之间的边界。这项工作为Stixel表示奠定了基础，详见第4节。而Badino等人(2007)的原始方法对平面路面进行假设，这种假设在实践中经常被违背。为了解决更复杂的路面问题，Wedel等人(2009)提出了一种利用b样条对非平面路面建模的算法。表面参数是通过立体测量来估计的，并通过卡尔曼滤波器进行跟踪。
  Suleymanov等人(2016)提出了一种基于用变分方法进行立体估计的无碰撞可遍历路径在线检测和驱动系统。除了可通行区域检测，他们的方法还建立了场景的语义分割，其中标签包括地面、天空、障碍物和植被。鱼眼相机比普通相机提供更广阔的视野，并允许检测靠近汽车的障碍物。Ha¨ne et al(2015)提出一个障碍检测方法使用单眼鱼眼相机。为了减少运行时间，他们避免使用视觉测距系统来提供精确的车辆姿态，而是依靠来自车轮测距的不太准确的姿态估计。
  远程障碍物检测：当观察者（即，自我车辆）高速移动时，远距离障碍物检测方法的准确性是及时进行障碍物定位的关键因素。不幸的是，与不受此问题影响的激光测距传感器或雷达相比，立体视觉系统的误差随深度呈二次方增加。为了解决这个问题，Pinggera et al(2015, 2016)提出了通过利用摄像机运动的几何约束和平面性的远程障碍物检测算法来利用立体视觉将障碍物检测作为统计假设检验问题。具体地，对分布在输入图像上的小局部斑块执行独立假设检验，其中可通行区域和障碍物分别由零和备选假设表示。图18显示了从他们的新数据集中提取的示例场景的检测结果。

![img](https://img-blog.csdnimg.cn/20190114220847340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

图18：该图改编自Pinggera等人（2016），并显示了在Lost and Found数据集中检测到的拟议方法的障碍。

## 7 重建

### 7.1 立体匹配

立体估计是从立体摄像机捕获的二维图像中提取三维信息的过程，不需要特殊的测距装置。特别地，立体算法通过在同一时间点拍摄的两幅图像中找到对应的位置来估计深度信息，通常是由两个相邻的摄像机安装在固定的机构上。这些对应关系是三维世界中相同物理表面的投影。深度信息对于自动驾驶或驾驶员辅助系统的应用至关重要。对稠密深度图的精确估计是三维重建的必要步骤，而许多其他问题如障碍探测、可通行区域分析和跟踪都得益于深度估计的可用性。
分类法:在文献中已经提出了用于立体匹配的多种分类法。在计算限制的引导下，最早的分类方法是根据输出视差图的密度(Franke & Joos(2000))。基于特征的方法只提供基于边缘的稀疏深度图，而基于区域的方法，如块匹配，以牺牲计算时间生成密集输出。立体算法的一种最新的常用分类是基于局部优化和全局优化。局部方法通过简单地选择匹配成本最低的点来计算视差，这被称为赢家通吃(WTA)解决方案。全局方法将视差计算作为基于相邻像素或区域间平滑假设的能量最小化框架。找到全局能量函数的最小值有很多种方法，包括连续域的变分方法和使用动态规划、图割和置信传播的离散方法。
**匹配成本函数**:立体匹配是一个对应问题，目标是基于成本函数来识别左右图像之间的匹配点。算法通常假设图像经过校正，搜索空间缩小为一条水平线，在这条直线上的距离编码了左右点之间的对应，定义为视差。匹配成本计算是在每个像素或所有可能的视差处计算成本函数的过程，在真正的视差处该值为最小值。然而，在实际应用中很难设计这样的代价函数，因此立体算法假设匹配点之间的外观是恒定的。这种假设在现实环境中经常被违背，例如设置稍有不同的相机会导致曝光变化、光晕变化、图像噪声、非兰博顿曲面、光照变化等。Hirschmuller & Scharstein(2007)将这些变化称为辐射测量差异，并系统地研究它们对常用的匹配成本函数的影响，即绝对差异、基于过滤的成本(LoG、Rank和Mean)、层次互信息(HMI)和规范化交叉关系。他们发现一个成本函数的性能取决于使用它的立体匹方法法。在具有模拟和真实辐射差异的图像上，基于关联方法的秩(rank)滤波效果最好。对于全局方法，在具有全局辐射测量变化或噪声的测试中，HMI的性能最好，而在存在局部辐射测量变化的情况下，秩和LoG滤波器的性能优于HMI。定性的结果表明，当使用全局方法时，基于滤波器的成本计算方法会导致模糊的对象边界。没有一项匹配成本进行评估能够成功地处理强光变化。
**SGM**:半全局匹配(SGM) (Hirschmuller(2008))因其速度和高精度而变得非常有影响力，这在各种基准测试中得到了证明，如Middlebury (Scharstein & Szeliski(2002))或KITTI (Geiger et al. (2012b))。SGM最近也被用在CNN的功能上，因为简单地为每个像素输出最有可能的配置，并不能与现代立体算法竞争(Zbontar & LeCun (2016);Luo等人(2016))。能量函数对小的和大的视差差异有两级惩罚，后一级根据局部强度梯度加权。能量的计算方法是采用动态规划的方法，将从多个方向到每个像素的一维路径的成本相加，并由WTA确定。对SGM的实践和理论方面进行了一些后续研究。Gehrig等人(2009)提出了SGM的实时、低功耗实现，并在可重新配置的硬件平台上为汽车应用程序提供算法扩展。Drory等人(2014)通过澄清SGM与置信传播和树型加权信息传递的关系，并以不确定性度量作为结果，为SGM的成功提供了原则性的解释。
结合立体估计的可靠性可以进一步提高SGMs的性能。Seki & Pollefeys(2016)利用CNNs预测立体估计的可信度。考虑到传统可信特征的思想，一致的相邻像素更有可能是正确的，从另一幅图像中估计的视差应该是对应的，他们设计了一个双通道视差图块作为CNN的输入。为了获得密集的视差，根据估计的置信度对每个像素进行加权，将置信度纳入SGM。
**可变基线/分辨率**:立体估计可以融合在一起，从而对三维场景的静态部分进行更完整的重建。然而，假设固定的基线，焦距，视场可能并不总是最好的策略。Gallup等人(2008)指出了传统立体视觉方法存在的两个问题:当距离较远时精度下降，而不必要的计算时间花费在较近的范围。鉴于在许多应用中，立体相机的视图选择非常灵活，例如从动作到结构,盖洛普et al。(2008)提出从一组可能的摄像机同时拍摄的图像中动态选择最适当的摄像机基线，以在较远的范围内准确估算距离。不仅如此，他们还降低了分辨率，加快了近距离范围的计算速度。与传统的固定基线立体视觉相比，所提出的变基线/分辨率立体视觉算法通过均匀地在整个体块中进行计算，在重建体块上获得了恒定的精度。
**平面性**:基于外观匹配成本的固有模糊性可以通过正则化来克服，例如将期望视差图的先验知识引入立体估计过程。最简单的先验倾向于相邻像素具有相同的视差值。然而，这种一般的平滑先验无法重建粗糙的纹理和倾斜的表面，因为它们更倾向于前面平行的平面。处理任意平滑先验的一种更通用的方法是使用超过成对对应的高阶连接。高阶先验能够表达关于深度图像的更真实的假设，但通常需要额外的计算成本。在文献中处理倾斜曲面的一种很常见的方法是假定分段平面。Geiger et al.(2010)通过在一组被称为支持点的鲁棒匹配对应点上形成三角剖分，构建了视差空间上的先验知识。这就减少了匹配的模糊性，并通过将搜索范围限制在合理的区域而得到高效的算法。Gallup等人(2010)首先训练分类器将图像分割为分段平面和非平面区域，然后只对平面区域执行分段平面先验。非平面区域通过标准多视点立体算法的输出来建模。
**变分方法**:类似地，在变分方法中，通常使用平滑性先验信息，全变分(TV)在弱的和模糊的观察条件下不能产生令人信服的结果，因为它激励分段常数区域会导致阶梯状结构。Haene等人(2012)以小型分段平面的形式将基于图块的先验知识引入全变分框架。总广义变分(TGV) (Bredies et al.(2010))被认为是比TV更好的先验知识，因为它不惩罚分段仿射解。然而，与TV不同，它仅限于凸数据项，在TV中，即使存在非凸数据项，也可以计算全局解。粗到细的方法作为非凸立体匹配问题的近似，通常会导致细节的丢失。为了保留细节，Kuschk & Cremers(2013)通过使用边缘检测和报告改进的结果，将自适应正则化权重集成到TGV框架中。Ranftl et al.(2013)通过将非凸泛函分解为两个子问题得到了更好的结果，这两个子问题在一个为凸的情况下可以全局解决，另一个可以通过将泛函提升到更高维度空间来解决。
**最先进的算法**:在表6中，我们展示了在KITTI2015基准上的立体匹方法法排名。KITTI基准测试报告了背景区域(D1-bg)、前景区域(D1-fg)和所有区域(D1-all)的错误(坏)像素百分比。Guney & Geiger(2015)使用对象知识来补偿反射和无纹理表面上的弱数据项。Seki & Pollefeys(2016)通过对立体对应可信度的预测和与SGM的集成，在背景区域取得了最好的性能。最近，深度学习方法(Zbontar & LeCun (2016);罗等人(2016);Mayer等人(2016)提出实现最先进的性能。Mayer等人(2016)提出的深度学习方法是最快的方法之一。

![img](https://img-blog.csdnimg.cn/2019041622152632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*表6:KITTI 2015立体匹配排行榜。根据Menze & Geiger(2015)在背景(bg)、前景(fg)或所有区域中定义的3px/5%标准，数字对应坏像素的百分比。水平线以下的方法是旧的条目，作为参考。*
**超像素**:分段平面性建模的一种替代方法是将图像明确划分为超像素区域并将每个超像素区域上的表面建模为一个倾斜的平面(Yamaguchi et al. (2012);Guney & Geiger(2015)。然而，必须注意的是，超像素化确实是图像相对于平面性的过度分割，例如没有一个超像素包含两个非共平面的表面。山口等(2012)联合推出了由连续和离散随机变量组成的混合MRF获得遮挡边界和深度信息的方法。Guney & Geiger(2015)使用了类似的框架来合并带有特定的3D形状的对象类别任务，该方法能在更远的距离范围内进行规范化。利用语义分割和3D CAD模型，解决了场景中汽车高度镜面反射和无纹理区域的模糊性，如图19所示。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190416221613531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图19:使用对象知识解决立体匹配的歧义。立体视觉方法在反射、无纹理或半透明表面上常常失败(top, Zbontar & LeCun(2016))。通过使用对象知识，Guney & Geiger(2015)鼓励了差异，使之与看似合理的表面(中心)一致。这在定量和定性上都改善了结果，同时恢复了场景中物体的3D几何形状(底部)。改编自Guney & Geiger(2015)。*
**深度学习**:近年来，深度学习方法(Mayer et al.， 2016);Zbontar & LeCun (2016);Luo等人(2016)在立体估计领域获得了广泛的应用。Mayer等人(2016)采用了Dosovitskiy等人(2015)提出的用于光流估计的编码器-解码器架构(见8.1节)。编码器计算抽象特征，而解码器重新建立原来的分辨率，两者之间有额外的关联结构用于连接收缩和扩大网络部分。与编码器-解码器架构相反，Zbontar & LeCun (2016);Luo等人(2016)使用Siamese网络，该网络由两个共享权重的子网络和一个最终得分计算层组成。其思想是通过学习小图像块上的相似度度量来训练网络计算匹配成本。Zbontar & LeCun(2016)将正/负示例定义为匹配和非匹配图块，并使用差额损失来训练一种采用简单的点乘积层的快速架构和一种使用全连接层来学习分数计算的缓慢但更精确的架构。Luo等人(2016)使用了类似的体系结构，但将问题表述为对所有可能的差异进行多类分类，以隐式地捕捉不同差异之间的相关性，如图20所示。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190416221655177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图20:立体匹配的深度学习。训练一个Siamese网络来提取每个像素的所有可能差异的边际分布。改编自罗等人(2016)。*
**讨论**:近年来，立体估计在精度和效率方面都有了很大的进步。然而，一些固有的问题使它没有被标记为已解决。立体匹配最终是在两幅图像中基于不变表象的假设寻找对应关系。然而，外观经常会因不同于几何图形的线索而改变，此外，帧之外的像素或遮挡区域也无法匹配。因此，在这些情况下失败是不可避免的，因为这些方法完全依赖于外观匹配，而没有任何关于几何的预先假设。我们在图21中展示了KITTI 立体匹配测评Geiger等人(2012b)上的前15个方法的累积误差。在自动驾驶环境中，最常见的失败案例是汽车表面由于闪亮和反射区域。Guney & Geiger(2015)通过整合对可能的汽车形状的先验知识，具体解决了这个问题。同样，反射和透明的窗口不能可靠地匹配。Hirschmuller & Scharstein(2007)的结论是，强烈的光照变化构成了另一个常见的误差来源，比如隧道内部或路面过度曝光。离开帧和遮挡的像素常常会对许多方法造成错误，这两种方法都需要推理，而不仅仅是匹配和局部交互。其他问题区域的具体例子包括像交通标志这样的薄结构，或者像围栏这样的重复结构。

![img](https://img-blog.csdnimg.cn/20190416221747709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图21:KITTI 2015立体分析。发表在KITTI 2015立体匹配基准15种表现最好的立体匹方法法的累积误差。红色是根据Menze & Geiger(2015)定义的3px/5%准则，大多数方法产生不良像素的区域。黄色对应于某些方法失败的区域。所有方法正确估计的区域都是透明的。*

### 7.2 多视角三维重建

多视图三维重建的目标是在一定的先验或平滑假设下，通过倒转图像形成过程，对基础三维几何模型进行建模。与双视图立体视觉不同的是，多视图重建算法特别解决了不同视点的问题以及从两个以上甚至可能是大量图像中完全重建3D场景 的问题。如果已知摄像机参数，求解场景的三维几何形状就相当于求解对应问题，基于测量不同视点之间一致性的照片一致性函数。
**分类**:文献中已经提出了几种多视图重构算法的分类方法，通常考虑到照片一致性函数的形式、场景表示、可见性计算、先验和初始化需求，如Seitz et al.(2006)。从应用角度来看，场景表示是一种常见方法，将多视图重构方法分为深度图、点云、网格和体块。
**表示**:深度图:深度图表示通常包括每个输入视图的深度映射，通过3D建模流程进行估计，首先进行图像匹配，然后进行姿态估计和稠密立体匹配。这种表示在场景分析中通常是首选的，因为它具有对大场景的灵活性和可伸缩性。对城市场景特别有效的一种策略是平面扫描立体算法(Collins(1996))。它扫描一个场景中的一系列平行平面，通过平面同质性将图像投射到平面上，然后评估每个平面上的照片一致性值。在大型场景中，挑战之一是实时处理大量数据。Pollefeys(2008)提出了一种基于深度地图表示的大规模实时三维重建系统。实时性能是通过结合一组在典型城市场景上特别有效的组件来实现的，例如一个2D特征跟踪器，它具有自动增益调整功能，可以在自然场景中处理大的动态范围，以及在GPU上并行实现平面扫描立体和深度地图融合。
**表示:点云**:与每个视图的部分深度图不同，基于点云或图块的表面表示使用所有输入图像重建单个3D点云模型。在空间一致性假设下，场景表面的点云可以增长或扩展，从而提供简单的模型操作，如合并和分割。这类方法的代表作品是Furukawa & Ponce(2010)基于图块的多视角立体匹配(PMVS)。PMVS从特征匹配步骤开始，生成稀疏的图块集，然后在展开步骤和过滤步骤之间迭代，使图块变得密集并删除错误的匹配。
**表示:体积**:体积法是在一个定期采样的3D网格上表示几何，即体块，或者作为一个离散的占用函数(Kutulakos & Seitz (2000))或一个编码到最近面(水平集)距离的函数 (Faugeras & Keriven, 1998)。最近的方法使用在规则体素位置定义的概率分布图对占用率进行编码 (Bhotika等人(2002);波拉德&曼迪(2007);Ulusoy等人(2015))。体积法的主要局限性在于需要的内存较大。有很多方法可以解决这个问题，如体元散列(Nießner et al .(2013))或数据自适应空间的离散化形式的德劳内三角(Labatut等人(2007))。一个有效的解决方案是八叉树数据结构，它本质上是一个自适应体素网格，只在表面附近分配高分辨率单元。
**表示:网格或表面**:重构中的最终表示通常是基于三角形网格的表面。体积表面是从中间过程的表征中提取融合了三维信息，如深度地图，点云，卷或扫描得到一个纯净的网格模型。Curless & Levoy(1996)提出了一种利用有符号的距离函数将表面信息积累到体素网格中的算法。表面被隐式地表示为聚集的有符号距离函数的零交叉。它可以使用三方阵算法Lorensen & Cline(1987)提取，也可以通过容积图切割的方法将每个体素标记为内部或外部。有一些方法直接从图像开始，使用基于光一致性函数的数据项和平滑的正则项组成的能量函数来细化网格模型。在这些方法中，能量通常使用梯度下降法进行优化，其中每个顶点的移动由目标函数的梯度决定。
**城市重建**: 在本次调查中，我们主要从自主驾驶的角度进行多视点的重建，主要关注的是大城市乃至整个城市的重建。城市重建算法的目标是通过解决诸如照明条件、遮挡、外观变化、高分辨率输入和大规模输出等固有挑战，实现城市区域的全自动、高质量、密集重建。Musialski等人(2013)通过基于输出的顺序，即建筑和语义、立面和图像，最后是街区和城市，对城市重建方法进行了调查。
输入数据:Musialski等人(2013)指出，地面、空中和卫星图像以及激光探测和测距(LiDAR)扫描是城市重建中最常用的传感器。地面图像是最普遍的，因为易于获取、存储和交换。由于网络地图项目的进步，航空和卫星图像变得更加容易获取。与空中或多视点成像相比，卫星成像提供了一种世界范围的成像技术覆盖频率高，成本低，但分辨率较低。激光雷达提供了半稠密的三维点云图，其在地面级别和航空级别都相当精确。一些方法还将这些数据类型合并在一起以结合他们的互补优势。处理在室外场景的挑战条件下，其他的方法利用额外的数据来源，如捕捉地球表面的数字表面模型(DSMs)。DSM是城市场景的2.5D表示，提供了每个规则网格上点的高度信息。下面我们将介绍近期不同输入方式的例子。
**立体序列**:Cornelis等人(2008)指出，从视频流中提取详细的3D信息会导致重构算法的计算成本很高。通过保持必要的低水平的细节信息，他们创建了一种高速的、紧凑的、内存使用效率高的立体城市模型，该模型是基于简化的几何假设，即立面和道路表面的直纹表面。由于在城市场景中普遍存在的汽车等物体违背了这些假设，它们将汽车的检测和定位融入到重构中。Geiger等人(2011)利用有效的立体匹配，提出了一种系统，可以实时生成立体序列静态场景的精确三维重建。对于在线重建，他们使用两个线程:第一个线程执行特征匹配和自我运动估计，而第二个线程执行密集立体匹配和三维重建。
**数字表面模型(DSM)**:数字表面模型是由空中激光雷达点云或多视点立体模型(MVS)生成的，适用于城市的几何描述场景。基于MVS的DSMs可能非常嘈杂，因此Lafarge等人(2010)提出，通过使用从3D参数块库中提取的简单城市结构组合来重建建筑，从而从MVS图像中生成DSMs。与基于MVS的DSMs不同，激光扫描在获取3D城市模型方面也非常流行。Lafarge & Mallet(2012)通过同时重建树木和地形复杂的地面，以及从空中数据生成的点云构建建筑，为城市场景提供了更完整的描述。他们通过结合两种不同类型的3D表现形式来模拟建筑的原始混合表现形式，一种是常规部分的原始形式，如Lafarge et al.(2010)中所介绍的；另一种是不规则屋顶等非典型表面的网格图块。
**空中或街道水平**: Fruh et al.(2005)向机载数据(DSMs)上传了一系列垂直2D表面扫描和相机图像，以生成城市的纹理立面网格。他们提出了一种数据处理技术，通过去除嘈杂的前景对象，并在建筑立面的几何和纹理上填充孔，来创建视觉上有吸引力的立面网格。Bodis-Szomoru等(2016)指出，机载和移动地图数据提供了互补信息，需要共同开发，以生成完整和详细的大规模城市模型。机载传感器可以大规模获取屋顶结构、地面和植被，同时通过多视点立体方法或激光雷达在道路上进行移动测绘，提供立面和街道侧面的细节。他们提出了一种解决方案，将道路上详细的移动地图和一个粗糙但更完整的点云融合在一个表面网格中。他们的评估显示，通过将街道细节融合到机载模型中，模型的质量得到了显著提高。
**立体卫星**:Duan & Lafarge(2016)提出了一种方法来制作紧凑的三维城市模型，由从成对的立体卫星图像中得到的地面和建筑对象组成。它们使用凸多边形表示场景，并对语义类(地面、屋顶和立面)和每个多边形的高程进行联合分类和重建。虽然他们的评估表明所得到的结果并不像激光雷达扫描那样精确，但该方法可以产生快速、紧凑和语义感知的模型，对低分辨率和遮挡问题具有鲁棒性。

### 7.3 重建和识别

在自动驾驶中，理解周围环境的结构和语义信息是很重要的。传统的图像分割方法都是在二维图像域中使用先验信息，例如空间平滑项，重建方法通常鼓励分段光滑表面。长期以来，人们一直认为语义和三维重建相互之间传递着有价值的信息。与立体匹配类似，在重建中引入语义的动机是在由于推测性、缺乏纹理、重复结构或强光照变化而导致图像信息不完善和不明确的情况下，照片的一致性下降。语义标签提供了关于某个位置可能的表面朝向的几何线索，并帮助解决了固有的模糊性。三维重建将推理从2D提高到3D，并作为一个强大的调整器，通过加强几何一致性对多个图像进行分割。
  平面性与基本物体:Micusik & Kosecka(2009)提出了一种通过利用图像分割线索以及存在主导场景方向和分段平面结构来克服这些困难的方法。特别地，他们采用了一种基于超像素的密集立体重建方法，使用了MRF公式中三个正交平面法线的曼哈顿世界假设。另一种利用分段平面结构和形状重复的方法是使用平面、球体、圆柱体、圆锥和圆环等基本形状(Lafarge et al. (2010);Lafarge & Mallet (2012);Lafarge等人(2013))。基于基本排列的方法提高了紧凑性并降低了复杂性。然而，它们仍然是简单的表示形式，无法模拟精细的细节和不规则的形状。因此，Lafarge等人(2013)提出了一种既紧凑又详细的混合方法。从最初的基于网格的重构开始，他们使用基元来处理常规结构，如柱和墙，而不规则的元素仍然由网格来描述以保留细节。
**体块**:体块场景重建通常将体块划分为占用空间和可通行区域区域。Haene等人(2013)提出了数学框架，将其扩展到一个多标签体分割框架，该框架将对象类或可通行区域标签分配给各个体素，如图22所示。他们首先从训练数据中学习表面方向上的外观概率和类特异性几何先验知识。然后，用这些数据驱动先验来定义一个连续的体积分割公式中的一元和成对项。联合推理得益于典型的类特定几何形状，比如指向上方的地面法线。此外，它为场景几何中信息较少的情况下，提供了一个特定类的平滑先验知识。他们的评估显示了这种先验优于标准平滑假设(如总变化)。

![img](https://img-blog.csdnimg.cn/20190416224613184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

Zhou等(2015)通过引入语义先验，提出了一种基于鱼眼摄像机序列的街景三维重建方法。受到户外场景中重复出现的类似3D形状物体的启发，他们首先使用3D物体探测器对建筑和车辆进行定位，然后在学习形状的体积模型的同时共同重建建筑和车辆。这种方法能够减少噪音，同时完成缺失的表面的重建，因为相似形状的物体能够从各自类别的所有观察结果中受益。
**单目视频**:多视点立体视觉的缺陷会给Haene等人(2013)等需要密集深度测量的方法带来问题。Kundu等人(2014)以单目图像流作为输入，提出了另一种基于SfM的稀疏点云和帧的密集语义标记的联合推理方法。这样，三维语义表示在时间上是一致的，而不需要额外的成本。他们用更高阶的CRF在三维世界中对问题进行建模，它允许真实的场景约束和先验，比如3D对象支持。此外，它们明确地模拟了可通行区域，提供信息以减少模糊性，尤其是在弱支撑的表面上。他们对Camvid和Leuven单目数据集的评估显示，与传统的SfM和先进的多视点立体视觉相比，3D结构得到了改善，并且在每像素精度和时间一致性方面都优于视频分割方法。
**容量:大尺度**:前人在语义重构方面的工作(Haene et al.,2013);Kundu et al.(2014))由于内存占用大，计算成本高，限制于小场景和低分辨率。Blaha等人(2016)指出，对于大的区域，如可通行区域、地下部分或建筑内部，并不需要高分辨率。他们提出了Haene等人(2013)的一个扩展，在一个应用中使用自适应八叉树数据结构并进行粗到精的优化，以从陆地和空中图像生成3D城市模型。从粗糙的体素网格开始，他们解决了一系列的问题，在这些问题中，解决方案只在预测的表面附近逐渐细化。自适应细分节省内存，运行速度快得多，同时在几何重建和语义标记方面仍与最高目标分辨率的固定体素离散化一样精确。
  除了空间范围外，由于内存需求的增加，不同语义标签的数量也是一个可伸缩的问题。由于不同标签之间转换的指标变量，其数量的复杂度是二次的。Cherabier等人(2016)提出将场景划分为块，其中只有一组相关标签是活动的，因为在早期可以确定的特定块中没有很多语义类。因此，他们可以从优化的一开始就禁用标签，从而得到更有效的处理。在迭代优化期间更新每个块中的活动标签集，以从错误的初始化中恢复。他们的评估显示，与Haene等人(2013)相比，他们可以将标签数量从6个增加到9个，在内存使用效率上有显著的提高。
**形状先验**:传感器获取三维形状的进展和目标检测算法的性能鼓励了在三维重建中使用三维形状先验。降维是表示形状知识的一种有效而流行的方法。早期的方法使用线性降维，如PCA，来捕捉低维潜伏形状空间中的形状差异。更近期的方法使用非线性降维，如高斯过程潜在变量模型(GP-LVM) (Dame et al.(2013))。
  Dame等人(2013)研究了单目SLAM中形状先验的重要性。与深度估计平行的是，他们精细化了一个物体的姿态，形状和比例，以匹配最初的分割和深度线索。最后融合到体积表示法中。他们的实验表明，在透明和镜面表面，甚至在未被观察到的部分场景中，效果都有所改善。除了表示形状外，Bao等人(2013)还候选区域学习一组锚点，作为多个实例中对象形状的代表。他们首先使用2D物体探测器进行初始对准。接下来，他们通过匹配锚点，将SfM中的点云与平均形状对齐，然后扭曲并细化它以接近实际形状。他们的评估表明，该模型足够通用，可以通过处理实例间的大形状变化来学习不同对象类别(如汽车、水果和键盘)的语义先验。
  而之前的方法(Dame et al. (2013);Bao等人(2013))试图将输入数据拟合成参数形状模型，Haene等人(2014)建立了一个对象的法线局部分布模型。他们以空间变化的各向异性平滑项的形式提出了一个对象类别的特定形状先验知识。与Haene等人(2013)的多标签分割方法类似，他们将重构过程划分为目标区域和支撑面，只对目标应用形状先验，引导优化到正确的形状。
**数据驱动**:与直接为每个物体建立语义信息模型的方法不同，Wei et al.(2014)提出了一种数据驱动正则化方法，利用SIFT流算法将训练数据库中的语义匹配块转化为视差或流的形状信息。它们将形状信息表示为场景属性的相对关系，而不是绝对值。它主要用于场景属性的可重复使用的条件下，如与位置无关的汽车的视差模型。他们将数据驱动的先验与Sintel上流行的平滑条件进行比较，与KITTI上的最先进的算法相比较，性能得到了改进。


  ## 8 行为和姿态估计

### 8.1 2D估计-光流

光流被定义为两个图像之间的亮度图案的二维运动。该定义仅表示图像平面中强度的运动，而不表示场景中的对象的3D运动。 恢复3D运动本身是8.2节中讨论的场景流的目标。 图23显示了合成Yosemit序列，其中光流基础数据是由Yosemite山谷的山脉深度图上的纹理映射生成的。 光流提供有关场景的重要信息，并作为若干任务的输入，例如自我运动估计（第8.3节），运动结构和跟踪（第9节）。 关于这个问题的研究始于几十年前，由Horn＆Schunck（1981）提出的变分公式假设像素的亮度随时间变化是恒定的。 光流是一个反问题，其中给出的信息不足以完全指定解决方案。 像素处的亮度仅提供一个约束，而未知运动矢量具有两个分量。 这被称为孔径问题，并且只能通过引入附加约束来解决，该附加约束通常是鼓励相邻像素之间的类似运动矢量的平滑度假设。 尽管光流问题有着悠久的历史，但是对于现代方法来说，遮挡，大位移和精细细节仍然是具有挑战性的。 光流定义的基本问题是除了感兴趣的实际运动之外，照明变化，反射和透明度也可以引起除运动之外的强度变化。

![img](https://img-blog.csdnimg.cn/20190416222655596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图23：Quam（1984）生成的Yosemite序列和Heeger（1988）创建的相应真实值流程。 该序列随后被并入Baker等人的Middlebury数据集中。（2011年）。 改编自Heeger（1988）。*

**变分公式**：传统上，光流问题已经采用变分公式。 变分方法最小化由数据项组成的能量方程，假设外观随时间变化很小，以及平滑项，鼓励空间相邻像素之间的相似性。 Horn＆Schunck（1981）引入了亮度恒定性假设，该假设像素的强度值随时间的恒定。 考虑一个像素，这个假设产生一个具有两个未知数的方程，这个方程不能这样解决（孔径问题）。 为了估计光流，需要额外的约束。 使变分光流估计正则化的常用方法是鼓励空间相邻流向量的相似性。这种先验的动机是流场通常是平滑的，并且不连续性通常仅在对象边界处发生。 Horn＆Schunck（1981）的原始公式在数据和平滑项中使用了二次罚函数。这具有主要限制，即不能处理违反亮度恒定性假设（如变化的照明条件）。缓解这个问题的一种非常流行的方法是使用Black＆Anandan（1993）提出的鲁棒惩罚函数。此外，已经提出了几种不同的数据项，它们受光照变化的影响较小。 Vogel等人（2013）在KITTI数据集的统一测试平台上系统地评估基于像素和基于图块的数据成本（Geiger等人（2012b））。在实际数据上，他们发现基于图块的数据项比基于像素的数据项表现更好。 Horn＆Schunck（1981）对原始方法的另一个限制是均匀的非鲁棒平滑项不允许流动不连续。然而，在现实世界场景中，不同的物体经常在其边界处引起光流不连续，因此违反了该假设。 Zach等人使用的总变异正则化（2007）用L 1范数代替二次惩罚，以保持流场中的不连续性。 该模型的另一个缺点是它适用于前平行表面，这对现实场景来说不是一个现实的假设。因此，Bredies等人提出了总广义变化（TGV）模型的高阶正则化。（2010年）。 TGV先验可以更好地表示真实数据，因为它们利用分段仿射运动模型。 Ranftl等人的非局部全广义变异。 （2014）是该模型的扩展，其在当地社区中实施分段仿射假设。 他们观察到，只考虑直接邻居会导致数据项模糊不清的区域的性能下降。 齐默等人（2011）为变分公式提供了图像和流动驱动正则化器的详细评估，并讨论了不同数据数据项的质量。 除了模型规范之外，优化方法的选择及其实现是影响变分光流估计算法性能的附加因素。Sun等人提供了光流方法的详细研究（2014）。 他们揭示了现代光学流动方法成功的原因，并提出了一种用现代技术优化经典方法的方法。

![img](https://img-blog.csdnimg.cn/20190416222805728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图24：快速手部动作（左）是经典变形方法失败的示例（中间左侧），但Brox＆Malik（2011）引入的稀疏匹配有助于估计流量（中间右侧）。 流程的颜色编码在右图中可视化。 改编自Brox＆Malik（2011）。*

**稀疏匹配**：一个主要的挑战，特别是对于变分方法，是对大位移的估计，因为通常使用仅在像素运动的情况下保持的线性近似。 这个问题通常采用从粗到细的策略来解决，在较粗糙的分辨率上估计流量以在更精细的分辨率上初始化估计。 虽然这种策略适用于复杂程度较低的大型结构，但在此过程中往往会丢失精细的几何细节。 此外，对于通信估计很重要的纹理细节在粗分辨率下丢失，因此导致优化器达到局部最小值。 图24中用快速移动的手示出了丢失细节的一个示例。如Brox＆Malik（2011）所提出的，通过将稀疏特征整合到变分公式中可以减轻这些问题。 从粗网格上的最近邻搜索获得的特征匹配在粗到细优化中用作软约束。 虽然在图24中，翘曲方法无法恢复手的光流，但是特征匹配导致优化到正确的解决方案。 Revaud等人提出了另一种处理大位移的可能性。（2015年）。它们用稀疏匹配的插值替换粗到精策略，以全分辨率初始化密集优化。 使用DeepMatching获得稀疏匹配，DeepMatching是Weinzaepfel等人引入的深度神经网络匹方法法。（2013年）。 与DeepMatching相比，Menze等人（2015a）使用近似最近邻搜索来生成一组候选区域作为要在离散优化框架中使用的候选。 通过将匹配的数量限制为具有非最大值抑制的最可能的匹配并利用成对电位的截断形式，推断是可行的。 由于Siamese网络在立体匹配领域的成功（Zbontar＆LeCun（2016））（见7.1节），Güney＆Geiger（2016）将这项工作扩展到2D图块匹配的学习。他们进一步研究了Yu＆Koltun（2016）提出的用于语义分割的扩张卷积的感受野大小的重要性。 Chen和Koltun（2016）认为，用于使推理更加可行的启发式剪枝操作破坏了映射空间的高度规则结构，并提出了对整个空间的离散优化。 Minonvolutions用于降低复杂性并使用Kolmogorov（2006）的Tree-Reweighted Message Passing的修改版本有效地优化大标签空间。 Wulff＆Black（2015）提出了一种从稀疏匹配中获得密集光流的不同方法。在他们的方法中，光流场被表示为从参考流场学习的基础流场的加权和，其已经从好莱坞电影中获得评估。 他们通过找到相对于检测到的稀疏特征对应关系最小化误差的权重来估计光流。 虽然这会导致流场过于平滑，但这种方法非常快。 此外，还采用了较慢的分层方法，可以更好地处理流动不连续性。

![img](https://img-blog.csdnimg.cn/20190416222849826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图25：KITTI 2012的性能和速度之间的权衡Geiger等。(2012B).改编自Wulff＆Black（2015）。*
**高速流动**：除了一些例外（Wulff＆Black（2015）; Timofte＆Gool（2015）; Weinzaepfel等人（2013）; Farneback（2003）; Zach等人（2007）），大多数光流方法效率很低 并且不能实时应用，但这些要求是自动驾驶应用所必需的。 在KITTI 2012基准测试中，不同算法的准确性和速度之间的权衡取决于Geiger等人（2012b）如图25所示。 基于变分推理的方法产生最佳精度，但属于最慢的运动估计方法集。 然而，Zach等人提出了基于对偶的全变差光流方法（2007）允许高效的GPU实现，以320×240的分辨率实时（30 Hz）执行。稀疏匹方法法通常比变分公式更有效率，但通常需要变分细化作为后处理步骤以实现亚像素精度。 最近对光流问题的深度学习的引入产生了几种几乎实时的方法（Dosovitskiy等人（2015）; Ranjan＆Black（2016）），包括Ilg等人（2016），它在流行的数据集上实现了最先进的性能。 这些方法将在下面讨论。Kroeger等人提出的方法（2016）实现权衡准确性和计算时间。 他们通过逆搜索实现了快速图块对应，并获得了密集的流场，并且沿着多个尺度聚集了图块。 这使他们能够以高达600 Hz的频率估算光流，但代价是精度低。
**当前最高水平**：目前，Sintel Butler等（2012年）和KITTI Geiger等人。 第2节中讨论的（2012b，2013）是用于评估光流算法的最流行的数据集。 但是，在本次调查中，我们专注于自动驾驶应用。 因此，我们在比较方法时只会参考KITTI排行榜。 尽管如此，并非专门为自动驾驶设计的光流方法在Sintel上具有相似的排名。 在表7中，我们显示了KITTI 2015基准测试的排行榜。使用异常值的百分比来评估方法的性能，异常值是绝对端点误差（EPE）超过3个像素和其真实值的5％的流向量。 异常值的百分比在背景（F1-bg），前景（F1-fg）和所有区域（F1-all）上取平均值。 此外，还提供了输出流场的密度和运行时间。表现最佳的方法要么学习端到端的光流（Ilg等人（2016）），要么使用语义分割将场景分割成独立移动的物体Bai等。（2016）; 塞维利亚 - 拉拉等人。（2016）。 性能最佳的方法FlowNet2（Ilg等人（2016））训练深度神经网络来解决光流问题。
**极线流**：在自动驾驶的背景下，简化的假设可用于缓解光流问题。 假定静态场景或将场景分解成刚性移动的物体允许将光流作为沿着从扩展焦点辐射的极线的匹配问题处理。 Yamaguchi等人（2013）提出了倾斜平面马尔可夫随机场，其表示具有倾斜平面的每个区段的极线流动。 该方法需要耗费时间进行优化，但可以通过Yamaguchi等人的联合立体匹配和光流的方法来避免。（2014）。 他们假设场景是静态的，并使用立体匹配和视频的联合信息实现新的半全局块匹配算法。 这一方法使他们在KITTI 2012中排名第三，同时比最佳表现方法快10倍。 与这些方法相反，Bai等人（2016）仅将倾斜平面模型用于背景流估计。实例分割允许它们为每个移动物体制定独立的极线流估计问题。 而对于KITTI 2012而言，由于静态场景，这种方法的优势并不明显，在包含动态场景的KITTI 2015上，它们可以获得更好的效果（表7）。
**语义分段**：自动驾驶环境中的场景通常由静态背景和动态移动交通参与者组成。 可以通过将场景分成独立移动的对象。 如上所述，Bai等人（2016）（2016）中提取使用实例级分割交通参与者和对不同的实例独立地估计光流。塞维利亚 - 拉拉等人（2016）以多种方式使用语义分割进行光流估计：一方面，语义提供关于对象边界的信息以及用于推理深度排序的对象之间的空间关系。另一方面，场景的划分允许Sevilla-Lara等人（2016）根据相应的对象类型利用不同的运动模型，类似于Bai等人（2016）。 平面区域的运动用单应性建模，而独立运动的物体通过仿射运动建模，允许偏差。 像植被这样的复杂物体用经典的空间变化密集流场建模。 最后，对象随时间的恒定性用于促进光流的时间一致性。
**置信度**：考虑到光流中仍存在的挑战，需要一种评估估计流量质量的置信度量。 已经提出了基于空间和时间梯度的若干度量（Uras等人（1988）; Anandan（1989）; Simoncelli等人（1991）），其量化了估计特定图像的流量的难度。 相比之下，已经提出了算法特定的度量（Bruhn＆Weickert（2006）; Kybic＆Nieuwenhuis（2011）），其仅对特定的一组方法给出估计的置信度。 基于学习的测量如Kondermann等人（2007,2008）学习了一种模型，该模型将流动算法成功与时空图像数据或计算流场相关联。 Mac Aodha等人给出了对不同置信度测量的详细评估。（2013年）。此外，他们提出了另一种基于深度学习的方法，该方法使用多种特征类型，例如时间，纹理，距图像边缘的距离等，以估计给定方法成功的置信度。
**深度学习**：大多数光流方法都没有包含任何高级信息，如语义，这使得难以解决歧义。 关于物体及其材料特性的知识可用于模拟反射率和透明度，这将使得光流估计不受这些现象的影响。最近卷积神经网络学习高级信息的成功导致了将它们用于光流问题的尝试。 Dosovitskiy等（2015）介绍了使用CNN学习端到端光流的FlowNet。 FlowNet由提取重要特征的收缩部分和产生高分辨率流动的扩展部分组成。 他们提出了两种不同的架构：堆叠图像的简单网络和关联单独处理图像的特征的复杂网络。 学习光流的一个问题是训练数据量有限。KITTI 2012 Geiger等人（2012b）和KITTI 2015 Menze＆Geiger（2015）仅提供约200个训练样例，而Sintel Butler等人（2012）拥有1041个训练图像对。 由于这些数据集太小而无法训练大型CNN，因此Dosovitskiy等人（2015）通过在Flickr的图像上渲染3D椅子模型来创建飞行椅数据集。 这种端到端光流学习的第一次尝试表明，有可能学习光流但无法在KITTI（表7）或Sintel上达到最先进的性能。 然而，与几乎实时执行的方法相比，它们表现最佳。与Dosovitskiy等人（2015）的承包和扩展网络形成对比，Ranjan＆Black（2016）展示了SpyNet，这是一种受传统光流估算技术中利用的粗到精匹配策略启发的架构。 网络的每个层表示不同的比例，并且仅估计相对于翘曲图像的残余流量。 这种方法使他们能够实现与FlowNet类似的性能，同时更快。 比FlowNet小96％，主要贡献是内存效率，这使其对嵌入式系统具有吸引力。 Ilg等人（2016）通过堆叠架构并将堆叠网络融合到专门针对小运动的子网来呈现FlowNet2，FlowNet的改进版本。与SpyNet类似，他们也将扭曲的图像输入到堆叠网络中。 但是，每个堆叠网络估计原始帧之间的流量，而不是SpyNet中的剩余流量。 与FlowNet和SpyNet相比，他们使用FlyingThings3D数据集（Mayer等人（2016）），其中包括22k静态3D场景渲染以及来自ShapeNet数据集的移动3D模型（Savvaetal（2015））。 FlowNet2与Sintel上最先进的方法相当，在KITTI 2015（表7）上表现优于其他所有方法，同时也是最快的方法之一。 它们为8 fps和140 fps之间的频谱提供不同的网络变体，允许在准确度和计算资源之间进行权衡。
**讨论**：强大的光流方法需要处理不是由感兴趣的实际运动引起的强度变化，而是由照明变化，反射和透明度引起的。 在现实世界的场景中，重复模式和遮挡是错误的常见来源。 虽然新的数据数据项已经解决了光照变化（Black＆ Anandan（1993）; Vogel等人（2013）），但反射，透明度，模糊和遮挡引起的问题仍然很大程度上没有得到解决。 在图26中，我们显示了KITTI 2015上15种表现最佳的方法的累积误差（Menze＆Geiger（2015））。 对于移动到图像域之外的区域，可以观察到最高误差。在许多情况下，无纹理，反射和透明区域也会导致大的错误。 为解决这些问题，必须更好地了解世界。 语义学（Bai等人（2016）; Sevilla-Lara等人（2016））和学习的高容量模型（Dosovitskiy等人（2015）; Ranjan＆Black（2016）; Ilg等人（2016））已经证明可以通过解决数据中的模糊来估计流量改善光流。 此外，场景流的方法，其共同推理流和深度已经实现了令人鼓舞的成绩。

![img](https://img-blog.csdnimg.cn/2019041622305586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图26：KITTI 2015光流分析。 在KITTI 2015 Flow基准测试中发布的15种最佳光学流动方法的累积误差。 根据Menze＆Geiger（2015）中定义的3px / 5％标准，红色对应于大多数方法导致坏像素的区域。黄色对应于某些方法失败的区域。 通过所有方法正确估计的区域显示为透明的。*

### 8.2 3D运动估测-场景流

立体匹配不会显示任何运动信息，并且来自单个摄像机的光流不受很好的约束，并且缺少投影丢失的深度信息。另一方面，人类能够随着时间的推移从观察中毫不费力地整合深度和运动线索。 这种推理对于自动驾驶中的许多任务是必不可少的，例如3D世界中的运动物体的分割。 场景流程将光流传播到3D，或者将密集的立体匹配推广到动态场景。给定立体图像序列，目标是估计三维运动场，其是场景中每个可见表面上的每个点的3D运动矢量。 基于图像的场景流估计的最小设置由图27中可视化的两个连续立体图像对给出。建立四个图像之间的对应关系导致两个帧中的表面点的3D位置，因此完全描述了那个表面点3D运动。 尽管存在一些用于实时目的的早期稀疏方法，但是密集输出是优选的（Franke等人（2005））。场景流与立体声和光学流动共享一些挑战，例如在弱纹理区域中匹配模糊度和孔径问题。

![img](https://img-blog.csdnimg.cn/20190416223200887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图27：场景流程。 基于图像的场景流估计的最小设置由两个连续的立体图像对给出。 改编自Geiger（2015）。*

**变分方法**：在Vedula等人（1999）的开创性工作之后，该问题传统上是在变化的环境中制定的，其中优化以粗到细的方式进行，并且利用局部正则化器来促进深度和运动的平滑性。 Wedel等人（2008,2011）通过将运动估计与视差估计解耦同时保持立体约束来提出变分框架。 从每个时间步长的预先计算的视差图开始，估计参考帧的光流和另一视图的视差。 通过为每个任务选择最佳技术，解耦的动机主要是计算效率。 此外，Wedel等人（2011）提出了一种基于残差图像改变光照条件的解决方案，并提供了一种不确定性测量，该测量显示对于物体分割是有用的。 Rabe等人（2010）将卡尔曼滤波器与时间平滑性和鲁棒性的解耦方法相结合。

**分段刚性**：类似于立体匹配和光流，可以利用先前关于几何和运动的假设来更好地处理场景流问题的挑战。 Vogel等人（2015）和Lv等人（2016）将动态场景表示为刚性移动平面区域的集合，如图28所示.Vogel等人（2015）联合恢复该分割，同时推断每个分割的形状和运动参数。他们使用离散优化框架，并以几何，运动和分割的空间正则化的形式结合遮挡推理以及其他场景先验。 此外，他们通过约束分段在时间窗口上保持稳定来推理多个帧。他们的实验表明，他们的视图一致的多帧方法显着提高了挑战场景的准确性，并在KITTI基准测试中实现了最先进的性能（见表8）。 使用相同的表示法，Lv等人（2016）专注于解决问题的有效方法。 他们假设固定的超像素分割并在连续域中执行优化以便更快地推断。 从基于深度匹配的初始化开始，它们独立地细化场景的几何和运动，并最终使用Levenberg-Marquardt算法执行全局非线性细化。

![img](https://img-blog.csdnimg.cn/20190416223228417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图28：分段刚度。 场景被建模为刚性移动的平面段的集合。 改编自Vogel等人（2015年）*。
**物体层面的分段刚性**：Menze＆Geiger（2015）也遵循倾斜平面方法，但除了Vogel等人（2015）; Lv等人（2016），他们将场景的分解模型化为少数独立移动的物体和背景。通过调节超像素化，他们联合估计这种分解以及物体的刚性运动和离散连续CRF中每个超像素的平面参数。与Vogel等人（2015年）相比; Lv等人（2016），他们利用更紧凑的表示，隐含地在更远的距离上正规化。他们还通过使用详细的3D CAD模型从KITTI原始数据集中注释动态场景来呈现新的场景流数据集。他们在Menze等人（2015b）中进一步展示了该模型的扩展，其中除了刚性运动和分割之外还推断出物体的姿态和3D形状。特别地，它们将可变形的3D活动形状车辆模型结合到场景流方法中。
**最新技术**：在表8中，我们显示了KITTI场景流程2015基准测试方法的排名（Menze＆Geiger（2015））。 根据错误像素的百分比比较这些方法。具体而言，表格的每一列显示第一帧（D1）中立体差异离群值的百分比，第二帧（D2）中立体差异异常值的百分比，光流偏离量（F1）的百分比，以及场景流异常值的百分比（ SF），即D0，D1或F1中的异常值。前景/背景区域的异常值分别可以在基准30的网站上找到，由于空间原因在此省略。 表现最佳的方法（Vogel等人（2015）; Menze＆Geiger（2015）; Lv等人（2016））使用刚性运动段的假设。 此外，Menze＆Geiger（2015）模拟了独立运动物体的运动，并在前景区域的FI和SF上表现更好，但它比其他两个更长。 Lv等（2016）通过关注连续域中的有效优化，更快地获得了良好的结果。 Derome等人（2016）提出了一种GPU上的两阶段方法，其运行速度比其他方法快几个数量级。 首先，他们使用立体和视觉测距来计算静态流量，并使用实时光学方法校正动态流量Plyer等人（2014）。

![img](https://img-blog.csdnimg.cn/20190416223331335.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*表8：KITTI 2015场景流程排行榜。 根据Menze＆Geiger（2015）中针对第一帧（D1）中的视差，第二帧（D2）中的视差，两帧（F1）之间的光流的定义的3px / 5％标准，数字对应于坏像素的百分比。 以及产生最终场景流量度量（SF）的所有标准的组合。 水平线下方的方法是较旧的条目，作为参考。*

**讨论**：场景流估计与立体声和光流共享大部分挑战，同时整合更多信息以获得更好的结果。 理想情况下，方法应该利用深度和运动线索来推理动态3D场景。 我们在图29中显示了KITTI场景流基准的前5种方法的累积误差。由于匹配问题和汽车的独立运动，汽车表面是最成问题的区域。 靠近图像边界的像素是另一种典型的误差源，尤其是在发生大规模变化的汽车前方的路面上。 尽管局部平面度和刚度假设缓解了这个问题，但由于植被，行人或自行车等复杂的几何对象，它们经常受到打扰。 错误的平面估计，例如延伸到多个表面的超像素会引起额外的问题，尤其是在物体的边界处。语义图像理解可以帮助解决这些问题，特别是在对象级别通过分割汽车实例。 整合更多信息的另一种方法是考虑长期的时间交互。

## 8.3 自运动估计

自我运动的估计，汽车的位置和方向，是实现自动驾驶的另一个基本问题。 传统上，这个问题通过车轮编码器来解决，车轮编码器通过随时间积分测量来测量车轮的旋转。 这些方法在不平坦的地形或不利条件下遭受车轮打滑，并且不能从测量中的误差中恢复。 视觉里程计或基于激光雷达的测距技术，它们可以从图像或激光测距中估算自我运动，因为它们受这些条件的影响较小，并且可以通过识别已经访问过的位置（称为环闭合）来纠正估计误差（第8.4节）。1）。 Scaramuzza＆Fraundorfer（2011）和Fraundorfer＆Scaramuzza（2011）提供了该主题的详细教程。

**公式**：在视觉测距中，目标是从图像中恢复一个摄像机或摄像机系统的完整轨迹。 这通过估计两个时间步长处的相机位置之间的相对变换并且随时间累积所有变换以恢复完整轨迹来递增地完成。 增量方法如图30所示。不同的方法可分为两类：基于特征的方法，从原始测量中提取中间表示（特征），以及直接在原始测量上操作的公式。 基于特征的方法通常仅适用于符合所使用的要素类型的环境。 特别是在人造环境中，考虑关键点，关于直边和弯边的重要信息被丢弃。相反，直接方法利用整个图像的梯度信息。 因此，这些方法通常在环境开关很少的关键点上实现更高的准确性和鲁棒性。 该领域由基于特征的方法主导，因为它们通常更有效，但直接方法最近越来越受欢迎。 在基于特征和直接的公式中，提取的表示或原始测量通常用作概率模型中的输入，以计算未知的隐藏模型参数，例如相机运动或世界模型。 最大似然法通常找到最大化获得测量概率的模型参数。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190416223424999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图29：KITTI 2015场景流分析。 在KITTI 2015场景流程基准测试中发布了15种表现最佳的场景流程方法的累积误差。 根据Menze＆Geiger（2015）中定义的3px / 5％标准，红色对应于大多数方法导致坏像素的区域。 黄色对应于某些方法失败的区域。 通过所有方法正确估计的区域显示为透明的。*

![img](https://img-blog.csdnimg.cn/2019041622350062.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图30：Scaramuzza＆Fraundorfer（2011）的视觉测距问题图示。 使用视觉特征获得两个相邻摄像机位置（或摄像机系统的位置）之间的变换T k，k-1。 所有变换的累积产生相对于初始坐标系k = 0的绝对姿态C k。改编自Scaramuzza＆Fraundorfer（2011）。*

**漂移**：增量方法极大地受到由各个变换的估计误差的累积引起的漂移的影响。它通常通过对最后x个图像的迭代细化来解决。这是通过三角测量将图像点重新投影到3D并最小化重投影误差平方和（滑动窗口束调整或窗口束调整）来完成的。减少漂移的另一种方法是同时定位和建图（SL等）（Lee等人（2013a）; Engel等人（2015）; Pire等人（2015）; Mur Artal等人（2015）），其共同估计位置和环境地图，用于识别之前访问过的地点。已映射位置的检测称为环闭合，用于减少轨迹和地图的漂移并实现全局一致性。一些工作侧重于特定的环闭合检测（Cummins＆Newman（2008）; Paul＆Newman（2010）; Lee等（2013b）），将在8.4.1节中详细讨论。这些方法在计算上是昂贵的，并且仔细选择提取的特征已经可以减少估计误差和漂移。基特等人。 （2010）例如使用bucketing来获得分布均匀的角落特征匹配，而Deigmoeller＆Eggert（2016）在流量和深度估计上使用不同的启发法来拒绝非稳定特征。
**2D到2D匹配**：根据两个时间步长之间的对应点的表示方式（2D或3D），必须使用不同的方法来获得相机变换。 在2D特征匹配（2D到2D）的情况下，可以估计基本矩阵，其表示两个相机之间的对极几何。 可以直接从基本矩阵中提取平移和旋转。 八点算法（Longuet-Higgins（1981））是使用校准和未校准相机的简单解决方案，而五点算法（Nistér（2004））是一种最小的解决方案，仅适用于校准场景相机。 Scaramuzza等人（2009）使用单一图像来估计基本矩阵，其中仅使用一个2D特征对应，使用轮式车辆的非完整约束来施加限制性运动模型。Lee等人（2013a）将这一想法扩展到一种新颖的两点最小解决方案，该解决方案能够使用多相机系统获得公制尺度。 与非完整约束相反，Lee等人（2014）假设垂直方向是已知的（来自惯性测量单元）并且为多相机系统提出了最小的四点和线性八点算法。 Kitt等人（2010）使用三焦张量估计自我运动，三焦张量涉及三幅图像之间的特征。 在RANSAC中使用这些算法，可以在这些特殊情况下稳健地获得所有6个自由度。 保证使用RANSAC找到正确解决方案所需的迭代次数取决于可以实例化模型的点数。 因此，减少的对应数量将减少迭代次数和方法的运行时间。

**3D到2D匹配**：在前一时间步的3D特征和当前时间步（2D到2D）的2D图像特征的情况下，根据立体数据（或使用单眼图像时的三角测量）估计变换。 Geiger等人（2011）提出了一种使用视觉测距法的实时三维重建方法。 它们使用斑点，角点检测器检测稀疏特征，并通过最小化重投影误差来估计自我运动。 使用卡尔曼滤波器来细化估计，而通过对图像点进行三角测量来获得密集的3D重建。 相比之下，Engel等人（2013）连续估计了一个半密集的逆深度图，用单目相机进行实时视觉测距。对于具有不合格梯度的像素，使用多视图立体匹配估计深度，并且由高斯概率分布表示。 深度估计在帧与帧之间传播，并且使用全图像对准来估计变换。 使用这种半密度方法，它们可以实现与完全密集方法相当的性能，同时不需要深度传感器。 Engel等（2016）提出了单眼视觉测距的直接稀疏方法。 他们使用完全定向的概率模型并联合优化所有模型参数（相机姿势，相机内在函数，反深度）。
**3D到3D匹配**：在处理3D对应（3D到3D）时，可以通过对齐3Dfeatures的twosets来获得变换。 在视觉测距的情况下，使用深度将来自图像的提取的特征投影到3D中，而诸如Zhang和Singh（2014,2015）的基于LiDAR的方法直接从传感器获得3D点。 由于小的基线和相对于距离的误差的二次增加，来自立体的三角形3D点将表现出大的各向异性不确定性。 因此，最小化图像中的重投影误差是更自然的，其中可以更容易地近似误差统计，而基于激光的方法不会遇到该问题，因此可以在3D空间中更容易地优化。



### 8.3.1 最先进的技术

只有少数数据集用于视觉测距，大多数数据集太短或由低质量图像组成。 KITTI基准Geiger等。 第2节中讨论的（2012b）提供了具有挑战性的序列和评估指标的大型数据集。 我们在表9中提供了单眼方法的KITTI排行榜，表10中的立体方法和基于激光雷达的方法在表11中提供。性能用所有可能的子序列的平均平移和旋转误差来测量（100，… ，800）米。
**单目视觉测距法**：单目视觉测距方法仅可以按照一定比例恢复运动。 然后可以通过计算场景中的对象的大小，运动约束或与其他传感器的集成来确定绝对标度。 Longuet-Higgins（1981）提出的八点方法在存在噪声的情况下表现不佳，特别是对于未校准的相机。 Mirabdollah＆Mertsching（2014）研究了基本矩阵的二阶统计量，以便用八点法减少估计误差。 他们使用泰勒展开到二阶项来获得协方差矩阵，该协方差矩阵与共面方程一起充当正则项。 由于缺乏深度信息，单眼视觉测距中的漂移问题尤其困难。通过实时单眼SfM系统中的地平面估计，Song＆Chandraker（2014）处理尺度漂移并改进Mirabdollah＆Mertsching（2014）的结果，如表9所示。它们将多个线索与学习模型相结合，以自适应权重地平面估计的帧观测协方差。 Mirabdollah＆Mertsching（2015）使用迭代五点法提出了一种实时且稳健的单眼视觉测距方法。 他们使用概率三角测量法获得具有不确定性的地标的位置，并估计地平面上具有低质量特征的运动的尺度。 通过这种方法，它们优于表9中的所有单目视觉测距方法。由于KITTI数据集需要度量输出，因此规模估计对方法的性能具有强烈影响。
**立体视觉测距法**：立体视觉测距法没有估算比例的问题，因为它是从摄像机之间的基线直接知道的。 此外，它们允许通过自我运动估计和映射的联合公式来处理漂移问题。 因此，立体方法通常优于KITTI数据集上的单眼方法（参见表9和表10）。 Engel等人（2015）提出了一种实时大规模直接SLAM算法，该算法将时间多视图立体声与来自摄像机设置的静态立体匹配耦合（图31）。 这允许他们估计静态立体匹配中受限的像素的深度，同时避免使用多视图立体匹配发生的尺度漂移。 基于高对比度像素的光照一致性直接对准图像。 Pire等人。 （2015）将问题分为可以并行运行的摄像机跟踪和地图优化。在共享相同的地图时，跟踪任务匹配要素，创建新点并估计相机姿势，而地图优化通过束调整来细化地图。 该方法允许它们在更快的同时实现相同的性能。 Deigmoeller＆Eggert（2016）采用了一种完全不同的方式，完全依赖于前面提到的纯测量。 通过对哈里斯角上的场景流的估计和具有不同启发式的特征的拒绝，它们在平移误差方面优于两个SLAM方法，但在表10中具有最高的旋转误差和运行时间。

![img](https://img-blog.csdnimg.cn/20190416223620427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图31：Engel等人（2015）的Stereo LSD-SLAM实时计算准确的相机移动以及半密集概率深度图。 深度可视化对于远处的场景点使用蓝色，对于近处的对象使用红色。 改编自Engel等人（2015年）。*
  Persson等人（2015）提出了一种基于单目视觉测距技术的汽车应用的立体视觉测距系统。 特别是，他们使用运动模型预测跟踪通过匹配，类似于Song等人（2013）和延迟异常值识别。 他们认为立体技术优于单眼技术，因为问题的表述更容易。 因为他们需要处理的本质更困难的问题单眼技术应该更加细化和稳健。 这使得它们在表10中的平移误差中胜过其他方法。两种表现最佳的方法将旋转和平移的估计分离，因为它们的估计之间存在根本差异。平移取决于与旋转相反的深度。 Buczko＆Willert（2016a）声称，深度估计误差会影响耦合公式中的旋转估计，并且可以通过解耦来避免。 因此，它们使用初始旋转估计来解耦旋转和平移光流。 然后使用所得特征来排除异常值。 Cvisic＆Petrovic（2015）使用五点法和使用三点法的平移来单独估算旋转来计算运动。 它们还提供了适用于嵌入式系统的改进的IMU辅助算法版本。

![img](https://img-blog.csdnimg.cn/2019041622370312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*表10：KITTI Odometry立体声排行榜。 数字显示相对平移误差和相对旋转误差，在所有长度为100米到800米的子序列上取平均值。 水平线下方的方法是较旧的条目以供参考。*

![img](https://img-blog.csdnimg.cn/20190416223742206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图32：Zhang和Singh（2014）的LOAM匹配两个连续的LiDAR扫描（LiDAR Odometry）并将新扫描注册到地图（LiDAR Mapping）。 改编自Zhang＆Singh（2014）。*

Kre so＆Segvi’c（2015）观察到摄像机校准对于视觉测距非常重要，并且像KITTI这样的预校准系统中的剩余校准误差对估计结果具有对抗性影响。 因此，他们建议通过利用真实值运动来校正相机的校准。 通过优化地面运动下相邻立体帧中点特征对应的重投影误差来恢复变形场。 使用变形场，他们获得了当时最先进的结果。
**基于LiDAR的测距法**：KITTI上表现最佳的方法是使用点云进行自我运动估计（表11）。 Zhang＆Singh（2014）将SLAM问题分解为高频率的基于LiDAR的测距法，低保真度和低频LiDAR映射，如图32所示。基于LiDAR的测距法匹配两个连续的LiDAR扫描，而LiDAR映射匹配和 注册新的扫描地图。 这导致低漂移和计算复杂性，而不需要高精度范围的orinertial测量。 章＆辛格（2015）通过视觉里程计在高频与低频这允许它们以进一步提高激光雷达映射组合扩展这项工作。
**讨论**：虽然有几种方法已经解决了单眼视觉测距中的尺度漂移问题，但他们无法与使用KITTI数据集上的3D信息的方法竞争。 虽然LiDAR为自我运动估计提供了最丰富的信息来源，但基于立体匹配的方法显示出有竞争力的结果。 在图33中，我们可视化KITTI基准测试中表现最佳的视觉测距方法的平均横向和旋转误差。 第二列显示平移误差，第三列显示旋转误差，而最后一列显示速度。 在强转弯中，最高的平移和旋转误差通常是正面的。此外，误差与速度和场景中独立移动的物体的数量相关，这降低了背景中的特征数量。 虽然在拥挤的高速公路场景中可以观察到大的错误（右起第二个），但是当高速公路为空时（右侧和第二个左侧）仅发生中度错误。 在特征位移较大的非常狭窄的环境中（右起第四个）也可以观察到更大的误差。 总的来说，到目前为止，使用3D信息实现了最准确的运动估计。 然而，与激光雷达扫描仪相比，立体相机是非常便宜的传感器，并且基于立体的方法获得了有竞争力的结果。

![img](https://img-blog.csdnimg.cn/20190416223823957.png)

*表11：KITTI Odometry LiDAR排行榜。 数字显示相对平移误差和相对旋转误差，在所有长度为100米到800米的子序列上取平均值。*

![img](https://img-blog.csdnimg.cn/20190416223839554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图33：KITTI Odometry。 从上到下：来自序列的示例图像，平均平移误差，平均旋转误差和速度。 平均值计算超过400米长的轨迹和KITTI网站上公布的15种表现最佳的方法。 较暗的颜色（即红色）表示较大的错误或较高的速度。 单行中的每个图形都以相同的方式标准化，以使它们具有可比性。*

### 8.4 即时定位和地图构建（slam）

详细的环境地图是自动驾驶汽车的路径规划和导航的常用先决条件。但是，在没有提供地图或不完整地图的地方，自动驾驶汽车需要在生成地图时定位自己。此外，需要不断更新地图以反映随时间的环境变化。在这种情况下，SLAM指的是在连续构建环境地图的同时估计主体的位置的任务。自动驾驶中的一个特殊挑战是这些系统需要实时处理大型环境。
**公式**:传统上，地图是由一系列地标来表示的，例如图像特征。SLAM的早期研究使用扩展卡尔曼滤波器(Smith et al.(1987))或粒子滤波器(Montemerlo et al.(2002))使用贝叶斯公式来解决这个问题。给定最后一个状态和当前观测值，当前状态(由姿态、速度和地标位置表示)将递归更新。但是，这个公式不适用于大型环境，因为过滤器更新的信任状态和时间复杂度随地图中地标数量的二次方式增长。信任状态表示所有变量对之间的所有相关性，即O（n2），并且每当观察到界标时，需要以相同的复杂度更新与所有其他变量的相关性。减少复杂性的一种解决方案是基于图形模型的过滤技术，该技术使用薄节点树(Paskin, 2003)保持对信任状态的可处理近似。然而，众所周知，当应用于非线性SLAM问题时，过滤总是会产生不一致的映射（Julier＆Uhlmann（2001）），这在处理实际数据时通常就是这种情况。相比之下，完整的SLAM方法，例如基于图形或最小二乘的公式，可以提供考虑所有姿势的精确解决方案。Kaess等人（2008）提出了一种基于快速增量矩阵分解的增量平滑和映射方法。他们将Dellaert & Kaess(2006)关于非线性最小二乘问题矩阵分解的工作扩展到一种增量方法，这种方法只重新计算矩阵中变化的项。Kaess等人（2012）引入了贝叶斯树，这是一种新颖的数据结构，可以更好地理解图形模型推理与SLAM中的稀疏矩阵分解之间的联系。分解概率密度编码在贝叶斯树中，贝叶斯树自然映射到稀疏矩阵。
**环境变化**:SLAM项目的一个主要挑战是环境的变化，这些变化可能无法用地图来表示。为了缓解这个问题，Levinson等人(2007)创建了一个只包含很可能是静态特征的地图。使用3D 激光雷达，它们仅保留平坦表面并获得路面俯视图的红外反射率图。然后该地图用于实时定位具有粒子滤波器的车辆。Levinson＆Thrun（2010）将这项工作考虑为地图是作为环境属性的概率分布而不是固定的表示。具体来说，概率图的每个单元都表示为其在汇出值上的高斯分布。这使他们能够更准确地表示世界，并以更少的错误定位世界。此外，他们还可以使用离线SLAM在不同时间对同一环境的多个通道进行对齐，以建立对世界日益强大的理解。

### 8.4.1 环路闭合检测

已构建地图区域的重定位是SLAM的一个重要子问题，称为环路闭合检测。重定位用于校正轨迹中的漂移和由漂移引起的地图中的不准确性。 Cummins＆Newman（2008）提出了一种基于外观识别场所的概率方法。他们使用词汇库学习地方外观的生成模型，因为视觉词的独特组合通常来自普通对象。这样生成的模型是健壮的，即使在视觉重复的环境中也能工作。该方法的性能在自记录数据集上得到证明，并在图34中可视化。Paul＆Newman（2010）通过将与视觉词对的观察相关联的词之间的距离与随机图相结合来扩展这一想法。随机图模拟的是单词之间的两两距离，除了它们的出现的分布。Lee et al. (2013b)的研究表明，在具有重叠视图的多摄像机系统中，两个闭环点之间具有度量尺度的相对位姿可以直接从超极几何中得到。他们用平面约束简化了问题，用最小二乘优化估计了环路约束。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190416223942477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图34:由Cummins & Newman(2008)提供的基于外观匹配的环路闭合。两个概率大于99%的图像被标记为红色。改编自康明斯&纽曼(2008)。*

**基于LiDAR**：在强烈的照明变化或强烈的视点变化的情况下，基于图像的闭环检测可能变得不可靠。 相比之下，基于LiDAR的定位不受照明变化的影响，并且不会因捕获的3D几何形状而受到视点变化的影响。 Dube’et al（2016）提出了一种基于匹配3D段的闭环检测算法。使用描述符的组合来提取和描述来自点云的片段。 通过在特征空间中获得具有kd树搜索的候选者并且使用随机森林估计候选者的匹配分数来执行片段的匹配。

![img](https://img-blog.csdnimg.cn/20190416224004718.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图35:Frahm et al(2010)来自罗马(左)和柏林(右)的示例模型，其模型计算时间少于24小时。改编自Frahm等人(2010)*。

### 8.4.2 视觉SLAM

Lategahn等人(2011)提出了一种密集立体视觉SLAM方法，可以估计出密集的3D地图。通过使用稀疏视觉SLAM系统，他们可以获得姿态和稀疏地图。对于密集的三维地图，他们在局部坐标系中计算立体的密集表示，并用稀疏SLAM系统跟踪局部坐标系，不断更新地图。Engel等人(2014)通过优化相似变换的公式，对视觉测距半密度法(Engel等人(2013))进行了扩展，采用图像对齐和闭环检测。利用小基线的多视点立体估计半密度深度，使用位图优化创建和细化半密度图。由Leutenegger et al.(2013)提出的视觉和惯性信号融合利用了它们的互补性。他们使用非线性优化方法而不是过滤，并将IMU误差与地标的重新定位误差集成到联合成本函数中。mura - artal等人(2015)使用Rublee等人(2011)提出的ORB特性来跟踪、映射、重新定位和闭环。他们结合了环路检测（Ga’lvez-Lo’pez和Tardo（2012）），循环闭合（Strasdat等人（2010,2011））和姿势图优化（Ku¨mmerle等人（2011）））成一个系统。

### 8.4.3 构建地图

对于自动驾驶应用程序，需要在不同层次的细节上使用度量和语义映射来解决不同的任务。尺度地图允许精确的定位，而语义地图可以提供特定问题的信息，比如自动停车的停车位。这些地图还可以在离线状态下使用昂贵的计算方法生成，然后整合到自动驾驶系统中。
**尺度地图**：Google街景项目（Anguelov等人（2010年））是世界各地城市中大量全景图像的一个突出示例。为了收集数据集，他们根据基于卡尔曼滤波器的方法估算姿势，融合来自GPS，车轮编码器和惯性导航的数据。100 Hz的估计允许精确地将来自15个小型相机的图像像素与来自激光扫描仪的3D光线相匹配。姿态估计通过网络的概率图形模型进行细化，该模型表示世界上所有已知的道路和交叉。从图像和激光扫描数据中，他们通过稳健地拟合粗网格来重建场景并获得逼真的3D模型。Frahm等人（2010）提出了一种来自互联网规模照片集的密集3D重建方法。使用2D外观，颜色和3D多视图几何约束的组合来估计图像之间的几何关系。他们通过快速平面扫描立体和深度地图融合的方法获得场景的密集几何形状。利用外观和几何限制，他们提出了一种高度平行的方法，允许一天之内在一台电脑上处理300万张图像。图35显示了从罗马和柏林的Flickr图像重建的两个示例模型。对于自动驾驶应用，通常足以在2D中（即，在鸟瞰视图中）绘制路面图，其允许相对于道路上的特征（例如道路标记或路面中的缺陷）进行定位。Geiger(2009)提出了一种在动态环境中进行道路拼接的方法，以创建无障碍的鸟瞰图。利用哈里斯角上的光流提取路面，用平面逼近。这允许用同构图描述图像之间的映射。最后采用多波段混合技术对道路图像进行组合。
**语义地图**:到目前为止讨论的所有方法都关注于创建尺度地图而忽略语义信息。然而，对于像自动泊车这样的任务，需要一个与尺度地图一起更新的语义地图。Grimmett等人(2015)将语义地图和尺度地图融合到仅用于视觉的自动停车中。他们用静态和动态标签更新地图，并使用主动学习来检测车道、停车位和人行横道。

## 8.5 定位

定位是机器人学和视觉学研究得很透彻的问题，涉及的技术范围很广，从使用噪声感官测量的机器人室内定位到定位在世界各地拍摄的照片的位置。从自动驾驶的角度来看，主要任务是在地图上精确定位自我汽车。定位也是SLAM方法的一个重要子例程，在绘制环境地图时用于检测环路闭包和纠正漂移，见8.4.1节。
  定位可以使用像GPS系统这样的传感器或基于图像的视觉信息来完成。单独使用GPS通常提供5米左右的精度。虽然在开放空间中使用传感器的组合可以达到厘米级精度(Geiger et al. (2012b))，但在交通场景中，由于植被和建筑物遮挡或由于反射而产生的多路径效应等干扰效果，这种方法通常是不可行的。因此，独立于卫星系统的基于图像的定位具有高度的相关性。
早期基于图像的技术(Li et al.， 2009);Zheng et al.(2009))将这个问题归类为一个预定义的地点集合，称为“地标”。其他的(如Hays & Efros(2008))则创建一个具有已知位置的图像数据库，并将定位作为一个图像检索问题进行表述。这些方法需要进行相似度测量，基于局部或全局外观线索来比较图像。数据库越大，定位任务就越困难。挑战包括外观的改变，相似的外观的地方，以及由于观察点或位置的改变。
**调查**:Lowry等人(2016)对地点识别研究的现状进行了全面的回顾。他们首先通过参考心理学和神经科学的研究来定义机器人导航环境下的位置。然后，他们回顾了使用本地或全局描述符以及尺度范围信息来描述地点的方法。它们还根据地图中的物理抽象级别以及地点描述中是否包含尺度信息提供分类。他们进一步讨论了地点识别解决方案如何隐式或显式地解释环境中的外观变化，最后就深度学习、语义场景理解和视频描述方面的进展提供了一些未来的方向。
**蒙特卡罗方法**:传统的地图定位方法采用蒙特卡罗方法，通过绘制一组样本来恢复物体位姿的概率分布。Dellaert等人(1999)将室内定位分为两个步骤:全局位置估计和随时间的局部位置跟踪。他们不是对概率密度函数本身进行建模，而是通过维护一组样本来表示不确定性，并使用蒙特卡罗方法随时间更新表示。这允许他们以高效存储的方式模拟任意多模态分布。与室内定位任务相比，由于规模问题以及传感器信息通常不可靠（例如GPS失效），室外定位通常更具挑战性。Oh等（2004）使用地图中可用的语义信息来补偿GPS传感器的失效情况。通过利用与环境有关的知识，他们将概率分配给地图上的目标区域，比如建筑物的零概率。他们在粒子滤波公式中加入了这些基于地图的先验，使运动模型偏向概率更高的区域。
**尺度、拓扑、地形**:视觉定位技术通常分为尺度和拓扑方法。尺度定位是通过关于地图的三维姿态计算来实现的。拓扑定位方法从有限的可能位置集合中提供粗略的估计，这些位置被表示为图中的节点，这些节点根据距离或外观标准被连接起来。尺度定位可以非常精确，但通常不适用于长序列，而拓扑定位可能更可靠，但只提供粗略的估计。Badino等人(2012)提出了一种地形测量方法，将拓扑定位和尺度定位结合起来，使用基于图形的方法提供精确的几何定位。与拓扑方法不同的是，图的粒度更细，每个节点对应于一个没有语义含义的尺度位置。在构建地图阶段，利用GPS在固定距离间隔下测得的车辆位置，将视觉或3D特征与相应的图形节点相关联，构建图形。在运行时，通过将从传感器数据中提取的特征与地图的特征数据库进行匹配，利用Bayes滤波器估计车辆在路线上位置的概率分布，实现实时定位。然而，与传统的定位方法不同的是，它们不需要环境的视觉特征数据库，而是直接根据从OpenStreetMap中提取的道路网络构建这个图。他们进一步提出了概率模型，其允许使用视觉里程测量来推断车辆位置上的分布。为了在非常大的环境中易于处理，它们利用了几种解析近似法来进行高效的推理，与基于粒子滤波技术相比，其稳定性更高，因为前者易出现当长时间存在模糊性时粒子会耗尽的问题。
**规模和精度**：对于定位问题，目标区域的规模是用来比较不同方法的独特属性，并且与所实现的准确性相关。规模和精度都取决于所使用的方法，例如基于地图的方法（Brubaker等人（2016）），这种方法可能受到地图上的错误以及基于描述符的方法的影响（Badino等人（2012）; Schreiber等人（ 2013））使用全局或局部描述符。Badino等人（2012）的基于描述符的方法在8 km路线上实现了1 m的平均定位精度，而Brubaker等人（2016）的基于道路网络的定位方法在18 平方公里的地图上获得了4 m的精度，其中还包含2,150公里的可行驶道路。Schreiber等人（2013）指出，自动驾驶和未来驾驶辅助系统所需的精度在几厘米的范围内，并且提出了基于特征的定位算法，该算法可以在大约50公里的乡村道路上实现这一点。他们从车道识别的角度来解决这个问题。在一个单独的驾驶过程中，他们创建了一个高度精确的地图，其中包括道路标记和路缘。在驾驶过程中，他们检测标记并将其与地图匹配，以确定车辆相对于标记的位置。
**基于结构的定位**:传统的定位方法的输出要么是粗糙的摄像机位置，要么是位置上的分布，而最近的一种工作称为“基于结构的定位”，目的是估计所有的摄像机矩阵参数，包括位置、方向和摄像机内部特性。定位是作为一个2D- 3D匹配问题来实现的，图像上的2D点与一个大的、标记过的3D点云相匹配，根据对应关系估计姿态，如图36所示。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190416224129573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图36:定位。查询图像会被匹配到一个基于运动点云的地理参考结构数据库(左图)。在基于结构的方法中，目标是通过匹配这个全球点云(右)来计算新查询图像的地理参考位姿。改编自Li等人(2012)。*

在基于结构的方法中，姿势估计为验证位置估计提供了强大的几何约束。然而，直接的解决方案，例如使用SIFT特征的近似最近邻搜索的直接匹配，将导致许多不正确的匹配。随着模型大小的增加，描述符的判别力降低，匹配变得更加模糊。因此，RANSAC技术很难找到正确的姿势。为了解决这个问题，Li等人(2012)发现了图像中3D模型点的统计共现，并将其作为RANSAC利用共现关系的先验抽样。此外，他们采用双向匹配方法，正向为从图像中的特征到数据库中的点，逆向则为从点到图像特征。结果表明，双向匹配方法的性能优于仅正向匹配和仅反向匹配。除了模糊之外，存储模型中包含的大量描述符所需的内存数量也是与大规模相关的另一个问题。通过减少点数来进行模型压缩会产生较少的匹配，并且会增加无法定位的图像数量。相反，更近期的方法（Sattler等人（2015年，2016年））将量化用于精细词汇表，其中每个描述符由其词ID表示。 Sattler等人（2015）将寻找独特的2D-3D匹配的难题分成两个更简单的问题。他们首先利用良好的视觉词汇以及对3D点和摄像机之间的可见关系进行编码的可视化图表，建立了本地独特的2D-3D匹配。然后，他们使用一个简单的投票方案来消除这些匹配的歧义，以加强选定的3D点的共同可见性（共现性）。他们的实验表明，基于视觉词汇的匹配可以达到最先进的水平。Sattler等(2016)提出了一种基于量化的优先匹方法案，以效率为中心。通过首先考虑更有可能的特性，并在找到足够匹配的情况下终止对应搜索，它们显著地加速了2d - 3d匹配。
**使用深度学习的基于结构的定位**：Kendall等人（2015）和Walch等人（2016）使用卷积神经网络将单端RGB图像以端到端的方式回归为相机姿态。使用CNNs完成这项任务的动机是为了消除由大的无纹理区域、重复结构、运动模糊和光照变化引起的问题，这些问题对于基于特征的方法来说是具有挑战性的。经典的的定位方法的运行时间取决于几个因素，比如查询图像中发现的特性的数量或模型中的3D点的数量，与之不同，基于cnn的方法的运行时间只取决于网络的大小。Kendall等(2015)对GoogLeNet (Szegedy等(2015))进行了修改，将softmax分类器替换为仿射回归，并在最终回归器之前插入另一个全连接层，该层可作为定位特征向量进行进一步分析。最终的体系结构，被称为PoseNet，是通过使用像ImageNet (Deng et al.， 2009)和Places (Zhou et al.， 2014)等大型数据集上训练的分类网络的权重来初始化的。此外，它在新的姿势数据集上进行微调，该数据集是通过使用SfM从场景的视频生成相机姿势而自动创建的。Walch等人（2016）使用了类似的方法，但是他们通过利用它们的记忆能力，在空间上将CNN输出的每个元素与长短期记忆（LTSM）单元相关联。通过这种方式，该网络能够捕获更多的上下文信息，并在不同的定位任务中包括大型户外、小型室内和新候选区域的大型室内定位基准等任务胜过PoseNet。虽然基于cnn的方法不能与目前最先进的基于筛选的方法的精度相匹配(Sattler et al.(2016))，但是在室内环境中，基于筛选的方法不能产生足够的匹配以获得正确的SfM重构，因此其重要性变得更加明显。
**交叉视图定位**:在全球范围内保持地面图像的更新是很困难的，而从航空图像和卫星上建立实时地图则要容易得多。这就产生了一种新的方法——地理定位，试图将地面图像记录到航空图像中。其基本思想是学习地面和空中图像视点之间的映射，以便在空中图像参考数据库中定位地面查询。林等人（2013）将地面级查询与传统地理定位中的其他地面参考照片进行匹配，然后使用这些地面匹配的航空外观和土地覆盖属性在空中和土地覆盖域中构建滑动窗口分类器。与以前的方法不同，通过学习不同视图中特性的共现性，即使数据库中没有相应的地面图像，它们也可以对查询进行定位。Lin等人（2015）使用来自Google街景的范围数据和相机参数来收集交叉视图图块数据集，以使主要建筑物表面平面大致看起来像45％的鸟瞰图。受到使用深度学习的人脸验证算法成功的启发，他们训练了一个暹罗网络来匹配相同位置的交叉视图对。Workman等人(2015)介绍了另一个大规模的交叉视图数据集。他们首先使用CNNs提取地面图像特征，然后，他们学习从相同位置的空中图像中预测这些特征。通过这种方式，CNN能够从航空图像中提取有意义的语义特征，而无需手动指定语义标签。他们的结论是，横向定位方法可以得到与上述方法不同的对地理位置的精确估计。否则，它可以用作更昂贵的匹配过程的预处理步骤。

![img](https://img-blog.csdnimg.cn/20190416224220829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图37:街道视图匹配。建筑物的重复模式可以提供有价值的信息，以便使用规则驱动的方法进行匹配。改编自Wolff et al.(2016)。*

**交叉视图定位:建筑**:在交叉视图匹配中有专门用于建筑立面的方法。重复模式可以为规则驱动的方法提供有价值的匹配指示器(图37)。Bansal et al.(2011)通过结合卫星和斜鸟瞰图，首先提取建筑轮廓和立面，然后根据立面形态的统计描述将地面图像与斜空中图像进行匹配Wolff等人（2016）定义了匹配成本函数，基于颜色，纹理和基于边缘的上下文特征的相似性将街景图案与鸟瞰图图案进行比较。
**交叉视图定位：重建**：另一项工作通过自动与卫星图像，平面图，地图或其他俯视图对齐来解决地理参考重建的问题。Kaminsky等人(2009)使用目标函数计算SfM重构与俯视图像之间的最优对齐，这个目标函数可以将3D点与图像边缘匹配，并根据每个相机中点的可见性施加可通行区域约束。直接匹配地面和空中图像是一项困难的工作，因为它们的摄像机视点、遮挡和成像条件等都有很大的差异。Shan等人（2014）没有寻找不变的特征检测，而是利用近似的对齐信息和基础三维几何结构提出一种基于视点的匹配技术。
**激光雷达的语义一致性**:一些公司通过安装在汽车上的扫描设备获取激光雷达数据，从而获得真实城市环境的3D模型。然而，三维扫描仪所获得的三维点位置的精度取决于GPS、惯性传感器和SfM所预测的扫描仪姿态，而这在城市环境中经常会失败。这些失调导致了点云配准方法的问题。Yu et al.(2015)提出了在不同尺度下能够进行稳定匹配的语义特征的对齐。通过遵循粗到精的方法，它们首先依次对齐道路、立面和极点，这些可以被稳定地匹配。再往后，他们匹配汽车和其他小物体，这需要更好的初始校准以找到正确的对应关系。语义特征的使用为激光雷达扫描提供了全局一致的对齐方式，它们的评估显示了相比初始对齐方式这种对齐方式有提高。

## 9 追踪

在跟踪中，我们的目标是在给定传感器的测量值的情况下估计一个或多个对象随时间的状态。通常，物体的状态由它在某一时刻的位置、速度和加速度来表示。对其他交通参与者的跟踪是自动驾驶中一个非常重要的任务。例如，考虑车辆的制动距离，它的速度是二次增长的。如果可能与其他交通参与者发生冲突，系统需要尽早做出反应。其他交通参与者的轨迹可以预测未来的位置并预测可能发生的碰撞。对于行人和骑自行车的人来说，预测未来的行为尤其困难，因为他们会突然改变自己的运动方向。然而，与交通参与者的分类相结合的跟踪允许车辆的速度适应该情况。此外，跟踪其他车辆可以用于自动距离控制并且可以在早期预测其他交通参与者的可能驾驶操作（例如接管）。
挑战:跟踪系统必须能够应对各种各样的挑战。通常情况下，对象会被其他对象或自身部分或完全遮挡。不同对象的相似性是另一个挑战，特别是对于同一类的对象。在有行人的情况下物体的相互作用进一步增加了遮挡的数量并且使得难以跟踪每个单独的物体。不佳的照明条件以及镜子或窗户的反射会带来额外的挑战。
**公式化**:目前已经开发了几种类型的传感器可以用来解决跟踪问题，例如单目摄像机、立体摄像机和激光扫描仪。传统上，跟踪被公式化为贝叶斯推理问题。在该公式中，目标是在给定当前观测和先前状态(s)的情况下估计状态的后验概率密度函数。后验通常以递归方式更新，其中使用运动模型的预测步骤和使用观察模型的校正步骤。在每次迭代中，都要解决数据关联问题以将新观察结果分配给被跟踪对象。扩展的卡尔曼和粒子滤波算法(Giebel et al(2004);Breitenstein et al (2011);Choi等人(2013))在这一背景下被广泛使用。不幸的是，递归的方法使得很难从检测的错误中恢复，并且由于缺少观测结果而无法跟踪遮挡。因此，非递归方法得到了广泛的应用，该方法优化了全局能量函数与时间窗口中所有轨迹的关系。然而，每个物体可能的目标轨迹的数量和场景中潜在目标的数量导致了一个非常大的搜索空间。

![img](https://img-blog.csdnimg.cn/20190417205659643.png)

*图38:Andriyenko & Schindler(2011)提出的能量函数分量。上一行和下一行显示了具有更高和更小能量的配置。较暗的灰度值对应较高的目标概率。改编自Andriyenko & Schindler(2011)。*
  解决这个问题的一种方法是限制可能的位置集并解决数据关联问题。Zhang等人(2008)提出了一个优雅的解决方案，将任务转换为一个最小成本流问题，该问题可以在一元和成对潜力存在的情况下在多项式时间内全局最优地解决。它们通过使用显式的遮挡模型来增强网络，从而处理长期的对象间遮挡。Leibe等人(2008b)将重点放在自动驾驶汽车应用上，提出了一种非马尔可夫假设选择框架用于在线跟踪。Ess等人(2009a)通过立体视觉和测距深度的整合，扩展了这种方法。
  作为离散化的替代方案，已经提出了连续能量最小化方法。对于这种高度非凸性的问题，Andriyenko＆Schindler（2011）使用搜索式能量最小化方案，重复跳跃移动以逃离周最小值并更好地探索变维搜索空间。其能量函数的不同分量的影响如图38所示。考虑到目标动力学、相互排斥和跟踪持久性等物理约束，Milan et al.(2014)扩展Andriyenko & Schindler(2011)的连续能量函数。本质即在离散域上将每个观测值分配给数据关联中的某个目标。因此，Andriyenko等人(2012)认为联合离散和连续的公式能够更自然地描述跟踪问题。他们的方法在用标签成本的离散优化方法来解决数据关联问题和在不考虑标签成本的情况下分析拟合连续轨迹之间交替进行。Milan等(2013)提出了一种混合离散-连续条件随机场模型，该模型专门针对数据关联和轨迹估计中的互斥问题。在数据关联中，每个观测值最多分配给一个目标，而在轨迹估计中，两个轨迹始终保持空间分离。
跟踪多目标最常用的方法是通过检测跟踪。分类器用于检测某个类的目标，这些目标随着时间的推移需要相互关联。这个公式在多目标跟踪中非常流行，因为只跟踪相关的目标，这样可以节省计算资源。然而，跟踪结果直接受到分类器检测误差的影响。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190417205725674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图39:Leibe et al(2008b)使用检测和相应的自顶向下的分割方法来学习用于跟踪特定对象的颜色模型。改编自Leibe et al. (2008b)。*
**多重线索**:对于数据关联，已经发现不同互补线索的组合，可以提高跟踪系统的鲁棒性。Giebel等人（2004）使用不同的线性子空间模型来学习时空形状表示，该模型可以处理外观变化并且在粒子滤波器的观察模型中组合来自立体的形状，纹理和深度。类似地，Gavrila & Munder(2007)将相同的线索集成到带有级联模块的检测和跟踪系统中。基于立体的兴趣生成区域、基于形状的检测、基于纹理的分类和基于立体的验证等能够使系统专注于相关的图像区域。他们通过形状匹配的结果对基于纹理的组件分类器进行加权，提出了一种新颖的专家混合体结构。Choi等人(2013)组合使用检测系统，每一种检测系统都专门针对不同的任务(行人和上半身、脸部、肤色、基于深度的形状和运动)进行外观跟踪。在观测似然中结合了所有探测系统的反馈，改善了观测与跟踪的匹配。

### 9.1 立体跟踪

目前一些工作已经研究了用于物体跟踪和立体深度估计的联合公式，在估计场景中的物体的轨迹的同时获得场景的结构。Leibe等(2007,2008b)提出了一种结合场景几何估计、二维目标检测、三维定位、轨迹估计和跟踪的方法。他们使用图39所示的对象检测和自顶向下的目标分割来学习对象特定的颜色模型。场景结构用于寻找物理上可信的时空轨迹，并且最终的全局优化准则考虑了物体与物体的相互作用，从而细化了三维定位和轨迹估计结果。Ess等人(2009a)使用图形模型，联合估计摄像机的位置、立体深度、目标检测和所有物体随时间的姿态。因此，图形模型表示了不同组件之间的交互，并合并了对象-对象交互。
**检测前跟踪**：除了便于解决跟踪问题之外，深度还允许将场景分割成与其类别无关的不同对象。在跟踪前检测中，这些被分割的类无关对象会被直接视为跟踪公式的观测值。这种方式下，跟踪系统是独立于分类器的，因此也允许跟踪以前从未见过的对象或只存在少量训练数据的对象。此外，来自物体估计轨迹的运动信息可以作为检测某一类物体的另一个线索。Mitzel＆Leibe（2012）通过使用立体深度对场景进行分割来提取对目标的观察。通过紧凑的3D表示，他们可以稳定地跟踪已知和未知的对象类别。该表示还允许它们检测诸如携带物品之类的异常形状。

### 9.2 行人跟踪

如前所述，对行人的跟踪和检测对于自动驾驶来说尤为重要。Andriluka等人(2008)将检测和人体姿态跟踪的优点结合在一个单一框架中。他们使用基于肢体的结构模型扩展了最先进的人体检测器，并使用分层高斯过程潜变量模型（hGPLVM）对检测到的肢体进行动态建模。相比只考虑一帧的方法，这使得他们能够更可靠地检测到人。结合一个隐藏的马尔科夫模型(HMM)，他们可以在很长的序列中跟踪人们。他们将Andriluka等人（2010）中的这一想法扩展到单目图像的3D姿态估计。在第一阶段，他们估计人们的二维清晰度和观点，并通过少数帧将他们联系起来。然后利用累积的二维图像证据用hGPLVM估计三维姿态。通过与HMM的组合可以实现更长时间的跟踪。这种方法使他们能够从单目图像中准确地估计出多人的三维姿态。

### 9.3 最先进的

最流行的多对象跟踪的数据集有PETS (Ferryman& Shahrokni(2009)), TUD (Andriluka et al .(2008)),ETHZ(Ess et al .(2008))、MOT (Leal-Taixe´et al。(2015);Milan et al.(2016))和KITTI (Geiger et al. (2012b, 2013))。而PETS和TUD只提供来自静态观察者的数据，其他则是通过移动平台获得的，移动平台更接近自动驾驶设置。在MOTChallenge（Leal-Taixe等人（2015））中，作者通过提出新的大型数据集和评估方法，解决了多目标跟踪缺乏集中基准的问题。该基准测试为跟踪任务提供了检测地面事实，允许采用基于跟踪对象的能力比较方法，而不受检测器引起的错误的影响。表12提供了使用检测真实值的方法的排行榜，而表13所示为使用私有检测器的方法的排行榜。对于自动驾驶应用程序，KITTI (Geiger et al. (2012b))提供了两个基准，一个用于表14中的汽车跟踪(KITTI car)，另一个用于表15中的行人跟踪。用星号标记的方法使用区域检测(Wang et al.(2015))对跟踪性能进行独立比较。与MOTChallenge不同的是，这两个单独的数据集允许将分析集中在一个对象类上，并深入研究与该类相关的问题。在表12、13、14、15中，我们考虑了Stiefelhagen et al.(2007)引入的两种流行的跟踪测量方法:多目标跟踪精度(MOTA)和多目标跟踪精度(MOTP)，以及多跟踪精度(MT)和多丢失轨迹(ML)的比值，ID交换器数量(IDS)和跟踪分割(FRAG)的数量。大多数被跟踪或被丢失的轨迹都是真实值轨迹，它们分别被假设覆盖了至少80%或最多20%。

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/2019041720582840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

**在MOT16上**:为了克服由于跟踪对象外观变化带来的问题Fagot-Bouquet等人(2016)在能量最小化公式中使用基于稀疏表示的外观模型。这种外观模型使用被分组到字典中的少量模板来对目标外观建模，从而定义了一个线性子空间。Tang等人(2016b)提出了一个最小成本子图多重公式，解决了观测的时空关联，同时还结合了局部的成对特征。成对特征是基于局部外观匹配的，其对于部分遮挡和相机运动是鲁棒的。这使他们能够使用一种有效的算法，可以处理多次检测的长视频，并且在MOT16上的表现优于Fagot-Bouquet等（2016）。Levinkov et al .(2016)提出的利用所提供的检测地面真相的方法在MOT16上表现最佳。他们考虑一个组合优化问题，其解决方案定义了一个图的分解和节点标记。他们用一个单调收敛于局部极小值的局部搜索算法来解决这个问题。Tang等人(2016b)的多重公式可作为该算法的特例。
**在KITTI上**：对于汽车跟踪的任务，Lenz等人（2015）提出了Zhang等人（2008年）提出的最小成本流动跟踪公式的计算和记忆有界版本。 这种方法实现了良好的准确性和精确度，同时也是KITTI汽车上最快的方法之一（表14）。
  Yoon等人(2015)提出了另一种用于跟踪汽车和行人的在线跟踪方法。在这项工作中，他们解决了复杂的摄像机运动问题，在这种情况下，传统的运动模型不成立。他们通过构建一个相对运动网络来描述物体之间的相对运动从而来排除摄像机的运动的影响。利用贝叶斯公式，他们展示了使用多个相对运动模型的优势以及与Lenz等人(2015)相比的改进。在KITTI行人基准测试(表15)中，它们属于性能表现最佳的方法中的一部分。类似的性能实现了由Choi（2015）提出的近在线多目标跟踪算法，该算法被公式化为全局数据关联问题。他们的主要贡献是聚合了用于相对运动模式编码的本地流描述符（ALFD）。无论在何种应用场合，它们都能很好的与远距离探测相匹配。由于使用多种特征线索，他们的方法优于所有的在线跟踪KITTI汽车的方法。

![img](https://img-blog.csdnimg.cn/20190417205905436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

与贝叶斯的最小成本公式相反，Xiang等（2015a）将跟踪问题视为马尔可夫决策过程（MDP）。 他们使用强化学习来学习MDP的策略，其对应于学习数据关联的相似性函数。Xiang et al（2015a）的这种方法是KITTI汽车上表现最好的方法之一。 Lee等人（2016b）将基于目标的卷积神经网络和运动检测器结合在贝叶斯过滤框架中。 它们使用变化点检测算法检测漂移和遮挡。 在两个KITTI基准测试（表14,15）中，这种方法在准确度（MOTA）和精度（MOTP）方面都优于其他方法。

### 9.4 讨论

只有通过合理的目标检测才能实现可靠的目标跟踪。当比较KITTI(表14,15)中有星号和没有星号的方法或MOT16排行榜中使用表12中的地面真值探测和表13中的目标探测器的方法时，可以观察到检测系统的影响。但是，在5.6节中已经讨论了目标检测器，因此我们将重点讨论跟踪问题。与检测问题类似，跟踪行人比跟踪汽车更具挑战性。原因是行人的运动很难预测，因为他们可能会突然改变方向，而汽车的运动则很容易建模。在真实场景中，车辆或行人的部分和完全遮挡经常会出现，从而导致检测失败。在这些情况下，跟踪系统需要重新识别被跟踪的对象，但光线条件的变化或被跟踪的对象与其他对象的相似性导致重新识别变得困难。这些问题会导致轨迹的重新初始化，这可以在MOT16和KITTI的FRAG和IDS中观察到。此外，我们注意到到目前为止大多数跟踪系统是很复杂的，并且在文献中尚未提出过端到端的多目标跟踪算法。因此弥补从检测到跟踪的这一差距可能是未来研究的一个有前途的途径。

## 10 场景理解

自动驾驶的基本要求之一是充分了解其周围的区域，如复杂的交通场景。户外场景理解的复杂任务包括深度估计、场景分类、目标检测与跟踪、事件分类等子任务。每一个任务都描述场景的特定方面。将这些方面的一些模型联合起来，利用场景中不同元素之间的关系，有利于获得整体的理解。大多数场景理解模型的目标是获得一个丰富而紧凑的场景表示，包括场景的所有元素，例如布局元素、交通参与者以及彼此之间的关系。与二维图像领域的推理相比，三维推理在解决几何场景理解问题上发挥着重要的作用，并以三维对象模型、布局元素和遮挡关系的形式使场景的信息表达更加丰富。场景理解的一个具体挑战是对城市和次城市交通场景的解释。与公路和乡村道路相比，城市场景包含了许多独立移动的交通参与者，道路和十字路口的几何布局变化更多，由于模糊的视觉特征和光照变化增加了难度。
  从单幅图像到视频:在他们开创性的工作中，霍伊姆等人(2007)从一张图像中推断出一个场景的整体3D结构。表面布局表示为一组粗糙的几何类，具有一定的方向，如支持、垂直和天空。这些元素是通过学习一个基于外观的模型通过多个分段来推断的。Ess等人(2009b)在分类和表示方面提出了一种更细粒度的方法，使用超像素来识别交通场景中的道路和对象类型。Liu等人(2014)也使用超像素通过从已知深度的图像池中检索相似的图像并建模超像素之间的遮挡关系来进行单个图像深度估计。虽然这些方法在应用于单个图像时显示出良好的效果，但视频序列中的运动是信息的丰富来源，尤其是在高度动态的场景中。Kuettel et al.(2010)通过学习共同发生的活动和它们之间的时间规则，对复杂动态场景中移动物体的时空依赖关系进行建模。然而，他们的方法假设一个静态的观察者，并且场景必须观察相当长的一段时间才能做出决定，因此它不适用于自动驾驶系统。Geiger等人(2014)采用概率模型，对交叉口的三维场景布局以及场景中车辆的位置和方位进行了联合推理。在这种方法中，轨迹独立的假设可能会导致难以置信的配置，比如汽车相撞。Zhang et al.(2013)通过在交通模式的形式中包含高级语义来解决这个问题，如图40所示。

![img](https://img-blog.csdnimg.cn/20190417210238656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图40:使用交通模式理解场景。在Geiger et al.(2014)中，对象之间的高阶依赖被忽略，导致了碰撞车辆的物理上不合理的推断结果(左)。Zhang et al.(2013)提出明确考虑交通模式，以改善场景布局和活动估计结果(正确，正确的情况用红色标注)。改编自zhang等人(2013)*
  结合目标检测和跟踪:场景标记通常与目标检测和跟踪相结合，以实现不同但相关任务之间的信息流。Wojek & Schiele (2008a)利用线性运动模型对车辆进行检测和跟踪。他们还估计摄像机的运动，并将其传播到下一帧的动态条件随机场模型中，用于对象和场景类的联合标记。Wojek等人(2010)将联合推理扩展到3D，在公式中加入行人。他们提出了一个包含多类对象检测、对象跟踪、场景标记和三维几何关系的概率三维场景模型。多帧联合场景跟踪模型在不使用立体图像的情况下提高了三维多目标跟踪任务的性能，但这种方法不能处理部分遮挡的对象。为了解决这个问题，Wojek等人(2011,2013)将多个物体部件检测器集成到3D场景模型中，进行明确的物体与物体之间的遮挡推理(图41)。

![img](https://img-blog.csdnimg.cn/20190417210359136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTA0MzY0,size_16,color_FFFFFF,t_70)

*图41:Wojek等人(2013)将目标检测与跟踪系统与外显遮挡推理相结合的概述。改编自Wojek等人(2013)。*
**其他表示**:除了上述方法使用的基于3D基元的表示外，还有其他表示街道场景的方法。Seff & Xiao(2016)定义了道路布局属性列表，如车道数、可行驶方向、到交叉口的距离等。他们首先利用现有的街景图像数据库和在线导航地图(如OpenStreetMap)，为这些属性自动收集大型数据集。基于此数据集，他们训练了一个深度卷积网络，从单个街景图像中预测每个属性。目标是通过在故障情况下充当备份来减少对高清晰度映射的依赖。de Oliveira等人(2016)受户外几何结构流行的启发，用一组由支撑平面和包围多边形描述的平面多边形来表示三维结构。根据激光雷达提供的三维点云，他们通过使用RANSAC和聚类内值来找到支持平面。随着时间的推移，对于场景的增量3D表示，它们随着新数据的到来而演变。这是通过对基本物体进行垂直和纵向扩展来实现的，以适应新的点云数据。尽管由于计算上的限制，直接从3D测量得到的场景的3D表示通常不是首选的，但是它们紧凑的表示能够快速计算和更新，同时仍然是准确的。

## 11.自动驾驶的端到端学习

目前最先进的自动驾驶方法由许多模型组成，例如检测(交通标志，灯光，汽车，行人)，细分(车道，立面)，运动估计，跟踪交通参与者，重建。然后将这些组件的结果组合到一个基于规则的控制系统中。然而，为了解决操纵汽车方向和速度的问题，这就需要对场景理解中的许多开放挑战提出健壮的解决方案。作为一种替代方法，端到端自动驾驶的几种方法已经在最近的文献中提出。端到端自动驾驶被定义为使用一个自包含系统的驾驶，该系统将从感觉输入(如前置摄像头图像)直接映射到驾驶动作(如转向角度)。
  Bojarski等人(2016)提出了一种针对车道线的端到端深度卷积神经网络，根据给定的专家数据，该神经网络将汽车前端摄像头的图像映射到转向角度。Chen等人(2015a)没有直接学习从像素到动作的映射，而是提出了一种方法，它首先估计少量的可解释的、预先定义的人提供与周围汽车的距离等措施。然后，这些预测的措施与汽车动作手动关联，以启用自动驾驶控制器。
现有的端到端学习方法将像素映射到驱动，并直接模拟演示的性能。然而，由于用于训练的公共数据集的可用性有限，这些方法的成功仅限于在某些情况下收集的数据、相应的模拟或特定的校准驱动设置。因此，Xu等人(2016)提出了一种利用未校准来源的大规模在线数据集学习驱动模型的替代方法。具体来说，他们将自动驾驶作为未来的自主驾驶预测问题，使用一种新的深度学习体系来学习预测当前运动路径。

## 12 结论

本文就自动驾驶汽车计算机视觉中的问题、数据集和方法进行了综述。为了实现这一目标，我们考虑了历史上最相关的文献以及几个特定主题的最新进展，包括识别、重建、运动估计、跟踪、场景理解和端到端学习。我们通过对KITTI基准测试的全新深入定性分析，并考虑到其他数据集，讨论了这些主题中的开放问题和当前的研究挑战。我们的交互式在线工具提供一个简单的导航用于搜索文献并使用一个图表对算法分类进行可视化。在未来，我们计划保持工具与相关文献的更新，以提供该领域的最新概述。我们希望我们的调查和工具将鼓励新的研究，并通过提供详尽的概述，以方便初学者进入该领域。